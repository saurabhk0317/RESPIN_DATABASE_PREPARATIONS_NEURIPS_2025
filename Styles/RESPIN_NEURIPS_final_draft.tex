\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025

% The authors should use one of these tracks.
% Before accepting by the NeurIPS conference, select one of the options below.
% 0. "default" for submission
 % \usepackage{neurips_2025}
% the "default" option is equal to the "main" option, which is used for the Main Track with double-blind reviewing.
% 1. "main" option is used for the Main Track
%  \usepackage[main]{neurips_2025}
% 2. "position" option is used for the Position Paper Track
%  \usepackage[position]{neurips_2025}
% 3. "dandb" option is used for the Datasets & Benchmarks Track
 % \usepackage[dandb]{neurips_2025}
% 4. "creativeai" option is used for the Creative AI Track
%  \usepackage[creativeai]{neurips_2025}
% 5. "sglblindworkshop" option is used for the Workshop with single-blind reviewing
 % \usepackage[sglblindworkshop]{neurips_2025}
% 6. "dblblindworkshop" option is used for the Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop]{neurips_2025}

% After being accepted, the authors should add "final" behind the track to compile a camera-ready version.
% 1. Main Track
 % \usepackage[main, final]{neurips_2025}
% 2. Position Paper Track
%  \usepackage[position, final]{neurips_2025}
% 3. Datasets & Benchmarks Track
 \usepackage[dandb, final]{neurips_2025}
% 4. Creative AI Track
%  \usepackage[creativeai, final]{neurips_2025}
% 5. Workshop with single-blind reviewing
%  \usepackage[sglblindworkshop, final]{neurips_2025}
% 6. Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop, final]{neurips_2025}
% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote.
% For workshops (5., 6.), the authors should add the name of the workshop, "\workshoptitle" command is used to set the workshop title.
% \workshoptitle{WORKSHOP TITLE}

% "preprint" option is used for arXiv or other preprint submissions
 % \usepackage[preprint]{neurips_2025}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


% --- Encoding and Fonts ---
\usepackage{fontspec}       % For custom fonts

% --- Language setup (polyglossia replaces babel) ---
\usepackage{polyglossia}
\setmainlanguage{english}
\setotherlanguages{hindi}
\newfontfamily\hindifont{Noto Sans Devanagari}[Script=Devanagari]
\newfontfamily\devanagarifont[Script=Devanagari]{Noto Serif Devanagari}

% --- Math & Symbols ---
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{microtype}
\usepackage{xcolor}

% --- Tables & Figures ---
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
\usepackage[table]{xcolor}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{pifont}   % for checkmarks and crosses
\usepackage{array}

% --- Algorithms ---
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

% --- Lists & Formatting ---
% \usepackage{titlesec}
\usepackage{enumitem}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{nicefrac}
\usepackage{lipsum}

% --- URLs & Hyperlinks ---
\usepackage[hyphens]{url}
\usepackage{hyperref}

% --- Icons & Logos ---
\usepackage{hologo}
\usepackage{fontawesome}

% --- Miscellaneous ---
\usepackage{arydshln}
\usepackage{colortbl}
\usepackage{adjustbox}


\title{RESPIN-S1.0: A read speech corpus of 10000+ hours in dialects of nine Indian Languages}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
\parbox{\textwidth}{
\centering
Saurabh Kumar$^{1}$,
Abhayjeet Singh$^{1}$,
Deekshitha G$^{1}$,
Amartyaveer$^{1}$,
Jesuraj Bandekar$^{1}$,
Savitha Murthy$^{1}$,
Sumit Sharma$^{1}$,
Sandhya Badiger$^{1}$,
Sathvik Udupa$^{1}$,
Amala Nagireddi$^{1}$,
Srinivasa Raghavan K M$^{2}$,
Rohan Saxena$^{2}$,
Jai Nanavati$^{2}$,
Raoul Nanavati$^{2}$,
Janani Sridharan$^{2}$,
Arjun Mehta$^{2}$,
Ashish Khuraishi K S$^{2}$,
Sai Praneeth Reddy Mora$^{2}$,
Prashanthi Venkataramakrishnan$^{2}$,
Gauri Date$^{2}$,
Karthika P$^{2}$,
Prasanta Kumar Ghosh$^{1}$\thanks{Corresponding author: \texttt{prasantg@iisc.ac.in}}
}\\[30pt]
$^{1}$Department of Electrical Engineering, Indian Institute of Science, Bangalore, India\\
$^{2}$Navana Tech, Mumbai, India\\%[6pt]
\texttt{spirelab.ee@iisc.ac.in, prasantg@iisc.ac.in}
}



\begin{document}
\maketitle
% \tableofcontents
\begin{abstract}
We introduce \textbf{RESPIN-S1.0}, the largest publicly available dialect-rich read-speech corpus for Indian languages, comprising more than 10,000 hours of validated audio across nine major languages: Bengali, Bhojpuri, Chhattisgarhi, Hindi, Kannada, Magahi, Maithili, Marathi, and Telugu. Indian languages exhibit high dialectal variation and are spoken by populations that remain digitally underserved. Existing speech corpora typically represent only standard dialects and lack domain and linguistic diversity. RESPIN-S1.0 addresses this limitation by collecting speech across more than 38 dialects and two high-impact domains: agriculture and finance. Text data were composed by native dialect speakers and validated through a pipeline combining automated and manual checks. Over 200,000 unique sentences were recorded through a crowdsourced mobile platform and categorised into clean, semi-noisy, and noisy subsets based on transcription quality, with the clean portion alone exceeding 10,000 hours. Along with audio and transcriptions, RESPIN provides dialect-aware phonetic lexicons, speaker metadata, and reproducible train, development, and test splits. To benchmark performance, we evaluate multiple ASR models, including TDNN-HMM, E-Branchformer, Whisper, and wav2vec2-based self-supervised models, and find that fine-tuning on RESPIN significantly improves recognition accuracy over pretrained baselines. A subset of RESPIN-S1.0 has already supported community challenges such as the SLT Code Hackathon 2022 and MADASR@ASRU 2023 and 2025, releasing more than 1,200 hours publicly. This resource supports research in dialectal ASR, language identification, and related speech technologies, establishing a comprehensive benchmark for inclusive, dialect-rich ASR in multilingual low-resource settings.\\[4pt]
\textbf{Dataset:} \url{https://spiredatasets.ee.iisc.ac.in/respincorpus}\\
\textbf{Code:} \url{https://github.com/labspire/respin_baselines.git}
\end{abstract}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

India's linguistic landscape, comprising 22 scheduled languages and hundreds of dialects~\footnote{\url{https://web.archive.org/web/20240914124112/https://censusindia.gov.in/nada/index.php/catalog/42561/download/46187/Language_Atlas_2011.pdf}}, demands inclusive speech technologies. However, the lack of curated and dialect-rich audio–text datasets~\cite{10.1007/978-3-030-95711-7_8,gumperz1961speech} has limited progress. Although 64\% of India’s population resides in rural areas and 57.8\% belong to agricultural households~\cite{wb}, most ASR research has focused on English or standard language forms~\cite{hari_thesis}. Existing corpora usually represent only standard dialects~\cite{Mandal1990}, resulting in degraded performance on regionally diverse speech.

To bridge this gap, \textbf{RESPIN-S1.0} presents a large-scale, multi-dialectal, multi-domain read-speech corpus covering nine Indian languages: Bengali (bn), Bhojpuri (bh), Chhattisgarhi (ch), Hindi (hi), Kannada (kn), Magahi (mg), Maithili (mt), Marathi (mr), and Telugu (te). These were selected based on speaker population, socio-economic diversity, and resource availability. Figure~\ref{fig:dialect_distribution_india_map} shows the district-level language distribution and dialect breakdown. RESPIN is the first public corpus to provide large-scale dialectal data for Bhojpuri, Chhattisgarhi, and Magahi. The pipeline, from text composition to audio validation, was implemented at the dialect level to preserve linguistic integrity and includes manually verified phonetic lexicons following Indian Language Speech Label (ILSL) guidelines~\cite{Samudravijaya2021} and speaker metadata such as pincode, gender, and age group.

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{Styles/figures/figure-1-neurips-v2_mod.eps}
    \caption{(i) District-level distribution of the nine RESPIN languages across India, based on the 2011 Census (illustrative, not to scale). Each language is shown in a distinct color, with an inset showing dialect-wise representation for Bengali. (ii) Language classification: A refers to scheduled, non-Devanagari; B to scheduled, Devanagari; and C to non-scheduled, Devanagari languages.}
    \label{fig:dialect_distribution_india_map}
    \vspace{-10pt}
\end{figure}

To promote reproducibility, RESPIN provides train, development, and test splits, dialect-level metadata, and ASR benchmarks using TDNN-HMM~\cite{povey18_interspeech}, E-Branchformer~\cite{Kwangyoun_EBF_SLT2022}, Whisper~\cite{Radford_whisper2023}, and wav2vec2-based SSL models such as IndicWav2Vec2~\cite{javed2021building} and SPRING-Data2Vec.\footnote{\url{https://asr.iitm.ac.in/models}} Fine-tuning on RESPIN consistently improves ASR performance over models trained on external corpora. The dataset has already supported multilingual ASR challenges including SLT Code Hackathon 2022~\footnote{\url{https://sites.google.com/view/slt-team}} and MADASR (ASRU 2023, ASRU 2025)~\footnote{\url{https://sites.google.com/view/respinasrchallenge2025/home}}, with over 1200 hours released to the community. By capturing India’s dialectal diversity, RESPIN advances inclusive voice technologies for underserved linguistic communities across India.

The RESPIN project was supported by the Gates Foundation to promote speech technologies for low-resourced Indian languages and marginalized communities. All data collection followed institutional ethics approval at Indian Institute of Science (IISc) Bangalore, with informed consent, privacy safeguards, and fair participant compensation. Additional details on demographics, compensation, and consent are provided in Appendix~\ref{sec:ethics}.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background}
\label{sec:background}
Dialectal datasets are essential for developing accurate and inclusive speech technologies. They reflect real-world language use, improve recognition across regions, ensure accessibility for marginalized communities, support public service applications, and help preserve linguistic diversity. Continued investment in open, community-driven, and dialect-rich data collection is crucial for building equitable and effective speech systems. As emphasized in \cite{yvideo}, treating dialects respectfully is fundamental to supporting underserved populations.

Globally, several initiatives have aimed to capture dialectal variation for inclusive speech technology. In the Arabic-speaking world, CASABLANCA~\cite{talafha-etal-2024-casablanca} and MADAR~\cite{bouamor-etal-2018-madar} provide large-scale, multi-dialect corpora representing regional Arabic. In China, projects such as AISHELL~\cite{bu2017aishell1opensourcemandarinspeech} and THCHS-30~\cite{THCHS30_2015} focus primarily on Mandarin but lay important groundwork for dialectal research. In Africa, Masakhane ASR~\cite{azime2023masakhaneafrisentisemeval2023task12} and CMU Wilderness~\cite{8683536} expand coverage for underrepresented African languages and dialects through open, community-based initiatives. Collectively, these efforts represent major progress toward bridging the global gap in speech technology resources.

\begin{table}[!ht]
\vspace{-5mm}
\centering
\caption{Existing Indic Datasets}
\label{tab:datasets}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{l l l l r r r l}
\toprule
\textbf{Dataset} & \textbf{Languages} & \textbf{Domains} & \textbf{Districts} & \textbf{Hours} & \textbf{Speakers} & \textbf{Sentences} & \textbf{Source} \\
\midrule
INDICVOICES~\cite{javed2024indicvoices} & 13 & 52 & 145 & 7348 & 16237 & 11,00,000+ & Wikipedia, Composed, Spontaneous \\
INDICVOICES-R~\cite{sankar2024indicvoices} & 22 & multi & multi & 1704 & 10496 & NA & NA \\
Kathbath~\cite{javed2023indicsuperb} & 12 & multi & 203 & 1684 & 1217 & 12,00,000+ & IndicCorp (Web data) \\
Shrutilipi~\cite{bhogale2023effectiveness} & 12 & multi & NA & 6457 & NA & 33,00,000 & All India Radio \\
NPTEL~\cite{bhogale2023vistaar} & 8 & 1 & NA & 857 & NA & NA & Lectures \\
Svarah~\cite{javed2023svarah} & 1 & 9 & 65 & 9.6 & 117 & NA & Wikipedia, Prompts, Spontaneous \\
SPRING-INX~\cite{gangwar2023spring} & 10 & multi & 40+ & 2000 & 7609 & NA & NA \\
SPIRE-SIES~\cite{singh2023spire} & 1 & NA & NA & 193 & 1607 & NA & NA \\
FLEURS~\cite{conneau2023fleurs} & 13 & NA & NA & 156 & 39 & NA & Wikipedia \\
Gram Vaani~\cite{bhanushali2022gram} & 1 & multi & 25 & 1108 & NA & NA & Spontaneous Speech \\
IISc-MILE~\cite{diwan2021multilingual} & 2 & NA & NA & 497 & 1446 & NA & NA \\
MUCS~\cite{diwan2021multilingual} & 3 & 4 & 4 (for Odia) & NA & 310 & 9080 & NA \\
\textnormal{V\=aks\~anca\-yah}~\cite{adiga2021automatic} & 1 & 8 & NA & 78 & 27 & 46,000 & Online stories \\
E\&NE languages~\cite{basu2021multilingual} & 4 & NA & multi & 19.75 & NA & NA & NA \\
NISP~\cite{kalluri2021nisp} & 6 & NA & NA & 56.86 & 345 & NA & news, TIMIT \\
CommonVoice~\cite{commonvoice:2020} & 8 & 4 & NA & 373 & NA & NA & Wikipedia, Composed \\
CMS~\cite{he2020open} & 6 & NA & NA & 35 & 243 & NA & Composed \\
IITB-MSC~\cite{abraham2020crowdsourcing} & 1 & 1 & 1 & 109 & 36 & 3000 & Textbooks \\
IndicSpeech~\cite{srivastava2020indicspeech} & 3 & NA & NA & 24 & 3 & 42,046 & Online news \\
MSR Challenge~\cite{srivastava2018interspeech} & 3 & NA & NA & 150 & 1286 & 1,02,397 & NA \\
Google TTS~\cite{sodimana2018step} & 1 & NA & NA & 3 & 6 & NA & NA \\
IIITH-ILSC~\cite{vuddagiri2018iiith} & 23 & NA & NA & 103.5 & 1150 & NA & NA \\
IndicTTS~\cite{wilkinson2016open} & 13 & 4+ & NA & 389.6 & 26 & NA & Literature, newspapers \\
IIITH-ISD~\cite{prahallad2012iiit} & 7 & NA & NA & 11 & 35 & 1000 & Wikipedia \\
\midrule
\textbf{RESPIN-S1.0} & \textbf{9} & \textbf{2} & \textbf{38+} & \textbf{10,416.58} & \textbf{18,000+} & \textbf{2,09,822} & \textbf{Composed} \\
\bottomrule
\multicolumn{8}{l}{\small \textit{NA} = Information Not Available}
\end{tabular}
}
\end{table}


Table~\ref{tab:datasets} compares major open-source Indic speech corpora across languages, domains, districts, duration, speaker count, and data sources. While many datasets cover multiple languages and include large audio volumes, most lack dialectal diversity and regional representation. They rely primarily on web content such as Wikipedia, books, or news articles, leading to limited relevance to everyday speech. RESPIN-S1.0 addresses these gaps by focusing on agriculture and banking, two domains central to India’s rural and low-literacy communities, and by manually composing 2,09,822 sentences that capture regionally grounded, colloquial usage.

RESPIN-S1.0 introduces several key contributions that distinguish it from existing corpora:

\textbf{New language and dialect coverage} Unlike large-scale datasets such as INDICVOICES~\cite{javed2024indicvoices} and INDICVOICES-R~\cite{sankar2024indicvoices}, which emphasize scheduled languages, RESPIN is the first publicly available corpus offering validated data for low-resource, non-scheduled languages including Bhojpuri, Chhattisgarhi, and Magahi. These are often grouped under Hindi but possess distinct linguistic characteristics. With over 10,000 hours of validated audio from more than 18,000 speakers across 38 dialect-rich districts, RESPIN is the most comprehensive dialect-aware resource for Indian languages.

\textbf{Domain-specific composition} In contrast to datasets derived from generic sources, RESPIN’s text corpus was authored by native dialect speakers for agriculture and finance. This design ensures vocabulary and sentence structures mirror natural communication within these key domains. The dataset enables voice-based digital services in native dialects, improving accessibility and fostering user trust.

\textbf{Dialectal integrity across the pipeline} Each stage of corpus creation—from text design to recording and validation—was implemented at the dialect level to preserve authenticity and ensure linguistic consistency across all 38 dialects.

Together, these design choices make RESPIN-S1.0 a distinctive and valuable resource for advancing inclusive, dialect-aware speech technologies in India’s linguistically diverse setting.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Collection and Validation Pipeline}
\label{sec:data_val}

\begin{wrapfigure}{r}{0.40\textwidth}
    \centering
    \vspace{-10pt}
    \includegraphics[width=0.4\textwidth]{figures/data_pipeline.png}
    \caption{Data creation pipeline maintaining dialectal integrity.}
    \label{fig:datapipeline}
    \vspace{-10pt}
\end{wrapfigure}

RESPIN is the first large-scale Indian speech corpus designed to preserve dialectal integrity throughout the data creation process. As shown in Figure~\ref{fig:datapipeline}, the pipeline includes language and dialect selection, manual text composition, multi-stage validation, and speaker-level audio collection. Unlike corpora built from scraped or generic online content, RESPIN focuses on agriculture and finance, with all text and audio created and validated at the dialect level. Contributors received guidance and updates throughout data collection, remaining well-informed and actively engaged with the project’s goals and progress.

Participants were authenticated through a secure WhatsApp-based workflow, completed a click-wrap consent form in the Bolo App, and were briefed on task goals and compensation in their native languages (see Appendix~\ref{sec:ethics} for onboarding and consent details).

\subsection{Language and Dialect Selection}

According to the Census of India (2011), 50.58M, 16.25M, 12.71M, and 13.58M people speak Bhojpuri, Chhattisgarhi, Magahi, and Maithili, respectively. While Magahi is often misclassified as a dialect of Hindi, it represents a distinct branch of the Indo-Aryan subfamily. To support such large speaker populations, it is essential to develop robust speech resources with rich vocabularies and diverse sentence corpora. RESPIN aims to build an ecosystem of speech recognition resources that empower India’s working-class population. Between 2022 and 2023, 45.76\% of India’s workforce was engaged in agriculture and allied sectors, while finance and banking continue to play a central role in daily communication and access to services. By focusing on these two domains, RESPIN seeks to bridge the gap between under-resourced language communities and accessible, voice-driven technologies.

To support domain-specific sentence creation, a comprehensive list of topics was curated across agriculture and finance to guide sentence composition. These topics were compiled from diverse sources including magazines, websites, academic portals, and Wikipedia’s outline articles. Wikipedia’s topic trees and linked articles were particularly useful for hierarchical organization. The final list contains around 1500 topics, each associated with relevant reference links. Starting from broad categories such as crop cultivation or digital banking, the list narrows to specific subtopics including sugarcane harvesting techniques, UPI PIN setup, and transaction history checks in mobile apps. This curated topic bank ensured comprehensive and contextually relevant coverage of the target domains.

\subsection{Text Data Acquisition and Validation}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\textwidth]{Styles/figures/figure_2_neurips_v3.eps}
    \caption{Flowchart showing the RESPIN text data preparation pipeline.}
    \label{fc1}
\end{figure}

The creation of a dialect-level text corpus formed the foundation of RESPIN. Figure~\ref{fc1} outlines the overall workflow. The process began with onboarding and training dialect experts who curated text with high dialectal specificity, ensuring the inclusion of regional nuances and natural variation. As described earlier, the corpus was designed to capture sentences from agriculture and finance domains, making RESPIN uniquely domain-specific. Native speakers were hired through a multi-stage selection process to compose these sentences. The raw text then passed through a validation pipeline that combined automatic and manual checks to ensure linguistic quality and compliance. Only validated sentences were used for subsequent audio collection.

\subsubsection{Sentence Creation}

Large volumes of digital text exist in standard language forms but often lack colloquial and dialectal variation. To address this, RESPIN prioritized sourcing sentences directly from native speakers across districts, ensuring that the text reflects authentic regional expressions and natural communication patterns. Composers were tasked with crafting conversational, domain-specific sentences aligned with assigned topics in agriculture and finance. This enriched linguistic diversity but also introduced challenges, as dialectal variation can change even within small geographic regions. Recognizing this fluidity, RESPIN adopted an inclusive strategy that embraced intra-dialectal variation, yielding a rich and representative dataset.

Sentence composition followed strict guidelines to ensure consistency and usability: limiting sentence length, avoiding sentence-initial pronouns, excluding non-language numerals, restricting punctuation to period, comma, and question mark, avoiding controversial content, adhering to topic relevance, and maintaining consistent acronym formatting. Manual composition, though resource-intensive, produced the highest quality data. Translation from composed sentences was used selectively to address dialectal gaps. The proportion of translated sentences in Bhojpuri, Chhattisgarhi, Hindi, Kannada, Magahi, Maithili, Marathi, and Telugu was 6.65\%, 100\%, 9.8\%, 0.1\%, 0.4\%, 16.5\%, 5.1\%, and 5.2\%, respectively. Bengali sentences were composed entirely from scratch.

\subsubsection{Sentence Validation}

The composed text corpus underwent a multi-stage validation pipeline involving both automated (AC) and manual checks (MC) by trained validators. As multiple contributors participated in composition, inconsistencies and minor errors were expected. Since each sentence serves as a prompt for crowd-sourced recording, validation ensures that every utterance is accurate, unambiguous, and compatible with the mobile interface. The pipeline structure was consistent across languages with minor adaptations for linguistic differences. Key checks included (1) duplicate removal (AC), (2) invalid character correction (MC), (3) sentence length pruning (MC), (4) acronym standardization (MC), (5) matra correction (MC), (6) word-level edits (MC), (7) similar sentence filtering (MC), (8) homophone disambiguation (MC), and (9) additional language-specific checks (see Appendix~\ref{appendix:text_val}). Approximately 3.6\% of the raw corpus was discarded due to unfixable errors or dialect mismatch. The process followed a version-controlled workflow, where each stage generated a new corpus version for auditing and rollback.

\subsection{Audio Data Acquisition and Validation}

Following text validation, audio data collection was conducted through a dedicated mobile application. Native speakers were prompted to read validated sentences aloud and record them in quiet environments. Each speaker was assigned a maximum of 577 sentences, though some recorded additional sentences to meet dialect-specific targets when others dropped out. To capture intra-dialectal acoustic variation, each sentence was recorded by multiple speakers, typically between 30 and 150. This many-to-one mapping enabled the dataset to represent a range of pronunciation styles, prosodic patterns, and speaking rates within each dialect.

\subsubsection{Audio Validation Pipeline}
\label{subsubsec:audio_validation}

The recorded audio underwent a structured validation process combining manual and semi-automated checks. Initially, about 5\% of utterances in each dialect were manually audited to verify audio-text alignment. Based on these results, the entire dataset was categorized into three quality slabs: \textit{Clean}, \textit{Semi-noisy}, and \textit{Noisy}, using a semi-automated scoring approach.

The slab categorization reflects the proportion of perfectly matched audio-text pairs. The clean slab contains the highest share of exact alignments, while the noisy slab includes those with the lowest. This design allows downstream ASR tasks to select subsets based on quality and robustness requirements. Complete definitions of slabs and associated thresholds are provided in Appendix \ref{appendix:slab_def}.

This validation framework ensures that the RESPIN audio corpus is high-quality, dialect-specific, and suitable for benchmarking robust ASR systems under realistic multilingual and multi-dialect conditions.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FINAL DATA STATISTICS
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{RESPIN-S1.0 Corpus}
\label{sec:respin_corpus}

\subsection{Text Data Analysis}
\noindent
\begin{minipage}[t]{0.61\textwidth}
  \vspace{0pt}  % Ensures top alignment
  \centering
  \includegraphics[width=\linewidth]{figures/text_stats/unique_textid_sentence_stats.eps}
  \captionof{figure}{Unique sentence count per dialect, domain and language.}
  \label{fig:sentence_counts}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.37\textwidth}
  \vspace{0pt}  % Ensures top alignment
  \centering
  \captionof{table}{Lexicon statistics across languages.}
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{lrrr}
      \toprule
      \textbf{LID} & \textbf{\#chars} & \textbf{\#phones} & \textbf{\#words} \\
      \midrule
      bn & 64 & 50 & 18571 \\
      bh & 71 & 54 & 14105 \\
      ch & 68 & 50 & 13230 \\
      hi & 72 & 55 & 16571 \\
      kn & 66 & 50 & 50822 \\
      mg & 72 & 54 & 21711 \\
      mt & 72 & 55 & 19336 \\
      mr & 68 & 51 & 35709 \\
      te & 63 & 48 & 39235 \\
      \bottomrule
    \end{tabular}
  }
  \label{tab:lexicon_stats}
\end{minipage}

Figure~\ref{fig:sentence_counts} presents the distribution of unique sentence counts across dialects, domains (Agriculture and Banking), and languages. Each language includes over 20,000 curated sentences covering 3--5 dialects with representation in both domains. Although perfect balance is constrained by the availability of dialect experts and regional factors, the dataset maintains approximate uniformity across dialect–domain pairs. Slight deviations, such as higher contributions from dialect D5 in \texttt{kn} and D3 in \texttt{mt}, reflect stronger regional participation or easier contributor access.

Table~\ref{tab:lexicon_stats} shows lexicon statistics including unique characters, phonemes, and words per language. Lexicons were generated from the full sentence set (see Appendix~\ref{appendix:lexicon_creation} for details). Kannada (\texttt{kn}) and Telugu (\texttt{te}) exhibit higher word counts (50k and 39k+), indicative of rich morphology. In contrast, Bhojpuri (\texttt{bh}) and Chhattisgarhi (\texttt{ch}) have smaller vocabularies, possibly due to lower lexical variation. Character counts (63--72) align with script complexity, while phoneme inventories (around 50--55) are consistent with Indo-Aryan and Dravidian phonological systems.

These statistics highlight the linguistic richness and dialectal coverage of the RESPIN text corpus. The balanced representation across dialects and domains, together with detailed lexicons, provides a strong foundation for multilingual and multidialectal ASR, language modeling, and speech-language research.


\subsection{Audio Data Analysis}
\subsubsection{Slab-Wise Audio Distribution}\label{sec:slabwise_distribution}

\begin{table}[t]
\centering
\caption{Dialect-wise duration (in hours) across Clean, Semi-noisy, and Noisy subsets for 9 Indian languages.}
\label{tab:duration_updated}
\begin{adjustbox}{width=0.85\textwidth}
\begin{tabular}{llrrrrrrrrr}
\toprule
\textbf{Dialect} & \textbf{Type} & \textbf{bh} & \textbf{bn} & \textbf{ch} & \textbf{hi} & \textbf{kn} & \textbf{mg} & \textbf{mr} & \textbf{mt} & \textbf{te} \\
\midrule
\multirow{3}{*}{D1} & Clean       & 351.25 & 206.40 & 344.89 & 205.25 & 237.38 & 340.88 & 312.58 & 195.10 & 348.78 \\
                    & Semi-noisy  &  32.09 &  64.61 &  31.75 &  49.35 &  58.72 &  15.12 &  63.69 & 117.80 &  51.61 \\
                    & Noisy       &  41.43 &   1.31 &  21.07 &  80.67 &  52.81 &  17.60 &  61.15 &  71.01 &  41.96 \\
\midrule
\multirow{3}{*}{D2} & Clean       & 417.74 & 271.45 & 329.20 & 159.78 & 245.03 & 349.01 & 328.89 & 112.16 & 333.28 \\
                    & Semi-noisy  &  11.25 &  12.97 &  22.37 &  90.78 &  37.07 &  13.94 &  54.39 & 139.60 &  74.12 \\
                    & Noisy       &   5.68 &   0.80 &  12.07 &  88.07 &  38.36 &  13.13 &  49.17 & 180.44 &  33.67 \\
\midrule
\multirow{3}{*}{D3} & Clean       & 347.97 & 283.17 & 297.63 & 195.93 & 235.92 & 333.33 & 321.62 & 203.16 & 331.65 \\
                    & Semi-noisy  &  62.53 &  22.55 &  77.19 &  70.51 &  55.35 &  26.11 &  60.99 & 164.29 &  58.34 \\
                    & Noisy       &  29.46 &   1.10 &  22.81 &  68.28 &  44.17 &  14.87 &  23.49 &  55.73 &  54.49 \\
\midrule
\multirow{3}{*}{D4} & Clean       &    --  & 216.14 & 324.25 & 138.83 & 248.10 & 321.18 & 316.39 & 212.64 & 290.27 \\
                    & Semi-noisy  &    --  &  62.64 &  67.56 & 116.41 &  34.66 &  57.22 & 156.14 &  88.55 &  38.46 \\
                    & Noisy       &    --  &   2.13 &  34.17 &  99.35 &  48.41 &  27.14 &  66.13 &  98.11 &  18.87 \\
\midrule
\multirow{3}{*}{D5} & Clean       &    --  & 236.08 &    --  & 245.14 & 228.13 &    --  &    --  &    --  &    --  \\
                    & Semi-noisy  &    --  &  27.19 &    --  &  35.74 &  64.40 &    --  &    --  &    --  &    --  \\
                    & Noisy       &    --  &   1.39 &    --  &  54.49 &  42.48 &    --  &    --  &    --  &    --  \\
\midrule
\textbf{Total} & Clean      & 1116.96 & 1213.24 & 1295.97 & 944.93 & 1194.56 & 1344.40 & 1279.48 & 723.06 & 1303.98 \\
\textbf{Total} & Semi-noisy &  105.87 &  189.96 &  198.87 & 362.79 &  250.20 &  112.39 &  335.21 & 510.24 &  222.53 \\
\textbf{Total} & Noisy      &   76.57 &    6.73 &   90.12 & 390.86 &  226.23 &   72.74 &  199.94 & 405.29 &  148.99 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

Table~\ref{tab:duration_updated} summarizes dialect-wise audio durations across the Clean, Semi-noisy, and Noisy slabs for all nine languages. The corpus contains over 12,000 hours of read-speech audio spanning more than 20,000 sentences per language. Based on transcription quality and alignment confidence (see Section~\ref{subsubsec:audio_validation}), audio is grouped into three slabs: \textit{Clean}, \textit{Semi-noisy}, and \textit{Noisy}.

The collection goal was 200 hours of clean data per dialect for languages with five dialects (e.g., Hindi, Bengali, Kannada) and 250 hours per dialect for those with four dialects (e.g., Magahi, Marathi, Telugu). Most dialects met these targets, particularly in Bengali, Chhattisgarhi, Kannada, and Marathi. Some under-resourced dialects (e.g., Hindi D2, D4, and Maithili D2) fell short, requiring higher proportions of semi-noisy and noisy data to ensure sufficient coverage. These shortfalls likely reflect challenges in recruiting fluent readers in specific dialects due to literacy variation, regional accessibility, and dialectal overlap. For instance, Maithili and Hindi show lower clean-slab totals (723.06 and 944.93 hours, respectively) compared to other languages that exceed 1100 hours.

Across the full dataset, the clean slab totals 10,416.58 hours, semi-noisy 2,288.06 hours, and noisy 1,617.47 hours. The inclusion of noisy subsets captures real-world transcription variability and supports ASR training under practical conditions. This stratification balances dialectal coverage with data quality, enabling robust model evaluation across varying acoustic and transcription conditions.

\subsubsection{Signal-Level Audio Quality}
\begin{table}[th!]
\centering
\caption{Audio statistics per language including low SNR and speaking rate.}
\label{tab:audio_stats}
\begin{adjustbox}{width=0.6\textwidth}
\begin{tabular}{lrrrrrrr}
\toprule
\textbf{LID} & \textbf{\#Files} & \textbf{\#Low SNR} & \textbf{\%SNR} & \textbf{Wds/Aud} & \textbf{Dur (s)} & \textbf{WPM} \\
\midrule
bn & 870{,}793    & 3712 & 0.43 &  9 & 4.18 & 142.00 \\
bh & 866{,}619    & 4404 & 0.51 & 10 & 3.94 & 159.37 \\
ch & 823{,}803    & 1605 & 0.19 & 12 & 4.87 & 161.18 \\
hi & 756{,}886    & 1686 & 0.22 & 11 & 3.81 & 173.91 \\
kn & 744{,}617    & 1749 & 0.23 &  8 & 4.84 & 110.16 \\
mg & 968{,}365    & 2981 & 0.31 & 10 & 4.25 & 153.97 \\
mt & 518{,}504    & 1144 & 0.22 & 10 & 3.87 & 150.73 \\
mr & 1{,}002{,}599 & 2055 & 0.20 &  8 & 4.27 & 132.66 \\
te & 895{,}131    & 3051 & 0.34 &  8 & 4.40 & 117.16 \\
\bottomrule
\end{tabular}
\end{adjustbox}

\vspace{4pt}
\noindent
\textbf{Abbreviations:} LID = Language ID; \#Files = No. of audio files; \#Low SNR = No. of low-SNR files (SNR < 4 dB); \%SNR = Percentage of low-SNR files;  
Wds/Aud = Avg. words per audio; Dur (s) = Avg. duration in seconds; WPM = Words per minute.
\vspace{-5mm}
\end{table}

Table~\ref{tab:audio_stats} presents signal-level quality metrics for clean-slab data, including the number and proportion of low-SNR files, average words per audio, average duration, and speaking rate (words per minute). Each audio was trimmed using forced-alignment timestamps to remove leading and trailing silence or prompts. SNR was computed using the pre-trained FB-Denoiser~\cite{defossez2020real}, with 4 dB chosen empirically as the threshold for low-SNR classification. Speaking rate was calculated as the ratio of transcript word count to aligned duration.

Although contributors were instructed to record in quiet environments, the crowdsourced nature of data collection introduced acoustic diversity. The corpus includes 10,416 hours of clean, 2,288 hours of semi-noisy, and 1,617 hours of noisy audio, with fewer than 1\% of clean files classified as low-SNR.

\subsubsection{Speaker Metadata Validation}
\label{sec:spk_meta}

Speaker metadata quality was assessed using two validation checks: (1) intra-speaker and (2) inter-speaker consistency. The intra-speaker check identified discrepancies within a single speaker’s recordings, while the inter-speaker check detected potential overlaps between recordings assigned to different speaker IDs. To address these issues, we developed a bucketization algorithm validated on unseen data (see Appendix~\ref{appendix:speaker_bucket}). The algorithm successfully resolved 99.28\% of intra-speaker inconsistencies and 52.91\% of inter-speaker mismatches, providing a reliable measure of speaker identity consistency. After this validation, speakers without any discrepancies were selected for the development and test sets, ensuring no overlap across train, dev, and test splits (see Appendix \ref{appendix:train_test_split} for more details on train-dev-test splits).

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ASR PERFORMANCE
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Benchmarking ASR Performance}
\label{asr_performance}\vspace{-3mm}

\subsection{Datasets}
\begin{table}[t]
\centering
\caption{Train, development, and test set statistics for each language.}
\label{tab:split_stats}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|rrrr|rrrr|rrrr}
\toprule
\multirow{2}{*}{\textbf{LID}} & \multirow{2}{*}{\textbf{\#Dialects}} 
& \multicolumn{4}{c|}{\textbf{Train Set}} 
& \multicolumn{4}{c|}{\textbf{Dev Set}} 
& \multicolumn{4}{c}{\textbf{Test Set}} \\
\cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-14}
 & & Dur (h) & \#Utts & \#Sents & \#Spks 
   & Dur (h) & \#Utts & \#Sents & \#Spks 
   & Dur (h) & \#Utts & \#Sents & \#Spks \\
\midrule
bh & 3 & 142.98 & 95280 & 19056 & 1445 & 2.14 & 1500 & 575 & 60  & 3.10 & 2220 & 694 & 120 \\
bn & 5 & 142.96 & 85800 & 17160 & 1280 & 2.27 & 1500 & 494 & 100 & 3.26 & 2174 & 648 & 200 \\
ch & 4 & 175.22 & 85800 & 17160 & 1586 & 2.40 & 1413 & 511 & 80  & 3.85 & 2234 & 695 & 160 \\
hi & 5 & 128.47 & 85800 & 17160 & 2172 & 2.21 & 1539 & 722 & 100 & 3.30 & 2288 & 853 & 201 \\
kn & 5 & 164.83 & 85800 & 17160 & 1859 & 2.37 & 1430 & 518 & 100 & 3.61 & 2161 & 663 & 200 \\
mg & 4 & 157.77 & 95280 & 19056 & 1493 & 2.10 & 1431 & 494 & 80  & 3.17 & 2193 & 640 & 160 \\
mt & 4 & 159.32 & 95280 & 19056 & 1913 & 2.06 & 1409 & 693 & 80  & 3.33 & 2172 & 993 & 160 \\
mr & 4 & 140.49 & 95280 & 19056 & 2305 & 1.98 & 1386 & 509 & 80  & 3.04 & 2170 & 711 & 160 \\
te & 4 & 155.89 & 95280 & 19056 & 1848 & 2.30 & 1438 & 500 & 80  & 3.37 & 2226 & 652 & 160 \\
\bottomrule
\end{tabular}%
}
% \vspace{1mm}
\begin{minipage}{\textwidth}
\footnotesize
\textbf{LID}: Language ID, \textbf{\#Dialects}: number of dialects, \textbf{Dur}: duration in hours, 
\textbf{\#Utts}: number of utterances, \textbf{\#Sents}: number of unique sentences, \textbf{\#Spks}: number of speakers.
\end{minipage}\vspace{-5mm}
\end{table}

To enable reproducible research and fair comparison, we release standardized train, development, and test splits for all nine languages. Table~\ref{tab:split_stats} summarizes duration, utterances, unique sentences, and speakers. Each language contains 3--5 dialects and roughly 130--175 hours of training audio with 85k--95k utterances. The dev and test sets contain 2--4 hours each and up to 2.2k utterances from 60--200 speakers. The train set reported in Table~\ref{tab:split_stats} is the \textit{small} balanced subset of the \textit{clean} corpus used for all experiments in this paper. For \texttt{mt\_D2}, where clean audio was insufficient, a small portion of semi-noisy audio was included. Additional variants are provided in Appendix~\ref{appendix:train_test_split}.

All dev and test sets are drawn from the \textit{uncontaminated} speaker bucket (Section~\ref{sec:spk_meta}), ensuring no speaker overlap with training and preserving dialectal balance and sentence diversity across splits.

\subsection{Models}

We evaluate a range of ASR systems: (i) traditional models trained from scratch on RESPIN subsets, (ii) multilingual and SSL models pretrained on external data and used without RESPIN fine-tuning, and (iii) the same pretrained models fine-tuned on RESPIN. Concretely, we use TDNN-HMM (Kaldi) and an E-Branchformer CTC/attention system (ESPnet), Whisper models (Tiny, Base, Small), IndicWav2Vec, and two SPRING SSL models (Wav2Vec2 and Data2Vec-AQC).

\subsection{Experimental Setup}
\vspace{-2mm}

All experiments were run on a single NVIDIA RTX 3090 GPU (24\,GB).  

\textbf{Whisper} We fine-tune Tiny (39M), Base (74M), and Small (244M) variants using Hugging Face checkpoints with the Trainer API and early stopping on dev WER. Decoding conditions include the language ID.  

\textbf{Fairseq SSL} We fine-tune IndicWav2Vec\footnote{\url{https://github.com/AI4Bharat/IndicWav2Vec}}, SPRING-Wav2Vec2, and SPRING-Data2Vec-AQC\footnote{\url{https://asr.iitm.ac.in/models}} on RESPIN.  

\textbf{ESPnet} We train an \texttt{e\_branchformer} encoder (8 blocks, 256 hidden), CTC/attention criterion, Adam optimizer, SpecAugment, AMP, and early stopping on dev CER.  

\textbf{Kaldi} We train TDNN-HMM using the chain recipe with 40-dim MFCCs, i-vectors, speed and volume perturbation, and a trigram LM trained on RESPIN transcripts.

All training recipes and checkpoints are available at \url{https://github.com/labspire/respin_baselines}.

\subsection{Results and discussion}
\begin{table}[t]
\centering
\caption{CER and WER (\%) for different models across languages. \textbf{Pretrained models} refer to models fine-tuned on publicly available data other than RESPIN. \textbf{Traditional models} are trained from scratch on RESPIN. \textbf{Fine-tuned models} are pretrained SSL or Whisper models further fine-tuned on a subset of RESPIN. For SeamlessM4T-v2-Large, \textbf{bh}, \textbf{ch}, and \textbf{mg}, and for the pretrained SSL models, \textbf{bh}, \textbf{ch}, \textbf{mg}, and \textbf{mt} are evaluated using Hindi-tuned models.}\vspace{3pt}
\label{tab:baseline_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrrrrrrrrrrrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{Model}} 
& \multicolumn{10}{c}{\textbf{CER (\%)}} 
& \multicolumn{10}{c}{\textbf{WER (\%)}} \\
\cmidrule(lr){2-11} \cmidrule(lr){12-21}
& \textbf{bh} & \textbf{bn} & \textbf{ch} & \textbf{hi} & \textbf{kn} & \textbf{mg} & \textbf{mr} & \textbf{mt} & \textbf{te} & \textbf{avg}
& \textbf{bh} & \textbf{bn} & \textbf{ch} & \textbf{hi} & \textbf{kn} & \textbf{mg} & \textbf{mr} & \textbf{mt} & \textbf{te} & \textbf{avg} \\
\midrule

\rowcolor{gray!10}
\multicolumn{21}{l}{\textbf{Pretrained Models (fine-tuned on non-RESPIN public data)}} \\
SeamlessM4T-v2-Large       & 29.09 & 17.54 & 33.20 & 15.34 & 18.91 & 30.07 & 14.44 & 27.15 & 14.33 & 22.23 & 56.77 & 45.56 & 71.86 & 25.43 & 55.38 & 56.49 & 42.09 & 66.64 & 46.11 & 51.81 \\
IndicW2V                  & 17.08 & 14.27 & 22.77 & 11.02 & 10.37 & 19.64 & 15.09 & 23.30 & 8.61  & 15.80 & 51.61 & 42.83 & 65.98 & 28.34 & 42.37 & 54.32 & 53.91 & 66.10 & 37.82 & 49.25 \\
SPRING-W2V2                & 15.10 & 12.50 & 20.81 & 8.80  & 11.43 & 16.35 & 7.56  & 20.12 & 6.97  & 13.29 & 41.32 & 25.93 & 55.42 & 22.99 & 44.35 & 42.09 & 34.15 & 53.69 & 36.32 & 39.58 \\
SPRING-Data2Vec-AQC        & 15.02 & 11.94 & 21.26 & 7.20  & 10.78 & 15.81 & 7.49  & 19.91 & 6.53  & 12.88 & 42.35 & 23.69 & 56.17 & 20.93 & 42.79 & 42.47 & 33.40 & 53.65 & 33.98 & 38.83 \\
\addlinespace

\rowcolor{gray!10}
\multicolumn{21}{l}{\textbf{Traditional Models (trained from scratch on RESPIN subset)}} \\
TDNN-HMM                        & 5.67  & 5.22  & 4.45  & 3.25  & 4.88  & 7.69  & 3.30  & 6.53  & 3.94  & 4.99  & 17.57 & 16.87 & 12.69 & 8.72  & 23.01 & 22.33 & 13.40 & 20.13 & 20.81 & 17.28 \\
E-Branchformer                  & 4.95  & 4.33  & 3.63  & 3.52  & 4.62  & 6.68  & 3.19  & 5.75  & 3.97  & 4.52  & 15.21 & 14.96 & 10.59 & 9.94  & 24.50 & 20.38 & 14.48 & 17.95 & 21.64 & 16.63 \\
\addlinespace

\rowcolor{gray!10}
\multicolumn{21}{l}{\textbf{Fine-tuned Models (fine-tuned on RESPIN subset)}} \\
Whisper-Tiny                    & 9.62  & 11.60 & 7.13  & 9.69  & 12.62 & 13.98 & 9.15  & 10.73 & 11.43 & 10.66 & 27.45 & 32.51 & 20.81 & 21.71 & 48.54 & 36.40 & 30.93 & 31.96 & 41.61 & 32.44 \\
Whisper-Base                    & 7.15  & 7.69  & 5.36  & 5.80  & 8.10  & 10.44 & 6.23  & 7.51  & 7.51  & 7.31  & 22.51 & 24.71 & 16.67 & 15.19 & 36.52 & 30.54 & 24.28 & 24.80 & 32.99 & 25.36 \\
Whisper-Small                   & 7.90  & 5.46  & 3.85  & 4.16  & 6.00  & 7.46  & 3.93  & 5.94  & 6.54  & 5.69  & 19.02 & 18.91 & 12.36 & 11.78 & 29.66 & 23.94 & 16.95 & 20.28 & 27.82 & 20.08 \\
IndicW2V                   & 4.42  & 4.28  & 3.24  & 3.16  & 4.68  & 6.02  & 3.19  & 5.19  & 4.54  & 4.30  & 16.07 & 16.65 & 11.36 & 10.47 & 24.86 & 21.51 & 15.13 & 19.19 & 24.03 & 17.69 \\
SPRING-W2V2                & 3.92  & 3.86  & 2.99  & 2.37  & 4.30  & 5.20  & 2.49  & 4.37  & 3.85  & 3.71  & 14.61 & 15.12 & 10.74 & 8.22  & 23.90 & 19.40 & 12.75 & 16.64 & 21.92 & 15.92 \\
SPRING-Data2Vec-AQC        & 3.95  & 3.63  & 2.84  & 2.27  & 4.11  & 4.98  & 2.38  & 4.30  & 3.72  & 3.58  & 14.84 & 14.15 & 10.25 & 7.91  & 23.13 & 18.50 & 12.28 & 16.41 & 21.17 & 15.40 \\

\bottomrule
\end{tabular}
}\vspace{-5mm}
\end{table}

Table~\ref{tab:baseline_results} reports CER and WER for nine languages. The results highlight the value of dialect-aware supervision.

\textbf{Pretrained models without RESPIN supervision} Models trained only on external data, such as SeamlessM4T-v2-Large, IndicWav2Vec (pretrained), and SPRING-Wav2Vec2 (pretrained), show high error rates for several languages, especially those with strong dialectal variation such as Bhojpuri and Chhattisgarhi. This gap reflects domain and dialect mismatch.

\textbf{Training from scratch on RESPIN} Traditional systems trained solely on RESPIN subsets outperform the above. E-Branchformer achieves an average WER of 16.63\%, underscoring the benefit of dialect-specific supervision even without large-scale pretraining.

\textbf{Whisper fine-tuning} Fine-tuned Whisper models improve over their pretrained counterparts but generally remain behind scratch-trained E-Branchformer, indicating limited adaptation to dialectal nuances.

\textbf{Fine-tuned SSL models} SSL models fine-tuned on RESPIN perform best overall. SPRING-Data2Vec-AQC attains the lowest average WER (15.40\%), and SPRING-Wav2Vec2 is consistently strong, showing that SSL pretraining combined with dialect-aware fine-tuning is effective for multi-dialect ASR.

\textbf{Generalization to public test sets} We also evaluate on CommonVoice, FLEURS, GramVaani, IndicTTS, Kathbath, and MUCS for \texttt{bn}, \texttt{hi}, \texttt{kn}, \texttt{mr}, and \texttt{te}. Pretrained models are slightly stronger on these non-domain-specific sets, yet RESPIN-fine-tuned SSL models remain competitive. Full results are provided in Appendix~\ref{appendix:asr_performance}.

\section{Applications, Impact, and Limitations}
\label{sec:applications}

RESPIN-S1.0 has been actively used in community challenges and research benchmarks. Over the past two years, subsets of the corpus have supported multiple workshops, challenges, and research efforts. A Bengali and Bhojpuri subset was used in the SLT Code Hackathon 2022 to build dialect-aware ASR systems. The first Multi-Dialect ASR Challenge (MADASR) was organized at ASRU 2023~\cite{Udupa_ASRU23,Tanel_ASRU23} using RESPIN data for Bengali and Bhojpuri. The ongoing MADASR 2.0 Challenge at ASRU 2025 expands this to 1,200 hours across eight languages (\texttt{bh}, \texttt{bn}, \texttt{ch}, \texttt{kn}, \texttt{mg}, \texttt{mr}, \texttt{mt}, \texttt{te}), enabling large-scale benchmarking of dialect-aware ASR systems. RESPIN has also been used for dialect identification across eight Indian languages~\cite{Amartyaveer_ICASSP25,kumar25_interspeech}. Beyond ASR, the corpus facilitates research in language and dialect identification (LID/DID), unsupervised speech translation, and other multilingual speech-language processing tasks. Its focus on agriculture and finance provides valuable coverage of socially relevant domains, particularly for underrepresented Indian languages.

Despite its scope, RESPIN-S1.0 has certain limitations. The current release includes only read speech, whereas spontaneous and conversational data are more reflective of real-world communication. Its domain coverage is limited to agriculture and finance, and future expansions into healthcare, education, and governance would enhance applicability. Finally, the reliance on literate native speakers with smartphone access may underrepresent marginalized communities. Nonetheless, RESPIN establishes a strong foundation for inclusive, dialect-rich ASR development in India, and future releases will expand linguistic coverage and include spontaneous speech to address these limitations.

\section{Conclusion and Future Work}

In this work, we introduced \textbf{RESPIN-S1.0}, a large-scale, dialect-rich speech corpus spanning nine Indian languages and two socially relevant domains---agriculture and finance. By integrating dialectal, phonetic, and demographic diversity at scale, RESPIN establishes a unified benchmark for automatic speech recognition (ASR) and related speech-language processing tasks in low-resource, multilingual settings. The corpus is accompanied by standardized train--development--test splits, dialect-aware lexicons, detailed metadata, and multiple ASR baselines to enable transparent and reproducible research. Beyond improving ASR performance across dialects, RESPIN-S1.0 provides a foundation for systematic research in dialect identification, multilingual speech translation, and cross-domain adaptation. Future releases will expand coverage to additional domains such as healthcare, education, and governance, and incorporate spontaneous and conversational speech. The dataset will also include new dialects and languages and will introduce open benchmark suites for dialectal ASR and DID evaluation. We further plan to explore integration with large multilingual and self-supervised models to advance inclusive speech technologies for Indian languages. Through open data, transparent benchmarks, and continued community collaboration, RESPIN aims to accelerate equitable speech technology development across India's diverse linguistic landscape.


\section*{Acknowledgements}

This work was supported by the Gates Foundation. We thank the numerous NGOs, volunteers, and contributors who participated in data collection, validation, and community outreach, both online and offline. Their collaboration and commitment were instrumental in realizing RESPIN-S1.0.

% \section*{References}
\bibliographystyle{unsrt}
\bibliography{respin_references}

\newpage
\appendix

\section*{Appendices}

\section{Language and Dialect Selection Process}
\label{appendix:lang_dial}
This appendix describes the systematic procedure used to select representative dialects for nine Indian languages (Bhojpuri, Magahi, Maithili, Bengali, Kannada, Chhattisgarhi, Telugu, Marathi, and Hindi). Our four-step approach targeted, for each language, the identification of 3--5 dialects that jointly cover \mbox{70--90\%} of native speakers while ensuring linguistic diversity. Selections were validated by expert linguists to confirm coverage and representativeness of the chosen dialects and districts.

\paragraph{Step 1: Literature survey}
We compiled major dialects per language and mapped their geographic distribution, identifying the core districts where each dialect is spoken.

\paragraph{Step 2: Dialect selection}
We chose dialects that (i) together cover 70--90\% of native speakers and (ii) are distinctive in structure/lexicon/phonology while being sufficiently resourced for collection.

\paragraph{Step 3: District selection}
For each dialect, we prioritized districts reflecting the standard/local norm of the dialect, minimizing overlap with other dialect regions to avoid duplicate speakers, and considering operational feasibility.

\paragraph{Step 4: Expert validation}
Language experts/linguists reviewed and validated the dialect--district choices for coverage and representativeness.

\begin{table}[htbp]
\vspace{-1cm}
\centering
\caption{Language-wise Dialect and Districts Selected}
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.2}
\begin{adjustbox}{max width=\linewidth,max height=0.9\textheight}
\begin{tabular}{|l|l|p{3.5cm}|p{4cm}|}
\hline
\textbf{Language} & \textbf{Dialect} & \textbf{Core Districts} & \textbf{Justification} \\ \hline

% Bhojpuri-Maithili Section
\multirow{3}{*}{Bhojpuri} & Southern Standard & Saran & Representative of standard variety \\ \cline{2-4} 
& Northern & East Champaran, Deoria & Captures sub-varieties in Bihar and UP \\ \cline{2-4} 
& Western & Varanasi & Major urban center for Western dialect \\ \hline

\multirow{4}{*}{Magahi} & Central Variety & Gaya, Jahanabad & Representative of standard Magahi \\ \cline{2-4} 
& Southern & Jamui, Lakhisaray, Nawada & Distinct from central variety \\ \cline{2-4} 
& Western & Vaishali & Shows Bhojpuri influence \\ \cline{2-4} 
& NE (Surjapuri) & Kishanganj, Purnia, Katihar & Distinct northern variety \\ \hline

\multirow{4}{*}{Maithili} & Standard (Sotipura) & Darbhanga, Madhubani & Representative of standard variety \\ \cline{2-4} 
& Bajjika & Samastipur, Saharsa & Distinct morphological features \\ \cline{2-4} 
& Eastern (Thēthi) & Araria, Madhepura & Eastern variety with distinct features
 \\ \cline{2-4} 
& Angika & Bhagalpur & Originally classified under Magahi \\ \hline

% Bengali-Kannada Section
\multirow{5}{*}{Bengali} & Western & Purba/Paschim Medinipur & Shows Odia influence \\ \cline{2-4} 
& Varendri/Pundra & Malda (Core), Dakshin Dinajpur & Northern variety \\ \cline{2-4} 
& Rajbangsi & Jalpaiguri, Cooch Behar & Distinct northern dialect \\ \cline{2-4} 
& Jharkhandi & Purulia, Bankura & Variety spoken in Jharkhand region \\ \cline{2-4} 
& Standard & Kolkata, Nadia/Hooghly & Standard variety \\ \hline

\multirow{5}{*}{Kannada} & Hyderabad & Bellary & Formerly classified as Central \& Hyderabad Karnataka \\ \cline{2-4} 
& Mangalore & Dakshin Kannada (Mangalore) & Coastal variety \\ \cline{2-4} 
& Dharwad & Dharwad, Uttar Kannada & North Western variety\\ \cline{2-4} 
& NE & Gulbarga & Shows strong Urdu influence \\ \cline{2-4} 
& Mysore & Mysore Rural, Mandya & Southern standard variety \\ \hline

% Other Languages Section
\multirow{4}{*}{Chhattisgarhi} & Kedri (Central) & Bilaspur, Durg & Central standard variety \\ \cline{2-4} 
& Utti (Eastern) & Raigarh & Eastern variety \\ \cline{2-4} 
& Budati/Khatahi & Kabirdham, Balaghat & Western variety \\ \cline{2-4} 
& Bhandar & Sarguja & Northern variety \\ \hline

\multirow{4}{*}{Telugu} & Mid-Coastal & Guntur, Krishna & Central variety \\ \cline{2-4} 
& Rayalseema & Chittoor, Anantpur & Southern variety \\ \cline{2-4} 
& Telangana & Karimnagar, Nalgonda & Northern variety \\ \cline{2-4} 
& Utterandhra & Vishakapattanam, Srikakulam & Eastern variety \\ \hline

\multirow{4}{*}{Marathi} & S Konkan & Sindhudurga & Coastal south \\ \cline{2-4} 
& N Konkan & Dhule, Nashik & Coastal north \\ \cline{2-4} 
& Varhadi & Nagpur Rural & Eastern variety \\ \cline{2-4} 
& Standard & Pune Rural & Standard variety \\ \hline

\multirow{5}{*}{Hindi} & Hindustani+Malvi & Muzaffarnagar & Phonological similarities \\ \cline{2-4} 
& Kannauji+Braj & Etah & Transitional district with speakers of both dialects \\ \cline{2-4} 
& Awadhi+Bundeli & Hamirpur & Transitional district with speakers of both dialects
 \\ \cline{2-4} 
& Marwari+Dhundhari & Nagaur & Phonological similarities \\ \cline{2-4} 
& Garhwali & Tehri Garhwal & Distinct variety requiring separate collection \\ \hline
\end{tabular}
\end{adjustbox}
\label{tab:dialect_selected}
\end{table}


Based on the dialect selection criteria mentioned above, Table~\ref{tab:dialect_selected} lists the selected dialects along with their core districts and the rationale for inclusion, illustrating how each contributes to the overall dialectal diversity of the corpus. Key considerations during the finalisation of dialects are summarised below:

\begin{itemize}
    \item \textbf{Bhojpuri:} Nagpuri was excluded as it is now recognized as a separate language.
    \item \textbf{Magahi:} The Eastern Magahi cluster was excluded due to internal variation, while Maithili-mixed Magahi (``Angika'') was reclassified under Maithili.
    \item \textbf{Maithili:} Bajjika and Angika were retained, though both could also be classified as separate languages.
    \item \textbf{Bengali:} Eastern and south-eastern dialects were excluded since they are primarily spoken in Bangladesh.
    \item \textbf{Chhattisgarhi:} The Rakshahun (Southern) dialect was excluded due to its smaller speaker population.
    \item \textbf{Marathi:} Zadi Boli was excluded due to operational challenges in data collection.
    \item \textbf{Hindi:} Owing to the large number of Hindi dialects (50+ according to the 2011 Census), accent-based variation was prioritized over fine-grained dialectal distinctions to ensure coverage of 70--90\% of Hindi speakers.
\end{itemize}

\section{Text Data Preparation and Validation Pipeline}
\label{appendix:text_validation}

The text data collection was carried out in two distinct phases. In \textbf{Phase~1}, a minimum of 5,000 sentences per language were collected, consisting primarily of interrogative sentences. In \textbf{Phase~2}, an additional $\sim$15,000 sentences were gathered, resulting in at least 20,000 sentences per language. These sentences were uniformly distributed across dialects and domains—\textit{agriculture} and \textit{finance}—and included all major sentence types, with a particular focus on maximizing dialectal coverage.

\subsection{Phase 1: Initial Collection and Validation}

\subsubsection{Domain Identification and Topic Mapping}
To ensure domain relevance, data were systematically collected from the agriculture and finance sectors following a structured topic-selection process. The finalized topics informed the creation of domain-specific Google Forms, each containing a standardized set of questions in the nine target languages. This process involved comprehensive market research to identify existing agricultural and financial service providers, cataloging the features and data structures of current applications, analyzing products relevant to the Indian market, and evaluating mobile and web applications designed for low-literacy users. The final step included assessing interaction topics and advisory systems to identify potential real-world deployment scenarios for conversational agents derived from the collected data.

Lexical resources were developed in parallel through keyword mining, identification of topic clusters within the Indian agricultural and financial ecosystems, and creation of semantic categorization frameworks to organize the collected terminology. These frameworks guided the design of the Google Forms. Table~\ref{tab:agricultural_categories} and Table~\ref{tab:financial_categories} list the subtopics finalized for the agriculture and finance domains, respectively.

\input{Styles/tables/topics_agri}
\input{Styles/tables/topics_finance}

\subsubsection{Text Collection from Domain Experts}
Dialect-specific domain experts contributed text data through standardized Google Forms. Each submission was reviewed by the Validation Team before being approved for the voice-collection phase. Figure~\ref{fig:nt_form_comp} shows a sample form for the subtopic ``Climate and Weather'' in Hindi.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{Styles/figures_appendix/NT_composition_form.jpg}
    \caption{Sample Google Form used for Phase~1 text data collection.}
    \label{fig:nt_form_comp}
\end{figure}

\subsubsection{Text Validation and Sanitization}
All text submissions underwent a rigorous multi-stage validation pipeline. Only linguistically verified entries advanced to the voice-collection stage.

\paragraph{Step~1: Preliminary Linguistic Validation.}
Submissions were first screened for linguistic quality. Those not meeting the required standards were rejected with feedback. Accepted entries were passed to the detailed sentence validation and sanitization process. The authenticity check performed by language resource managers ensured that sentences were domain-relevant, vocabulary-rich, and conversationally natural, representing realistic expert–user communication. A parallel linguistic diversity check by computational linguists verified variation in terminology, dialectal coverage across regional and socio-geographic varieties, demographic balance in contributors (age, literacy, gender), and documentation of sociolinguistic patterns relevant to ASR development.

\paragraph{Step~2: Systematic Validation Protocol.}
Before validation began, linguistic consultants (LCs) received detailed orientation on the data-collection methodology, form structure, domain-specific terminology, and expected response formats. During initial review, validators identified common issues such as non-interrogative responses, script inconsistencies, repeated submissions, and transliteration errors. For each Google Form, LCs then conducted systematic verification to produce grammatically correct regional-language sentences paired with English translations. This included verifying question formats, removing off-domain or duplicate entries, checking dialect authenticity, and normalizing grammar, spelling, and orthography. Ambiguous cases were flagged for expert resolution.  

Quality assurance followed through cross-verification of validator corrections, comparison between manual and automated translations, documentation of recurring error patterns to improve the pipeline, and compensation based on validated submissions.

\paragraph{Step~3: Text Sanitization and Corpus Standardization.}
After validation, the corpus underwent further cleaning and normalization. Texts were organized by domain, dialect, and language, and assigned to dialect-specific consultants for standardization. The process ensured consistent punctuation, capitalization, and formatting; normalized numerical expressions, abbreviations, and units; and re-categorized cross-domain sentences under the most appropriate subtopics. Transliteration management was implemented to maintain consistency for non-native terms, separate inflectional morphology from borrowed words, and document multiple transliteration variants for ambiguous cases.

Phonological completeness was ensured by comparing the monophone, diphone, and triphone distributions against language reference data. Expert linguists verified these distributions and proposed augmentations where coverage was insufficient. Additional sentences were then composed to fill phonological and lexical gaps, ensuring balanced representation across dialects. The resulting corpus thus achieved uniform linguistic quality, phonological coverage, and dialectal diversity across all languages.

Table~\ref{tab:data_entries_val} presents two Hindi examples from the Marwari region within the agricultural domain, as reviewed and corrected by the Validation Team.

\begin{table}[htbp]
\centering
\caption{ Illustration of Text data validation}
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|>{\raggedright}p{1.2cm}|c|c|>{\raggedright}p{1.8cm}|>{\raggedright\arraybackslash}p{2.8cm}|>{\raggedright\arraybackslash}p{2.8cm}|>{\raggedright\arraybackslash}p{2.5cm}|}
\hline
\textbf{DE Name} & \textbf{Pin Code} & \textbf{District} & \textbf{Feature Category} & \textbf{Sentence provided by DE} & \textbf{Corrected sentence in Hindi by LC} & \textbf{Translation provided by LC} \\ \hline
XXXX & 341319 & Nagaur & 
1 - Crop Names \& Seasons & 
\texthindi{अगेती रबी मौसम के अंतर्गत कौन से महीने आते हैं?} & 
\texthindi{अगेती रबी मौसम के अंतर्गत कौनसे महीने आते हैं?} & 
Which months fall under early rabi season? \\ \hline
XXXX & 341319 & Nagaur & 
2 - Seeds, Varieties \& Hybrids & 
\texthindi{मक्का की सबसे अच्छी उपज वाली किस्म कोनसी है?} & 
\texthindi{मक्का की सबसे अच्छी उपज वाली किस्म कौनसी है?} & 
Best yielding variety of maize? \\ \hline
\end{tabular}
\end{adjustbox}
\label{tab:data_entries_val}
\end{table}


We programmed scripts to automatically tag issues, helping the Validation Team with their preliminary screening. Table\ref{tab:data_entries_autotag} shows an example for a single sentence to illustrate this process. In the actual validation workflow, the items generated for each field in every sentence are presented as columns in a Google Sheet.
In the example provided in the table, the script tagged the following:
\begin{enumerate}
    \item Special symbol: “?”
    \item Abbreviation: “HP”
    \item Numeric translation: “\texthindi{आठ}” (meaning “eight”)
    \item Letter transliteration: “\texthindi{के}” (this could be the English letter “K” or the Hindi postposition “\texthindi{के}”)
\end{enumerate}
The data underwent multiple rounds of automatic issue tagging, followed by manual corrections by language consultants and a subsequent multi-stage review by the Validation Team.

\begin{table}[htbp]
\centering
\caption{Illustration of automatic tagging of text for text data validation}
\footnotesize
\renewcommand{\arraystretch}{1.5} % Increase row height
\setlength{\tabcolsep}{8pt} % Increase cell padding
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|l|p{4cm}|p{4cm}|}
\hline 
\textbf{Field Type} & \textbf{Example Sentence 1} & \textbf{Example Sentence 2} \\ \hline

LC Sentences & 
\texthindi{अगेती रबी मौसम के अंतर्गत कौनसे महीने आते हैं ?} & 
\texthindi{आठ} H.P \texthindi{की मोटर चलाने के लिए बिजली का कौनसा कनेक्शन लेना होगा ?} \\ \hline

Transliteration & 
ageetii rabii mousam kee atargat koun see mahiinee aatee hei & 
aaṭh eecapii kii moṭar calaanee kee liee bijalii kaa kounasaa kaneekśan leenaa hogaa \\ \hline

Translations & 
Which months come under the early rabi season? & 
Which connectivity of electricity we should use to run eight H.P motor? \\ \hline

Special Symbols & 
{[}'?'{]} & 
{[}'.', '?'{]} \\ \hline

Acronyms & 
{[}{]} & 
{[}{]} \\ \hline

Roman Numerals & 
{[}{]} & 
{[}{]} \\ \hline

Alphanumerics & 
{[}{]} & 
{[}{]} \\ \hline

Numerics & 
{[}{]} & 
{[}{]} \\ \hline

English Words & 
{[}{]} & 
{[}{]} \\ \hline

Abbreviations & 
{[}{]} & 
{[}H.P{]} \\ \hline

Letters & 
{[}{]} & 
{[}{]} \\ \hline

Letter Transliteration & 
{[}'\texthindi{के}'{]} & 
{[}'\texthindi{के}'{]} \\ \hline

Numeric Translation & 
{[}{]} & 
{[}'\texthindi{आठ}'{]} \\ \hline

Non Whitespaces & 
{[}{]} & 
{[}{]} \\ \hline
\end{tabular}
\end{adjustbox}

\label{tab:data_entries_autotag}
\end{table}

\subsection{Phase 2 — Corpus Expansion and Dialect Balancing}

\subsubsection{Domain and Topic Coverage}
Phase~2 focused on large-scale corpus expansion and dialectal balancing through a more diverse and comprehensive data collection approach. The process began with curating an extensive list of topics across the agriculture and finance domains. This initiative was designed to assist contributors—especially those unfamiliar with specific subjects—by offering structured prompts and reference material. The goal was to ensure exhaustive domain coverage, leaving no subtopic unexplored.

Topic compilation was performed manually using a variety of sources, including magazines, Wikipedia, specialized websites, and relevant academic and research literature. Magazines sourced from online archives, educational institutions, and research organizations enriched topic breadth, while the Wikipedia “Outline” pages provided structured hierarchies of subtopics and relevant reference links. The final list comprised approximately 1,500 topics, each linked to corresponding online references for contextual understanding. Compared to Phase~1, this represented nearly a 100-fold increase in topic diversity, enabling far greater linguistic and contextual variety. The process began with broad domain segmentation (e.g., agriculture and finance) and progressed toward finer granularity—from crop-specific topics such as sugarcane cultivation to technological and financial subtopics like UPI PIN setup or transaction history retrieval in digital banking applications.

\subsubsection{Sentence Preparation and Composition}
The primary source of data in Phase~2 was the manual composition of sentences by trained language experts referencing the curated topic lists. In cases where the target of approximately 15,000 sentences per language was not achieved, supplementary strategies were adopted. These included web scraping for high-resource languages, translation from other dialects within the same language, and cross-language translation where appropriate.

Digital text resources in standard written forms, while voluminous, often lacked colloquial authenticity or dialectal variation. To address this, the collection strategy intentionally prioritized contributions from native speakers of specific districts. Engaging these individuals allowed the corpus to capture locally grounded expressions and speech patterns, thus ensuring linguistic authenticity and alignment with real-world spoken communication. This approach not only improved representativeness but also deepened the understanding of regional linguistic and cultural diversity.

Given the curated topics, sentence composers were instructed to generate conversational and colloquial examples tailored for Automatic Speech Recognition (ASR) model development. Managing dialectal variability was challenging, as linguistic differences could vary significantly even within a 5–10~km radius. However, this diversity was embraced as an advantage rather than a limitation. By accommodating regional variation and the absence of standardized orthography in several dialects, the corpus design intentionally reflected the natural heterogeneity of Indian speech. The resulting data thus embodies both linguistic flexibility and broad coverage.

To maintain uniformity and usability, contributors followed a standardized set of rules during composition:

\begin{enumerate}
    \item \textbf{Character Length Limit:} Sentences should not exceed 120 characters to maintain readability and consistency.
    \item \textbf{Avoid Pronoun Start:} Sentences should not begin with pronouns to ensure contextual coherence, as each sentence is treated independently.
    \item \textbf{Numerical Representation:} All numbers must be written in words to enhance readability and prevent ambiguity.
    \item \textbf{Special Characters:} Only full stops (.), commas (,), and question marks (?) are permitted; other special characters are disallowed for uniform formatting.
    \item \textbf{Avoid Controversial Statements:} Content must remain neutral and non-political to ensure objectivity.
    \item \textbf{Topic Adherence:} Each sentence must remain relevant to the assigned topic.
    \item \textbf{Domain Balance:} Equal representation of agriculture and finance topics must be maintained across the dataset.
    \item \textbf{Acronym Formatting:} Acronyms should follow a consistent format, such as A.T.M., ensuring clarity and uniform representation.
\end{enumerate}

Adhering to these guidelines ensured a cohesive, balanced, and linguistically standardized text corpus suitable for ASR training and dialectal analysis.

\paragraph{Translation Strategy.}
To achieve the target of 15,000 sentences per language, a portion of the corpus was created via translation by expert linguists. Source sentences were drawn either from the same language’s standard dialect or from another linguistically related language. These translations preserved semantic meaning while incorporating dialect-specific vocabulary and stylistic features, enhancing intra- and inter-language diversity.

\subsubsection{Text Corpus Validation}
\label{appendix:text_val}
After sentence composition, the entire dataset underwent a comprehensive validation pipeline consisting of multiple automated and manual checks performed by language validators. Since sentences were produced by several contributors, inevitable inconsistencies and typographical deviations were corrected to ensure the text’s suitability as stimuli for the crowd-sourced speech recording interface. Sentences required to be accurate, unambiguous, coherent, and compatible with the recording application, making this validation pipeline a critical stage of quality control.

The pipeline architecture remained consistent across languages, with minor language-specific adaptations. Validation proceeded through successive rounds, each producing a versioned release of the dataset. This approach allowed traceability and rollbacks when required. Independent checks were executed within a single version, while dependent checks were performed sequentially.

The major categories of validation checks are summarized below:

\begin{enumerate}
    \item \textbf{Duplicate Sentence Removal (Automatic):} A pairwise Word Error Rate (WER) analysis was applied to the raw corpus to identify and remove duplicate sentences, reducing redundancy before manual review.
    
    \item \textbf{Invalid Character Check and Correction (Manual):} Non-printable and redundant whitespace characters were eliminated. Validators reviewed the character inventory of the corpus and manually corrected sentences containing non-language symbols (excluding allowed punctuation marks: comma, full stop, and question mark). The process was iterated until only valid characters remained.
    
    \item \textbf{Sentence Length Pruning (Manual):} Due to recording-interface constraints, sentences exceeding 90 characters were either pruned or rejected by validators to maintain compatibility with UI display limits.
    
    \item \textbf{Acronym Standardization (Manual):} Acronyms were required to follow the “A.B.C.” format. Tokens containing full stops were extracted and validated to confirm correct acronym usage. Non-standard acronyms, including transliterated English terms, were flagged and reformatted.
    
    \item \textbf{Invalid Matra Check and Correction (Manual):} Words containing redundant or incorrect matra usage—such as consecutive matras or visually overlapping diacritics—were flagged and manually corrected to ensure orthographic accuracy.
    
    \item \textbf{Interchangeable Character Correction (Manual):} Validators referenced a curated list of commonly confused characters. Words containing such letters were manually reviewed for potential spelling errors and corrected as needed.
    
    \item \textbf{Similar Sentence Check (Manual):} Near-duplicate sentence pairs with $0 < \text{WER} < 0.3$ were identified and manually reviewed. Validators retained, corrected, or removed variants depending on linguistic relevance.
    
    \item \textbf{Homophone Check (Manual):} Using phonetic transcriptions provided by Navana Tech, phonetic WER was computed to detect homophones. Validators examined flagged pairs for potential spelling or pronunciation inconsistencies and corrected them.
    
    \item \textbf{Language-Specific Checks (Manual):} Additional validations were implemented for particular languages to handle unique orthographic, script-level, or dialectal issues. Details of these custom checks are available in the language-specific corpus documentation.
\end{enumerate}

Following validation, each version of the corpus was archived to maintain a complete record of revisions. Table~\ref{tab:sentence_statistics} provides statistics for the Phase~2 text corpus, including the total number of sentences by domain (agriculture and finance), their method of creation (composition or translation), and the number and distribution of dialect experts involved. Table~\ref{tab:text_stats} presents additional dialect-level statistics, including total samples, Phase~1 contributions, vocabulary sizes, and average sentence lengths.

\input{Styles/tables/text_sources}
\input{Styles/tables/text_stats}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Phonetic Lexicon Construction}
\label{appendix:lexicon_creation}

To generate the pronunciation lexicon, we developed a deterministic, rule-based grapheme-to-phoneme (G2P) conversion pipeline following the sound label set creation process defined in the Indian Language Speech-sound Label set (ILSL)~\cite{Samudravijaya2021}. This tagset-based process ensures consistent transcription of consonants, vowels, diacritics, and prosodic markers across all Indic languages.

\subsection{1. Text Processing}
\begin{enumerate}[label*=\arabic*.]
    \item Remove punctuation and extraneous symbols from the input text.
    \item Normalize graphemes by mapping each Unicode character to a reduced “base grapheme” set aligned with ILSL’s consonant and vowel categories. This normalization step removes visual variants while preserving phonetic distinctions.
\end{enumerate}

\subsection{2. G2P Conversion Pipeline}
\begin{enumerate}[label*=\arabic*.]
    \item \textbf{Basic Conversion Rules:}
    \begin{enumerate}[label*=\arabic*.]
        \item Each grapheme or grapheme pair is converted into its corresponding ILSL sound label.
        \item \textit{Examples:}
        \begin{enumerate}[label*=\arabic*.]
            \item A consonant plus vowel, such as क + ा, becomes “k + aa”.
            \item A consonant with a halant (्) drops its default vowel and joins the next consonant, e.g., क + ष → “k + sh”.
        \end{enumerate}
    \end{enumerate}
    \item \textbf{Special Pronunciation Patterns:}
    \begin{enumerate}[label*=\arabic*.]
        \item \textit{Schwa deletion:} Removes the unstressed “अ” sound where it is not pronounced, which is common in Indo-Aryan languages.
        \item \textit{Nasalization and gemination:} Correctly handle \textit{anusvāra} (nasal sounds) and doubled consonants to preserve pronunciation accuracy.
    \end{enumerate}
\end{enumerate}

\subsection{3. Rule Ordering and Language Overrides}
\begin{enumerate}[label*=\arabic*.]
    \item General phonological rules that apply across all Indic scripts are implemented first.
    \item Language-specific overrides are applied subsequently to handle exceptions such as irregular spellings or orthographic variations, ensuring the overall G2P conversion remains efficient and scalable.
\end{enumerate}

This rule-based framework enables consistent phonetic lexicon generation across multiple scripts and dialects while allowing flexibility for language-specific adjustments. The resulting lexicons were validated manually for a subset of entries to ensure alignment with native pronunciation norms.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage
\section{Audio Recording and Quality Validation}
\label{appendix:audio_data}
\subsection{Participant Assignment and Recording Workflow}

Voice data providers are referred to as \textbf{Voice Participants (VPs)}. Before contributing, they are screened by \textbf{Preliminary Audio Validators}, who compare recordings against text prompts to check for audio quality and reading accuracy. Participants failing to meet quality standards are removed early from the process.

The validation workflow parallels that of text validation, with a key distinction: \textbf{Language Consultants (LCs)} undergo rigorous screening, interviews, and specialized training. They are provided with custom audio validation tools and integrated into a continuous training and feedback loop before handling audio validation tasks.

\paragraph{Voice Participant Criteria}
\begin{itemize}
\item \textbf{Geographic Authenticity}: Must reside in the district or village where the target dialect is natively spoken.
\item \textbf{Literacy}: Must be able to read and speak the dialect.
\item \textbf{Smartphone Proficiency}: Able to navigate the collection app and record sentences.
\item \textbf{Demographic Compliance}: Must be over 18 years old and meet predefined age-gender quotas.
\end{itemize}

\paragraph{Preliminary Audio Validator Criteria}
Validators assess submission quality and influence payment decisions. Each validator must possess:
\begin{itemize}
\item Native-level dialectal fluency
\item Training in phonetic error detection
\item Strong text-audio alignment skills
\item Proficiency with smartphones
\item High concentration and consistency
\item Excellent auditory skills
\item Familiarity with regional accent variations
\end{itemize}

\subsection{Validation and Payment Pipeline}

\paragraph{System Components}
\begin{enumerate}
    \item WhatsApp-based participant authentication
    \item Mobile application (\textit{Bolo App}) for guided speech recordings
    \item Manual and hybrid validation pipelines
    \item Cloud-based backend for task management and payments
\end{enumerate}

\subsubsection{Participant Authentication via WhatsApp}

A dedicated WhatsApp bot, \textbf{Bolo Code Bot}, served as the authentication interface for registering and onboarding voice participants (VPs). Prior to participation, phone numbers were submitted by regional partners and stored in a secure database. Each verified participant received a unique 16-digit access code via WhatsApp, which was required to log in to the Bolo App. Unregistered users attempting to access the app received an error message and were redirected to the respective project coordinator. 

The bot supported multilingual instructions to ensure accessibility across linguistic backgrounds. Once authenticated, participants received onboarding material including installation links, demo videos, and recording guidelines. This pre-screening ensured that only vetted users satisfying dialectal and demographic criteria could contribute recordings, thereby enhancing data quality and project security.

\subsubsection{Data Collection Process}

The voice collection pipeline was executed through regional partners responsible for recruiting, training, and supervising contributors. Once authenticated via WhatsApp, each participant accessed the \textit{Bolo App} for guided recording. Submitted utterances were automatically uploaded to a centralized cloud backend, where they were processed through validation pipelines for technical quality and transcript alignment. Participant compensation was computed based on the count of validated recordings.

Each language corpus targeted approximately 1152 hours of audio distributed across five categories. These comprised 556 hours of phonetically balanced sentences (\textbf{Phase-2}), 556 hours of domain-specific content (\textbf{Phase-1}), and 10 hours each of shared agricultural and banking-domain prompts recorded by all speakers. An additional 22 hours were allocated to spontaneous prompts designed to elicit natural prosody and conversational style. Table~\ref{tab:task-split} summarizes the per-language and per-dialect task allocation. For instance, languages with five dialects contributed $\sim$111 hours per dialect per phase, whereas those with three dialects contributed $\sim$185 hours.

\begin{table}[htbp]
\centering
\caption{Dialect-wise task distribution per language.}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Task Type} & \textbf{Total (h)} & \textbf{3 Dialects} & \textbf{4 Dialects} & \textbf{5 Dialects} \\\hline
Phase-2 (Phonetic) & 556 & 185.33 & 139.00 & 111.20 \\\hline
Phase-1 (Domain) & 556 & 185.33 & 139.00 & 111.20 \\\hline
Common Agri & 10 & 3.33 & 2.50 & 2.00 \\\hline
Common Bank & 10 & 3.33 & 2.50 & 2.00 \\\hline
Spontaneous & 22 & 7.33 & 5.50 & 4.40 \\\hline
\end{tabular}
\label{tab:task-split}
\end{table}

Each participant was assigned 279 Phase-2 and 278 Phase-1 sentences, plus 5 common agricultural and 5 common banking prompts shared across all speakers to maximize speaker diversity. For spontaneous speech, each speaker responded to 10 open-ended prompts (e.g., “Describe a festival in your area,” “What local dishes do you usually eat?”). While intended to elicit natural speech, some participants read the questions aloud; such instances were excluded from the public release.

\subsection{Bolo App Workflow}

\paragraph{Authentication and Profile Setup}

Participants authenticated into the \textit{Bolo App} via:
\begin{enumerate}
    \item OTP-based mobile verification,
    \item Entry of the 16-digit access code issued by the WhatsApp bot, and
    \item Acceptance of the privacy policy.
\end{enumerate}

They then completed a brief profile setup collecting optional photo, gender, year of birth, and pincode (for dialect verification). These steps ensured demographic balance and traceability without compromising anonymity.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{Styles/figures_appendix/NT_Bolo_App_Screens_P2.png}
\caption{\textbf{(a)} NT Bolo App homescreen and task interface. 
The homescreen displays the number of available, completed, and verified speech tasks for each domain.
Participants can refresh assigned tasks, monitor submission progress, and track validation status directly within the app. 
Each task card represents a text-prompt set, color-coded by completion state, enabling efficient management of recordings across multiple phases and domains.}
\label{fig:bolo_home}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{Styles/figures_appendix/NT_Bolo_App_Screens_P3.png}
\caption{\textbf{(b)} Payment and verification workflow in the NT Bolo App. 
Screens illustrate the full post-recording process: account registration via bank or UPI, automatic verification of account validity, credit confirmation with a unique transaction reference (UTR), and display of balance and transaction history on the contributor dashboard. 
This integrated payment interface ensured transparent, traceable compensation and streamlined validation feedback to participants.}
\label{fig:bolo_payment}
\end{figure}

\paragraph{Recording Interface}

The recording screen presented categorized sentence lists with waveform visualization and playback controls to ensure recording quality before submission. Tasks were color-coded by status (available, submitted, or verified), and progress summaries encouraged participant completion.

\paragraph{Audio Validation and Payment}

All recordings were subjected to a hybrid validation pipeline comprising both manual and automated review stages. Manual validation was performed on a 5 \% random sample of each dataset and checked for:
\begin{itemize}
    \item Accuracy of audio–text alignment,
    \item Absence of excessive background noise, and
    \item Sufficient speech volume and clarity.
\end{itemize}

The automated stage used a neural acoustic model to compute phoneme- and word-level match scores, producing a quality score per utterance. Recordings were categorized into three slabs:
\begin{itemize}
    \item \textbf{Clean}: $\sim$5 \% mismatch,
    \item \textbf{Semi-noisy}: $\sim$12 \% mismatch,
    \item \textbf{Noisy}: $\ge$15 \% mismatch.
\end{itemize}

Languages such as Bengali and Bhojpuri exhibited slightly higher baseline mismatch rates due to orthographic complexity. Slab scoring, validation logic, and pseudocode are detailed in Appendices \ref{subsec:slab_score}–\ref{subsec:slab_pseudocode}.  

Following successful validation, participant payments were automatically computed and disbursed through either direct bank transfer or UPI. Figures \ref{fig:bolo_home}–\ref{fig:bolo_payment} summarize the end-to-end participant workflow, from recording to verified payment, implemented within the NT Bolo ecosystem.


% \subsection{Voice Data Collection Framework}

% \paragraph{System Components}
% \begin{enumerate}
% \item WhatsApp-based authentication
% \item Mobile app ("Bolo App") for guided recordings
% \item Manual and hybrid validation pipelines
% \item Cloud-based backend and task management
% \end{enumerate}

% \subsubsection{Participant Authentication via WhatsApp}

% A dedicated WhatsApp bot, \textbf{Bolo Code Bot}, served as the primary interface for authenticating voice participants (see Figure~\ref{fig:whatsapp-auth}). Prior to participation, phone numbers were submitted by field partners and registered in a secure database. Only these pre-registered users received a unique 16-digit access code via WhatsApp, enabling them to log in to the Bolo application. If an unregistered user attempted access, the bot returned an error and redirected them to the project coordinator.

% The authentication system supported multilingual communication to ensure accessibility for participants across various language backgrounds. Once authenticated, participants received onboarding materials including app installation instructions and video tutorials demonstrating the recording workflow. This gatekeeping mechanism ensured that only vetted participants who met dialectal and demographic criteria could proceed to record speech data, thereby enhancing both the security and quality of the collected corpus.

% \begin{figure}[htbp]
% \centering
% \begin{tabular}{cc}
% \begin{subfigure}[b]{0.45\linewidth}
%     \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image1.jpg}
%     \caption{Failure scenario: unregistered user receives no code.}
% \end{subfigure} &
% \begin{subfigure}[b]{0.45\linewidth}
%     \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image2.jpg}
%     \caption{Success scenario: registered user receives a code.}
% \end{subfigure}
% \end{tabular}
% \caption{WhatsApp-based authentication via Bolo Code Bot.}
% \label{fig:whatsapp-auth}
% \end{figure}


% \subsubsection{Data Collection Process}

% The voice data collection pipeline involved close collaboration with regional partners, who were responsible for identifying, training, and onboarding voice participants (VPs). Once registered, participants were authenticated using the WhatsApp-based Bolo Code Bot and were granted access to the Bolo mobile application for guided speech recording. The submitted recordings were processed through a server-side validation system to assess both technical quality and alignment with the provided text. Compensation for each participant was calculated based on the number of validated contributions.

% Each language corpus was designed to comprise 1152 hours of audio, distributed across five task categories. These included 556 hours of phonetically balanced sentences under Phase-2 and 556 hours of domain-specific content categorized under Phase-1. An additional 10 hours each were dedicated to a small set of common agricultural and common banking domain sentences. These sentences were designed to be spoken by a larger number of participants in order to capture broader speaker variation within the corpus. The final 22 hours were allocated to spontaneous speech tasks intended to elicit natural prosody and informal speaking styles.

% The dialect-wise distribution of hours is summarized in Table~\ref{tab:task-split}. In cases where the language included five dialects, each dialect contributed approximately 111.20 hours per phase, whereas languages with three dialects allocated about 185.33 hours per dialect per phase.

% \begin{table}[htbp]
% \centering
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Task Type} & \textbf{Total} & \textbf{3 Dialects} & \textbf{4 Dialects} & \textbf{5 Dialects} \\\hline
% Phase-2 & 556 & 185.33 & 139.00 & 111.20 \\\hline
% Phase-1 & 556 & 185.33 & 139.00 & 111.20 \\\hline
% Common Agri & 10 & 3.33 & 2.50 & 2.00 \\\hline
% Common Bank & 10 & 3.33 & 2.50 & 2.00 \\\hline
% Spontaneous & 22 & 7.33 & 5.50 & 4.40 \\\hline
% \end{tabular}
% \caption{Dialect-wise task distribution per language.}
% \label{tab:task-split}
% \end{table}

% Each participant was assigned 279 Phase-2 sentences and 278 Phase-1 sentences, along with five common agricultural and five common banking domain sentences. These common prompts were shared across all participants and served to increase the number of unique speakers per sentence, thereby enhancing speaker diversity in the dataset. 

% In addition to read speech, participants were presented with 10 open-ended prompts selected from a pool of 20 to encourage spontaneous speech. Example prompts included: “What local dishes do you typically eat in a day?”, “Can you describe a well-known festival celebrated in your area?”, “What did you like or dislike about this project?”, “What stories do you narrate to children at bedtime?”, and “How has your neighborhood changed in the last ten years?”

% Despite the intent to collect free-form, conversational speech, many participants ended up reading the questions aloud rather than responding naturally. Consequently, this subset of the data did not capture the desired spontaneous characteristics and was not included in the open-source release of the corpus.

% \subsection{Bolo App Workflow}

% \paragraph{Authentication Process}
% As shown in Figures~\ref{fig:bolo-auth1} and~\ref{fig:bolo-auth2}, the app required:
% \begin{enumerate}
% \item OTP verification of mobile number
% \item Entry of 16-digit access code from WhatsApp bot
% \item Acceptance of the privacy policy
% \end{enumerate}

% % \begin{figure}
% %     \centering
% %     \includegraphics[width=\linewidth]{Styles/figures_appendix/NT_Bolo_App_Screens_P2.png}
% %     \caption{Caption}
% %     \label{fig:placeholder}
% % \end{figure}

% % \begin{figure}
% %     \centering
% %     \includegraphics[width=\linewidth]{Styles/figures_appendix/NT_Bolo_App_Screens_P3.png}
% %     \caption{Caption}
% %     \label{fig:placeholder}
% % \end{figure}

% \begin{figure}[htbp]
% \centering
% \resizebox{0.65\linewidth}{!}{%
% \begin{tabular}{cc}
% \includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image3.jpg} &
% \includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image4.jpg}
% \end{tabular}%
% }
% \caption{Bolo App: Mobile number and OTP verification screens.}
% \label{fig:bolo-auth1}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \resizebox{0.65\linewidth}{!}{%
% \begin{tabular}{cc}
% \includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image5.jpg} &
% \includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image6.jpg}
% \end{tabular}%
% }
% \caption{Bolo App: Access code entry and privacy policy acceptance.}
% \label{fig:bolo-auth2}
% \end{figure}

% \paragraph{Participant Metadata Collection}

% Figures~\ref{fig:bolo-profile1} and~\ref{fig:bolo-profile2} show profile setup screens that collected:
% \begin{itemize}
% \item Optional profile photo
% \item Gender
% \item Year of birth
% \item Pincode for dialect verification
% \end{itemize}

% \begin{figure}[htbp]
% \centering
% \resizebox{0.65\linewidth}{!}{%
% \begin{tabular}{cc}
% \includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image7.jpg} &
% \includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image8.jpg}
% \end{tabular}%
% }
% \caption{Bolo App: Profile photo and gender selection.}
% \label{fig:bolo-profile1}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \resizebox{0.65\linewidth}{!}{%
% \begin{tabular}{cc}
% \includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image9.jpg} &
% \includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image10.jpg}
% \end{tabular}%
% }
% \caption{Bolo App: Year of birth and pincode entry.}
% \label{fig:bolo-profile2}
% \end{figure}

% \paragraph{Recording Interface}

% The app presented categorized tasks, tracked progress, and supported features such as waveform display and playback (see Figure~\ref{fig:bolo-recording}).

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.4\linewidth]{Styles/figures_appendix/audio_image11.jpg}
% \caption{Bolo App: Guided recording interface with waveform and playback.}
% \label{fig:bolo-recording}
% \end{figure}

% \subsection{Audio Validation Pipeline}

% \paragraph{Manual Validation (5\% sample)}

% As shown in Figure~\ref{fig:bolo-validation}, manually validated recordings were reviewed for:
% \begin{itemize}
% \item Audio-text alignment
% \item Volume and background noise
% \item Overall clarity
% \end{itemize}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.4\linewidth]{Styles/figures_appendix/audio_image12.jpg}
% \caption{App-based manual audio validation interface.}
% \label{fig:bolo-validation}
% \end{figure}

% \paragraph{Hybrid Manual-Automated Validation}

% Recordings were bucketed into slabs based on quality:
% \begin{itemize}
% \item \textbf{Clean}: $\sim$5\% error
% \item \textbf{Semi-noisy}: $\sim$12\% error
% \item \textbf{Noisy}: ≥15\% error
% \end{itemize}

% Bengali and Bhojpuri had slightly higher baseline error in all slabs.

% \textbf{Error Definition}: A 5\% error rate means $\sim$5 mismatches in 100 audio-text pairs.

% A neural acoustic model computed phone- and word-level match scores, with full methodology described in Section~\ref{subsec:slab_score}. Multi-stage validation and slab pseudocode are detailed in Sections~\ref{subsec:slab_valid} and~\ref{subsec:slab_pseudocode}, respectively.

% The task management platform, integrated with cloud storage, enabled both manual and hybrid validation across stages.








% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Audio Quality Categorization and Thresholding}
\label{appendix:slab_def}

This section describes the comprehensive process used to categorize utterances into three quality slabs: \textbf{Clean}, \textbf{Seminoisy}, and \textbf{Noisy}. The slab assignment is driven by automatic scoring models and verified through manual validation across multiple stages.

\subsection{Score Computation for Slab Assignment}
\label{subsec:slab_score}

To begin, consider a target dialect $Z$ and its corresponding validation model $V_{Z,O}$, where $O$ indicates the model iteration.

Let $D = [U, W, P]$ denote the dataset for dialect $Z$:
\begin{itemize}
    \item $U = [u_1, u_2, ..., u_N]$: the set of $N$ utterances.
    \item $W = [W_1, ..., W_N]$: word-level sentence prompts, where each $W_i = [W_{i1}, ..., W_{iJ}]$.
    \item $P = [P_1, ..., P_N]$: phone-level transcriptions. Each $P_i = [P_{i1}, ..., P_{iK}]$ is derived from $W_i$ using a pronunciation lexicon.
\end{itemize}

For each utterance $u_i$:
\begin{enumerate}[label=\alph*.]
    \item Extract acoustic feature sequence $F_i = [F_{i0}, ..., F_{iT}]$.
    \item Pass $(W_i, P_i, F_i)$ into $V_{Z,O}$ for alignment.
    \item The model produces alignment-based scores:
    \[
    S_{wi} = [S_{wi1}, ..., S_{wiJ}], \quad S_{pi} = [S_{pi1}, ..., S_{piK}]
    \]
    representing word- and phone-level pronunciation quality.
    \item Apply a statistical aggregation function $Y$ to get the utterance score:
    \[
    SO_i = Y[S_{wi}, S_{pi}]
    \]
\end{enumerate}

\subsection{Manual Validation Workflow}
\label{subsec:slab_valid}

Manual validation ensures the reliability of score-based slab thresholds. The process is led by a multi-tiered team comprising Language Resource Managers (LRMs), Senior LRMs, and Dialect-Specific Language Consultants (LCs).

\subsubsection*{1. Bin Selection and Task Delegation}
\begin{itemize}
    \item The LRM selects $L$ score bins (width 0.02) and randomly chooses $M$ utterances from each bin.
    \item The Senior LRM reviews and approves the bin set.
    \item The validated bins are then assigned to LCs for manual annotation.
\end{itemize}

\subsubsection*{2. Manual Annotation by Language Consultants}
\begin{itemize}
    \item LCs receive the validation model ID, bin range, and $M$ utterances per bin.
    \item Each sample is labeled as \texttt{MATCH} or \texttt{MISMATCH}:
    \begin{itemize}
        \item \texttt{MATCH}: Audio is correct or has minor phonetic variations.
        \item \texttt{MISMATCH}: Audio is incorrect, has insertions/deletions, or meaning-altering substitutions.
    \end{itemize}
    \item Difficult cases are resolved through discussions with LRMs or scientists.
\end{itemize}

\subsubsection*{3. Validation Portal Process}
LCs perform validation using a structured interface:
\begin{enumerate}[label=\roman*.]
    \item Login and model selection (VZO).
    \item Input score interval and number of samples ($M$).
    \item Review each utterance with playback and transcription.
    \item Submit final MATCH/MISMATCH label.
\end{enumerate}

\subsubsection*{4. Review Mechanism}
LRMs and Senior LRMs:
\begin{itemize}
    \item Access all LC decisions with filters.
    \item Overrule incorrect labels with justifications.
    \item Coordinate feedback loops for quality control.
\end{itemize}

\textbf{Note:}
\begin{itemize}
    \item LCs are native dialect speakers.
    \item LRMs and Senior LRMs are experienced linguists or domain experts.
\end{itemize}

\subsection{Slab Generation Pseudocode}
\label{subsec:slab_pseudocode}

The slab generation involves multiple stages of automatic scoring and manual validation:

\begin{enumerate}
    \item \textbf{Initialization:}
    \begin{itemize}
        \item Dialect $Z$ has $N$ utterances scored using $V_{Z,O}$ as described earlier.
        \item All $SO_i$ scores are ranked. The range covering 90\%+ of data is binned into $L$ intervals of width 0.02.
    \end{itemize}

    \item \textbf{Voice Validation Dataset Creation:}
    \begin{itemize}
        \item Sample $M$ utterances from each bin.
        \item Manually annotate these using the LC process.
    \end{itemize}

    \item \textbf{Analysis-I:}
    \begin{itemize}
        \item For each bin $l$, compute $P_l = \%$ of MATCH labels.
        \item Identify thresholds:
        \[
        SO_{\text{clean}}: P_l > 0.95, \quad SO_{\text{seminoisy}}: P_l > 0.88
        \]
    \end{itemize}

    \item \textbf{Additional Validation (Optional):}
    \begin{itemize}
        \item Validate $K$ bins before/after identified thresholds.
        \item Update scores, rerun Analysis-I.
    \end{itemize}

    \item \textbf{Analysis-II with Refined Model:}
    \begin{itemize}
        \item Compute updated scores $SN_i$ for all $N$ utterances using refined model $V_{Z,N}$.
        \item Repeat binning, annotation, and thresholding steps to refine:
        \[
        SN_{\text{clean}}, \quad SN_{\text{seminoisy}}
        \]
    \end{itemize}

    \item \textbf{Final Slab Assignment:}
    \begin{itemize}
        \item Label each utterance:
        \[
        \text{Clean: } SN_i > SN_{\text{clean}}, \quad
        \text{Seminoisy: } SN_{\text{seminoisy}} < SN_i \leq SN_{\text{clean}}, 
        \]
        \[
        \text{Noisy: } SN_i \leq SN_{\text{seminoisy}}
        \]

    \end{itemize}
\end{enumerate}

In summary, the slab generation process combines automatic scoring with expert manual validation to ensure that speech quality annotations are both reliable and reproducible. This categorization is crucial for downstream training, evaluation, and dataset release pipelines.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Speaker Metadata Validation and Consistency Checks}
\label{appendix:speaker_bucket}

In crowdsourced speech data collection frameworks, accurate speaker metadata is crucial but difficult to guarantee. Among various metadata types, speaker identity plays a pivotal role in training robust models and enabling tasks like speaker adaptation and verification. Errors in speaker IDs broadly fall into two categories:

\textbf{Intra-Speaker ID Errors:} A single speaker is assigned multiple distinct speaker IDs.

\textbf{Inter-Speaker ID Errors:} Multiple speakers are incorrectly assigned the same speaker ID.

Figure~\ref{fig:spk_id_issue} illustrates both these error types using simplified speaker ID relationships.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/speaker_id_issue.png}
    \caption{Illustration of intra-speaker and inter-speaker ID inconsistencies}
    \label{fig:spk_id_issue}
\end{figure}

\subsection{Speaker Meta-Data Error Prevalence and Degree}

To assess the prevalence of speaker ID inconsistencies, we extracted speaker embeddings using the pretrained SpeechBrain TDNN model\footnote{\url{https://huggingface.co/speechbrain/spkrec-xvect-voxceleb}}. For intra-speaker ID analysis, cosine similarity was computed across all utterances with the same speaker label. For inter-speaker ID errors, similarity was measured between utterances from different speaker IDs within the same district.

The distributions of cosine similarities for intra- and inter-speaker pairs in Bengali are shown in Figure~\ref{fig:cosine_similarity}. Based on manual inspection across cosine similarity intervals, we empirically identified a threshold of 0.92. Utterance pairs with cosine similarity below 0.90 are likely to be affected by intra-speaker error, while inter-speaker error is suspected for similarity values above 0.92.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/cosine_similarity.pdf}
    \caption{Cosine similarity distributions for intra- and inter-speaker embeddings in Bengali}
    \label{fig:cosine_similarity}
\end{figure}

\subsection{Speaker Meta-Information Recovery via Multi-Stage Clustering}

To correct speaker labeling inconsistencies, we propose a multi-stage clustering framework consisting of calibrated threshold estimation, intra- and inter-speaker clustering, post-processing, and validation. An overview is shown in Figure~\ref{fig:bucketization}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Bucketization.pdf}
    \caption{Overview of the speaker clustering and correction (bucketization) pipeline}
    \label{fig:bucketization}
\end{figure}

\subsubsection{Dialect-Specific Calibration}

We computed dialect-specific similarity thresholds using an Equal Error Rate (EER)-based calibration method. For each dialect, we curated a speaker verification set with 500 speakers (4 samples each) and 50 speakers (20 samples each), yielding 20k trials equally divided into genuine and impostor pairs. The similarity threshold at EER was adopted for clustering.

Speakers for calibration were selected to maximize demographic diversity (gender, age). Manual validation ensured reliability of these calibration samples.

\subsubsection{Intra-Speaker Clustering Methodology}

Given a potentially inconsistent speaker label, we clustered the associated utterances using a similarity threshold:
\begin{enumerate}
    \item A random utterance is chosen as the seed.
    \item Utterances exceeding the threshold are clustered.
    \item A centroid is computed for the cluster.
    \item The cluster is refined using the centroid similarity.
    \item Clustered utterances are removed; process repeats.
\end{enumerate}

To address over-fragmentation, post-clustering merges were performed if cross-cluster centroid and utterance similarities both exceeded the threshold.

A confidence scoring mechanism was implemented using centroid similarity, intra-cluster consistency, and post-validation cross-similarity scores. Low-confidence utterances were filtered to improve cluster purity.

Gender metadata was also integrated to flag inconsistencies. Clusters with mixed-gender predictions were reviewed or split.

\subsubsection{Inter-Speaker Clustering Framework}

To merge clusters representing the same speaker across speaker labels:
\begin{enumerate}
    \item Compute centroid similarities between different speaker clusters.
    \item If similar, sample utterances from both clusters and compute pairwise similarities.
    \item Merge clusters only if both conditions are satisfied.
\end{enumerate}

A hierarchical verification protocol was adopted to ensure reliability:
\begin{enumerate}
    \item Initial inter-speaker cluster proposals
    \item Re-run intra-clustering on merged sets
    \item Filter by confidence score
    \item Manual review of borderline cases
\end{enumerate}

\subsection{Sampling Uncontaminated Speakers for Dev and Test Sets}

Following speaker clustering and verification, we compiled a list of \emph{uncontaminated speakers}—those not involved in any intra- or inter-speaker ID errors. This uncontaminated speaker pool formed the basis for dev and test set sampling, ensuring no overlap with the training data. This separation is critical to maintain dataset partition integrity and enables reliable benchmarking.

\subsection{Testing the Bucketization Algorithm}

To evaluate the effectiveness of our clustering-based bucketization, we designed a synthetic blind test corpus. This test set was constructed by combining samples from standard and internal Indian speech corpora with ground-truth speaker labels. New synthetic speaker labels were introduced to simulate known intra- and inter-speaker ID errors.

Figure~\ref{fig:bucketization_test} outlines the creation of this test corpus.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Bucktezisation_Test.pdf}
    \caption{Creating the blind test corpus by simulating intra and inter speaker label errors}
    \label{fig:bucketization_test}
\end{figure}

Figure~\ref{fig:bucketization_comp} shows the application of the bucketization algorithm to recover correct speaker IDs.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth,trim={0cm 0 0 2cm},clip]{figures/Bucketization_test_composition.pdf}
    \caption{Inference of bucketization algorithm on blind test set}
    \label{fig:bucketization_comp}
\end{figure}

\textbf{Intra-Speaker Evaluation:} Alignment score is used to measure the purity of predicted speaker clusters.

\begin{algorithm}[H]
    \caption{Calculating Alignment Score}
    \label{intra_align_score}
    \For{each bucket in Blind Test Corpus}{
        Create a confusion matrix (cm) between predicted and ground-truth speakers\;
        Initialize alignment score to 0\;\\
        \While{cm is not empty}{
            Find the maximum value in cm\;\\
            Delete the row and column corresponding to the maximum value\;\\
            Increment alignment score by the maximum value\;
        }
        Normalize the alignment score\;
    }
\end{algorithm}

\textbf{Intra-Speaker Evaluation:} To evaluate the ability of the bucketization algorithm to resolve intra-speaker ID errors, we compute an \textit{alignment score} for each speaker bucket. This metric quantifies how well the predicted speaker clusters align with the ground-truth speaker labels.

Figure~\ref{fig:bucketization_intra_eval} illustrates the evaluation workflow for intra-speaker analysis. The alignment score is computed using the confusion matrix between predicted and actual speaker labels, as formalized in Algorithm~\ref{intra_align_score}. Higher alignment scores indicate better cluster purity and minimal fragmentation.

Figure~\ref{fig:bucketization_eval} (left) shows a sample confusion matrix for one speaker cluster, where the maximum alignment path is highlighted. The algorithm achieved an average alignment score of \textbf{99.28\%}, confirming that intra-speaker identity inconsistencies were almost completely resolved.

\textbf{Inter-Speaker Evaluation:} For inter-speaker errors, where multiple labels are used for the same speaker, the algorithm proposes pairs of speaker clusters to merge. We evaluate this using \textit{Intersection over Union (IoU)} between the sets of ground-truth speaker IDs represented in each proposed cluster pair.

The evaluation process is illustrated in Figure~\ref{fig:bucketization_inter_eval}, while Figure~\ref{fig:bucketization_eval} (right) depicts how IoU is calculated between the overlapping speaker identity sets. The algorithm achieved an average inter-speaker IoU of \textbf{52.91\%}, suggesting moderate success in merging duplicated speaker identities. The lower IoU relative to the alignment score indicates that resolving inter-speaker ID inconsistencies remains more challenging.

\textbf{Final Evaluation Summary:} Overall performance is summarized in Figure~\ref{fig:bucketization_res}, which visualizes intra and inter evaluation metrics across the test corpus. Although intra-speaker resolution was highly accurate, approximately \textbf{10\% of utterances} in the blind test corpus remained unassignable due to confidence score thresholds or conflicting metadata.

\begin{figure}[htbp]
    \begin{minipage}{.45\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/bucketization_test_eval.pdf}
    \caption{Blind Test Intra-Speaker Evaluation}
    \label{fig:bucketization_intra_eval}
    \end{minipage}
    \hspace{1cm}
    \begin{minipage}{.45\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Bucketization_Inter_speaker_eval.pdf}
    \caption{Blind Test Inter-Speaker Evaluation}
    \label{fig:bucketization_inter_eval}
    \end{minipage}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Bucketization_Evaluation_Metrics.pdf}
    \caption{Evaluation metrics: (left) Confusion matrix alignment for intra-speaker analysis; (right) IoU for inter-speaker cluster overlap}
    \label{fig:bucketization_eval}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Bucketization_Eval_Res.pdf}
    \caption{Bucketization performance results across the test corpus}
    \label{fig:bucketization_res}
\end{figure}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Dev, Test and Train Sets Creation}

% \subsection{Dev and test sets creation}
% Creating the dev and test sets from the dataset $D_{Z}$ of Dialect Z involves careful selection of speakers and utterances, ensuring diversity and avoiding speaker contamination between sets. The dataset $D_{Z}$ consists of audio files categorized into three slabs: $A_{c}$ (\textit{clean} slab), $A_{sn}$ (\textit{seminoisy} slab), and $A_{n}$ (\textit{noisy} slab), as detailed in Appendix-A1. The audio files are divided into dev, test, and train sets, ensuring no overlap between the sets regarding speakers and text IDs.

% As the dev and test sets are relatively small compared to the train set and need to be balanced across dialects, domains and speakers, we first create the dev and test sets, keeping all the parameters balanced to the extent possible. The following pseudocode summarizes the key steps in creating the dev, test sets:

% \begin{algorithm}[H]
% \SetAlgoLined
% \caption{Test Set and Dev Set Creation}
% \begin{algorithmic}[1]

% \State \textbf{Initialize:}
% \State Define $A$ as the total number of audio files in $D_{Z}$.
% \State Define subsets $A_{c}$, $A_{sn}$, and $A_{n}$ corresponding to the three slabs (see Section~\ref{sec:slab_def} for slab definitions).
% \State Define $S$ as the total number of speakers, and $S_{uncont}$ as the set of uncontaminated speaker IDs (see Section~\ref{appendix:speaker_bucket} for contamination criteria).
% \State Let max\_rep represent the maximum allowed repetitions of any text ID across the test and dev sets.

% \State \textbf{Diversity Constraints:}
% \State Maintain balance across task (Question, Statement), domain (Banking, Agriculture), and gender (male, female).
% \State Ensure minimal loss of utterances during test and dev set creation.
    
% \end{algorithmic}
% \end{algorithm}

% \subsubsection{Test Set Creation}
% The test set is created by filtering out uncontaminated speakers and selecting utterances from $A_{c}$ for each speaker. Gender diversity is ensured by forming two subsets: $S_{ms}$ (sampled male speakers) and $S_{fs}$ (sampled female speakers). For each selected speaker, one utterance from $A_{c}$ is chosen for each combination of task (Question/Statement) and domain (Banking/Agriculture). After selection, all utterances containing the selected text IDs are removed once max\_rep is reached.

% Once the selection is complete, a diversity analysis is performed to ensure that the balance between tasks, domains, and genders is maintained. The test set $A_{test}$ is finalized if the diversity constraints are satisfied.

% \begin{algorithm}[H]
% \caption{\textbf{Test Set Creation:}}
% \begin{algorithmic}[1]
% \State Filter speakers based on $S_{\textit{uncont}}$.
% \State Select $A_c$ from $S_{\textit{uncont}}$ for test set creation.
% \State Form subsets $S_{ms}$ and $S_{fs}$, ensuring balance in tasks and domains.
% \State \textbf{for} each selected speaker per gender \textbf{do}
% \State \hspace{1em} Select a single $A_c$ utterance for each of the following:
% \State \hspace{2em} Task: Question, Domain: Agriculture
% \State \hspace{2em} Task: Statement, Domain: Agriculture
% \State \hspace{2em} Task: Question, Domain: Banking
% \State \hspace{2em} Task: Statement, Domain: Banking
% \State \hspace{1em} Delete all utterances with text IDs selected \texttt{max\_rep} times.
% \State \textbf{end for}
% \State Perform diversity analysis. If constraints are met, freeze the test set $A_{\textit{test}}$.
% \end{algorithmic}
% \end{algorithm}

% \subsubsection{Dev Set Creation}

% The dev set is created similarly to the test set but with the additional condition that no speaker or text ID from the test set is included in the dev set. Speakers are selected similarly, ensuring gender balance and diversity analysis are performed to confirm that the dev set satisfies the required constraints.

% \begin{algorithm}[H]
% \caption{\textbf{Dev Set Creation:}}
% \begin{algorithmic}[1]
% \State Filter speakers based on $S_{\textit{uncont}}$, excluding those from $A_{\textit{test}}$.
% \State Select $A_{\textit{slab1}}$ from $S_{\textit{uncont}}$ for dev set creation.
% \State Form subsets $S_{ms}$ and $S_{fs}$, ensuring balance in tasks and domains.
% \State \textbf{for} each selected speaker per gender \textbf{do}
% \State \hspace{1em} Select a single $A_c$ utterance for each of the following:
% \State \hspace{2em} Task: Question, Domain: Agriculture
% \State \hspace{2em} Task: Statement, Domain: Agriculture
% \State \hspace{2em} Task: Question, Domain: Banking
% \State \hspace{2em} Task: Statement, Domain: Banking
% \State \hspace{1em} Delete all utterances with text IDs selected \texttt{max\_rep} times.
% \State \textbf{end for}
% \State Perform diversity analysis. If constraints are met, freeze the dev set $A_{\textit{dev}}$.
% \end{algorithmic}
% \end{algorithm}


% \subsection{Training Set Creation}

% The training set is created by removing all speakers and text IDs present in the test and dev sets. A diversity analysis ensures that the distribution of sentences, domains, and genders in the training set is not skewed compared to the original dataset $D_{Z}$. Once the diversity constraints are validated, the training set $A_{train}$ is finalized.

% \begin{algorithm}[H]
% \SetAlgoLined
% \begin{algorithmic}[1]

% \State \textbf{Training Set Creation:}
% \State Filter out utterances belonging to any speaker or text ID from $A_{test}$ and $A_{dev}$.
% \State Perform diversity analysis to verify that the training set $A_{train}$ reflects the diversity of $D_{Z}$.
% \State Freeze the training set $A_{train}$.

% \end{algorithmic}

% \end{algorithm}

% \subsection{Handling High Loss Dialects}

% For dialects with high loss or failure to meet diversity constraints, such as Mt\_D2, Hi\_D2, and Hi\_D4, a similar process is followed. However, in these cases, both $A_{slab1}$ and $A_{slab0.5}$ are used, in addition to uncontaminated speakers, to ensure the creation of valid test and dev sets.

% \begin{algorithm}[H]
% \SetAlgoLined
% \begin{algorithmic}[1]

% \State \textbf{Sampling for High Loss Dialects:}
% \State Follow the same steps as for the test and dev sets, but include both $A_{slab1}$ and $A_{slab0.5}$ for speaker selection.
    
% \end{algorithmic}
% \end{algorithm}

% Furthermore, to provide the RESPIN corpora to a wider audience, we have also open-sourced a smaller version of the training set for each of the 9 languages. These sets are referred to as \textit{train\_$<$lang$>$\_small}.

% \begin{algorithm}[H]
% \caption{Balanced Sampling of Text IDs Across Dialects and Domains}
% \label{alg:balanced_sampling}
% \textbf{Input:} 
% \begin{itemize}
%     \item $\mathcal{T}$: Set of all text IDs
%     \item $\mathcal{D}_{\text{did}}$: Set of dialect IDs
%     \item $\mathcal{D}_{\text{dom}}$: Set of domain IDs
%     \item $N_{\text{total}}$: Total number of text IDs to sample
% \end{itemize}

% \textbf{Output:} 
% \(\mathcal{T}_{\text{sampled}}\): Balanced sampled text IDs

% \begin{algorithmic}[1]
%     \State \textbf{Filter Valid Text IDs:}
%     \[
%     \mathcal{T}_{\text{valid}} = \{t \in \mathcal{T} \mid |\text{utterances}(t)| \geq 5\}
%     \]

%     \State \textbf{Set Sampling Targets:}
%     \[
%     N_{\text{dialect}} = \frac{N_{\text{total}}}{|\mathcal{D}_{\text{did}}|}
%     \]
%     \[
%     N_{\text{target}}(d, o) = \frac{N_{\text{dialect}}}{|\{o \in \mathcal{D}_{\text{dom}} \mid d \text{ fixed}\}|}
%     \]

%     \State \textbf{Sample Text IDs:}
%     For each $(d, o) \in \mathcal{D}_{\text{did}} \times \mathcal{D}_{\text{dom}}$:
%     \[
%     \mathcal{T}_{\text{sampled}}(d, o) = 
%     \begin{cases} 
%         \text{Compensate from other domains}, & \text{if } |\text{available}(d, o)| < N_{\text{target}}(d, o) \\
%         \text{Sample within domain}, & \text{otherwise}
%     \end{cases}
%     \]
%     Append \(\mathcal{T}_{\text{sampled}}(d, o)\) to \(\mathcal{T}_{\text{sampled}}\)

%     \State \textbf{Save Results:}
%     \[
%     \text{Save } \mathcal{T}_{\text{sampled}} \text{ to output files}
%     \]
% \end{algorithmic}
% \end{algorithm}

% The purpose of this algorithm is to perform balanced sampling of text IDs across different dialects and domains. The main steps are:

% \paragraph{1. Filter Valid Text IDs}
% The algorithm filters the set of all text IDs (\(\mathcal{T}\)) to retain only those that have at least five associated utterances, ensuring sufficient representation.

% \paragraph{2. Set Sampling Targets}
% The total number of text IDs to sample (\(N_{\text{total}}\)) is divided equally among all dialects (\(\mathcal{D}_{\text{did}}\)). Within each dialect, the target number of text IDs is further divided among its associated domains (\(\mathcal{D}_{\text{dom}}\)).

% \paragraph{3. Sample Text IDs}
% For each dialect-domain pair \((d, o)\):
% \begin{itemize}
%     \item If the available text IDs are fewer than the target (\(N_{\text{target}}(d, o)\)), the shortfall is compensated by sampling additional text IDs from other domains within the same dialect.
%     \item Otherwise, text IDs are sampled within the domain to meet the target.
% \end{itemize}

% \paragraph{4. Save Results}
% The sampled text IDs (\(\mathcal{T}_{\text{sampled}}\)) are saved, ensuring that the dataset is balanced across dialects and domains.

% This approach ensures a fair distribution of text IDs across dialects and domains while maintaining sufficient representation for each pair.

% \begin{algorithm}[H]
% \caption{Sample 5 Utterances for Each Text ID}
% \label{alg:sample_utterances}
% \textbf{Input:} 
% \begin{itemize}
%     \item $\mathcal{T}_{\text{sampled}}$: Set of sampled text IDs from the previous algorithm
%     \item $\mathcal{U}$: Set of all utterances with associated text and speaker IDs
%     \item $\mathcal{U}_{\text{train}}$: Set of utterances in the training set
% \end{itemize}

% \textbf{Output:} 
% \begin{itemize}
%     \item $\mathcal{S}_{\text{sampled}}$: Table of speaker IDs sampled for each text ID
%     \item $\mathcal{U}_{\text{sampled}}$: Table of utterance IDs sampled for each text ID
% \end{itemize}

% \begin{algorithmic}[1]
%     \State \textbf{Filter Utterances:}
%     \[
%     \mathcal{U}_{\text{filtered}} = \{u \in \mathcal{U} \mid \text{tid}(u) \in \mathcal{T}_{\text{sampled}} \text{ and } u \in \mathcal{U}_{\text{train}}\}
%     \]

%     \State \textbf{Compute Speaker Frequencies:}
%     \[
%     f_s(s) = \text{Count of occurrences of each speaker ID } s \text{ in } \mathcal{U}_{\text{filtered}}
%     \]

%     \State \textbf{Sample Speaker IDs:}
%     For each $t \in \mathcal{T}_{\text{sampled}}$:
%     \[
%     \mathcal{S}_t = \text{Select 5 speakers with the lowest } f_s(s) \text{ for } t
%     \]

%     \State \textbf{Sample Utterances:}
%     For each $t \in \mathcal{T}_{\text{sampled}}$:
%     \[
%     \mathcal{U}_t = \text{Select utterances associated with } \mathcal{S}_t
%     \]

%     \State \textbf{Save Results:}
%     \[
%     \mathcal{S}_{\text{sampled}} = \{(t, s_1, s_2, \dots, s_5) \mid t \in \mathcal{T}_{\text{sampled}}, s_i \in \mathcal{S}_t\}
%     \]
%     \[
%     \mathcal{U}_{\text{sampled}} = \{(t, u_1, u_2, \dots, u_5) \mid t \in \mathcal{T}_{\text{sampled}}, u_i \in \mathcal{U}_t\}
%     \]
% \end{algorithmic}
% \end{algorithm}

% The purpose of this algorithm is to sample five utterances for each text ID in the set of sampled text IDs (\(\mathcal{T}_{\text{sampled}}\)) from the previous algorithm. The main steps are as follows:

% \paragraph{1. Filter Utterances}
% The algorithm filters the set of all utterances (\(\mathcal{U}\)) to include only those that belong to the sampled text IDs (\(\mathcal{T}_{\text{sampled}}\)) and are part of the training set (\(\mathcal{U}_{\text{train}}\)).

% \paragraph{2. Compute Speaker Frequencies}
% The frequency of each speaker ID in the filtered set of utterances (\(\mathcal{U}_{\text{filtered}}\)) is calculated.

% \paragraph{3. Sample Speaker IDs}
% For each text ID in \(\mathcal{T}_{\text{sampled}}\), five speakers with the lowest frequency are selected.

% \paragraph{4. Sample Utterances}
% For the selected speakers of each text ID, the corresponding utterances are sampled.

% \paragraph{5. Save Results}
% The sampled speaker IDs (\(\mathcal{S}_{\text{sampled}}\)) and sampled utterance IDs (\(\mathcal{U}_{\text{sampled}}\)) are saved in tabular format. Each text ID is associated with exactly five speaker IDs and five utterance IDs.

% This ensures a balanced and fair sampling of utterances for each text ID.

\section{Train–Dev–Test Partitioning Strategy}
\label{appendix:train_test_split}

\subsection{Dev and Test Sets Creation}
Creating the dev and test sets from the dataset $D_{Z}$ of Dialect Z involves careful selection of speakers and utterances, ensuring diversity and avoiding speaker contamination between sets. The dataset $D_{Z}$ consists of audio files categorized into three slabs: $A_{c}$ (Clean), $A_{sn}$ (Semi-noisy), and $A_{n}$ (Noisy), as described in Appendix~\ref{appendix:slab_def}. Each utterance belongs to one of these slabs, and no speaker or text ID is repeated across dev, test, and train splits.

Since the dev and test sets are relatively small compared to the training set, and need to be balanced across dialects, domains, and genders, we prioritize their creation. Balance is maintained wherever possible in the number of utterances across domains (Agriculture and Banking), task types (Question and Statement), and speaker gender.

The pseudocode below outlines the initialization and constraints applied to construct these sets:

\begin{algorithm}[H]
\SetAlgoLined
\caption{Test Set and Dev Set Creation}
\begin{algorithmic}[1]
\State \textbf{Initialize:}
\State Define $A$ as the total number of audio files in $D_{Z}$.
\State Define subsets $A_{c}$, $A_{sn}$, and $A_{n}$ corresponding to the three slabs (see Appendix~\ref{appendix:slab_def} for slab definitions).
\State Define $S$ as the total number of speakers, and $S_{uncont}$ as the set of uncontaminated speaker IDs (see Appendix~\ref{appendix:speaker_bucket} for contamination criteria).
\State Let max\_rep represent the maximum allowed repetitions of any text ID across the test and dev sets.
\State \textbf{Diversity Constraints:}
\State Maintain balance across task (Question, Statement), domain (Banking, Agriculture), and gender (male, female).
\State Ensure minimal loss of utterances during test and dev set creation.
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsubsection{Test Set Creation}
The test set is constructed by filtering the list of uncontaminated speakers and selecting utterances from the clean slab $A_c$. Gender balance is enforced by sampling from both male and female speaker pools, denoted $S_{ms}$ and $S_{fs}$. For each selected speaker, exactly one utterance is chosen for each of the four combinations of task and domain.

To avoid repeated evaluation content, utterances with overused text IDs are discarded after reaching the \texttt{max\_rep} limit. A final diversity analysis ensures the test set $A_{test}$ is balanced. The process is summarised below:

\begin{algorithm}[H]
\caption{\textbf{Test Set Creation:}}
\begin{algorithmic}[1]
\State Filter speakers based on $S_{\textit{uncont}}$.
\State Select $A_c$ from $S_{\textit{uncont}}$ for test set creation.
\State Form subsets $S_{ms}$ and $S_{fs}$, ensuring balance in tasks and domains.
\State \textbf{for} each selected speaker per gender \textbf{do}
\State \hspace{1em} Select a single $A_c$ utterance for each of the following:
\State \hspace{2em} Task: Question, Domain: Agriculture
\State \hspace{2em} Task: Statement, Domain: Agriculture
\State \hspace{2em} Task: Question, Domain: Banking
\State \hspace{2em} Task: Statement, Domain: Banking
\State \hspace{1em} Delete all utterances with text IDs selected \texttt{max\_rep} times.
\State \textbf{end for}
\State Perform diversity analysis. If constraints are met, freeze the test set $A_{\textit{test}}$.
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsubsection{Dev Set Creation}
The development set is generated using a similar strategy to the test set, with the added constraint that neither speakers nor text IDs from the test set are reused. As before, gender and task-domain balance is ensured through careful sampling. The finalized dev set $A_{dev}$ is frozen once diversity constraints are satisfied.

\begin{algorithm}[H]
\caption{\textbf{Dev Set Creation:}}
\begin{algorithmic}[1]
\State Filter speakers based on $S_{\textit{uncont}}$, excluding those from $A_{\textit{test}}$.
\State Select $A_{\textit{slab1}}$ from $S_{\textit{uncont}}$ for dev set creation.
\State Form subsets $S_{ms}$ and $S_{fs}$, ensuring balance in tasks and domains.
\State \textbf{for} each selected speaker per gender \textbf{do}
\State \hspace{1em} Select a single $A_c$ utterance for each of the following:
\State \hspace{2em} Task: Question, Domain: Agriculture
\State \hspace{2em} Task: Statement, Domain: Agriculture
\State \hspace{2em} Task: Question, Domain: Banking
\State \hspace{2em} Task: Statement, Domain: Banking
\State \hspace{1em} Delete all utterances with text IDs selected \texttt{max\_rep} times.
\State \textbf{end for}
\State Perform diversity analysis. If constraints are met, freeze the dev set $A_{\textit{dev}}$.
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsection{Training Set Creation}\vspace{-3mm}
The training set $A_{train}$ is created by filtering out any utterances associated with speakers or text IDs from the test and dev sets. Diversity analysis is then performed to ensure the distribution of sentences, domains, and gender is consistent with the full dataset $D_Z$. The final training set is frozen after confirming that diversity constraints are met.

\begin{algorithm}[H]
\SetAlgoLined
\begin{algorithmic}[1]
\State \textbf{Training Set Creation:}
\State Filter out utterances belonging to any speaker or text ID from $A_{test}$ and $A_{dev}$.
\State Perform diversity analysis to verify that the training set $A_{train}$ reflects the diversity of $D_{Z}$.
\State Freeze the training set $A_{train}$.
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsection{Handling High Loss Dialects}
Some dialects, such as Mt\_D2, Hi\_D2, and Hi\_D4, suffer from low availability of clean and semi-noisy samples or fail to meet the diversity constraints due to skewed distributions. For such cases, we expand speaker selection to include both slabs $A_{slab1}$ and $A_{slab0.5}$. This ensures sufficient representation while preserving uncontaminated speaker quality.

\begin{algorithm}[H]
\SetAlgoLined
\begin{algorithmic}[1]
\State \textbf{Sampling for High Loss Dialects:}
\State Follow the same steps as for the test and dev sets, but include both $A_{slab1}$ and $A_{slab0.5}$ for speaker selection.
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsection{Balanced Sampling for Small Train Subsets}
To support broader usage and evaluation of the RESPIN corpus, we release a compact version of the training set for each of the nine languages, named \texttt{train\_<lang>\_small}. This subset is constructed using a principled balanced sampling strategy that ensures fair representation across dialects and domains while maintaining sufficient speaker and utterance diversity.

The first step in this process is to perform balanced sampling of text IDs from the available training data. The goal is to distribute the sampling quota equally across all dialects (\(\mathcal{D}_{\text{did}}\)), and then within each dialect, further divide it among its constituent domains (\(\mathcal{D}_{\text{dom}}\)). To ensure meaningful inclusion, only those text IDs with at least five utterances are considered valid for sampling. If a particular domain lacks enough valid text IDs to meet its quota, the deficit is compensated by drawing additional IDs from other domains within the same dialect. This ensures the target size is met without sacrificing dialectal balance.

The detailed logic is formalized in Algorithm~\ref{alg:balanced_sampling}, which outputs a set of sampled text IDs, \(\mathcal{T}_{\text{sampled}}\), balanced across dialect-domain pairs. These sampled IDs serve as the foundation for constructing the final training subset.

Once the text IDs are sampled, the next step is to select corresponding utterances. For each sampled text ID, we aim to include five utterances from distinct speakers. To avoid speaker imbalance, speakers with the lowest frequency of participation in the training set are prioritized.

Algorithm~\ref{alg:sample_utterances} outlines the procedure to filter utterances, compute speaker frequencies, and perform speaker-aware sampling. This guarantees both text diversity and speaker variation in the final subset.

\begin{algorithm}[H]
\caption{Balanced Sampling of Text IDs Across Dialects and Domains}
\label{alg:balanced_sampling}
\textbf{Input:} 
\begin{itemize}
    \item $\mathcal{T}$: Set of all text IDs
    \item $\mathcal{D}_{\text{did}}$: Set of dialect IDs
    \item $\mathcal{D}_{\text{dom}}$: Set of domain IDs
    \item $N_{\text{total}}$: Total number of text IDs to sample
\end{itemize}

\textbf{Output:} 
\(\mathcal{T}_{\text{sampled}}\): Balanced sampled text IDs

\begin{algorithmic}[1]
    \State \textbf{Filter Valid Text IDs:}
    \[
    \mathcal{T}_{\text{valid}} = \{t \in \mathcal{T} \mid |\text{utterances}(t)| \geq 5\}
    \]
    \State \textbf{Set Sampling Targets:}
    \[
    N_{\text{dialect}} = \frac{N_{\text{total}}}{|\mathcal{D}_{\text{did}}|}
    \quad\quad
    N_{\text{target}}(d, o) = \frac{N_{\text{dialect}}}{|\{o \in \mathcal{D}_{\text{dom}} \mid d \text{ fixed}\}|}
    \]
    \State \textbf{Sample Text IDs:}
    For each $(d, o) \in \mathcal{D}_{\text{did}} \times \mathcal{D}_{\text{dom}}$:
    \[
    \mathcal{T}_{\text{sampled}}(d, o) = 
    \begin{cases} 
        \text{Compensate from other domains}, & \text{if } |\text{available}(d, o)| < N_{\text{target}}(d, o) \\
        \text{Sample within domain}, & \text{otherwise}
    \end{cases}
    \]
    Append \(\mathcal{T}_{\text{sampled}}(d, o)\) to \(\mathcal{T}_{\text{sampled}}\)
    \State \textbf{Save Results:}
    \[
    \text{Save } \mathcal{T}_{\text{sampled}} \text{ to output files}
    \]
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\begin{algorithm}[H]
\caption{Sample 5 Utterances for Each Text ID}
\label{alg:sample_utterances}
\textbf{Input:} 
\begin{itemize}
    \item $\mathcal{T}_{\text{sampled}}$: Set of sampled text IDs from the previous algorithm
    \item $\mathcal{U}$: Set of all utterances with associated text and speaker IDs
    \item $\mathcal{U}_{\text{train}}$: Set of utterances in the training set
\end{itemize}

\textbf{Output:} 
\begin{itemize}
    \item $\mathcal{S}_{\text{sampled}}$: Table of speaker IDs sampled for each text ID
    \item $\mathcal{U}_{\text{sampled}}$: Table of utterance IDs sampled for each text ID
\end{itemize}

\begin{algorithmic}[1]
    \State \textbf{Filter Utterances:}
    \[
    \mathcal{U}_{\text{filtered}} = \{u \in \mathcal{U} \mid \text{tid}(u) \in \mathcal{T}_{\text{sampled}} \text{ and } u \in \mathcal{U}_{\text{train}}\}
    \]
    \State \textbf{Compute Speaker Frequencies:}
    \[
    f_s(s) = \text{Count of occurrences of each speaker ID } s \text{ in } \mathcal{U}_{\text{filtered}}
    \]
    \State \textbf{Sample Speaker IDs:}
    For each $t \in \mathcal{T}_{\text{sampled}}$:
    \[
    \mathcal{S}_t = \text{Select 5 speakers with the lowest } f_s(s) \text{ for } t
    \]
    \State \textbf{Sample Utterances:}
    For each $t \in \mathcal{T}_{\text{sampled}}$:
    \[
    \mathcal{U}_t = \text{Select utterances associated with } \mathcal{S}_t
    \]
    \State \textbf{Save Results:}
    \[
        \mathcal{S}_{\text{sampled}} = \{(t, s_1, s_2, \dots, s_5) \mid t \in \mathcal{T}_{\text{sampled}}, s_i \in \mathcal{S}_t\}
    \]
    \[
        \mathcal{U}_{\text{sampled}} = \{(t, u_1, u_2, \dots, u_5) \mid t \in \mathcal{T}_{\text{sampled}}, u_i \in \mathcal{U}_t\}
    \]
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsection{Training Set Statistics and Insights}

\begin{table}[htbp]
\centering
\caption{Language-wise statistics for Clean, Semi-noisy, and Noisy train sets including duration, utterances, unique sentences, and speaker counts.}
\label{tab:trainset_stats}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ll|rrrr|rrrr|rrrr}
\toprule
\textbf{LID} & \textbf{\#Dialects} & 
\multicolumn{4}{c|}{\textbf{Clean Train Set}} & 
\multicolumn{4}{c|}{\textbf{Semi-noisy Train Set}} & 
\multicolumn{4}{c}{\textbf{Noisy Train Set}} \\
 &  & \textbf{Dur (h)} & \textbf{\#Utts} & \textbf{\#Sents} & \textbf{\#Spks} & 
       \textbf{Dur (h)} & \textbf{\#Utts} & \textbf{\#Sents} & \textbf{\#Spks} & 
       \textbf{Dur (h)} & \textbf{\#Utts} & \textbf{\#Sents} & \textbf{\#Spks} \\
\midrule
bh & 3 & 889.61  & 694738 & 21967 & 1851 & 91.83   & 58179  & 16493 & 1505 & 68.53  & 43543  & 15256 & 1392 \\
bn & 5 & 894.60  & 645791 & 20187 & 1791 & 162.84  & 92208  & 17766 & 1479 & 5.77   & 6587   & 5393  & 805  \\
ch & 4 & 1009.09 & 640649 & 19400 & 2082 & 161.28  & 78942  & 17334 & 2048 & 73.96  & 36486  & 14456 & 2024 \\
hi & 5 & 712.61  & 574544 & 19345 & 2305 & 273.27  & 181710 & 19180 & 2304 & 304.55 & 186275 & 19286 & 2314 \\
kn & 5 & 888.72  & 554398 & 23294 & 1931 & 202.42  & 104324 & 22004 & 1919 & 191.18 & 97806  & 21918 & 1935 \\
mg & 4 & 1066.30 & 769679 & 21500 & 2078 & 91.30   & 53414  & 16361 & 2037 & 60.18  & 36206  & 14943 & 2025 \\
mr & 4 & 1026.06 & 809934 & 23069 & 2644 & 285.46  & 191098 & 21669 & 2629 & 174.16 & 109910 & 20306 & 2634 \\
mt & 4 & 552.16  & 392739 & 23108 & 1917 & 400.04  & 237484 & 23154 & 1916 & 314.21 & 174313 & 22609 & 1934 \\
te & 4 & 1021.16 & 702883 & 19978 & 2287 & 186.10  & 111471 & 19370 & 2273 & 127.66 & 73390  & 17914 & 2258 \\
\midrule
\textbf{Total} & \textbf{38} & \textbf{8060.31} & \textbf{5785355} & \textbf{191848} & \textbf{18886} & 
\textbf{1854.54} & \textbf{1108830} & \textbf{173331} & \textbf{18110} & 
\textbf{1320.20} & \textbf{764516} & \textbf{152081} & \textbf{17325} \\
\bottomrule
\end{tabular}
}
\begin{minipage}{\textwidth}
\footnotesize
\textbf{LID}: Language ID, \textbf{\#Dialects}: number of dialects, \textbf{Dur}: duration in hours, 
\textbf{\#Utts}: number of utterances, \textbf{\#Sents}: number of unique sentences, \textbf{\#Spks}: number of speakers.
\end{minipage}%\vspace{-5mm}
\end{table}

Table~\ref{tab:trainset_stats} provides a comprehensive summary of language-wise training set statistics across the Clean, Semi-noisy, and Noisy slabs in the RESPIN corpus. For each language, the table presents the number of dialects, total duration in hours, number of utterances, number of unique sentences, and number of speakers for each slab.

The Clean Train Set comprises high-quality, curated audio data and represents the largest portion of the training set, totaling 8060.31 hours and over 5.78 million utterances. Languages like Marathi (mr), Magahi (mg), and Chhattisgarhi (ch) contribute significantly to the clean set, with over 1000 hours each. The number of speakers per language ranges from approximately 1800 to over 2600, ensuring high speaker diversity.

The Semi-noisy Train Set introduces moderate background noise and variability, offering a middle ground between clean and highly degraded conditions. It adds approximately 1854.54 hours and 1.1 million utterances to the training data. Hindi (hi), Maithili (mt), and Marathi (mr) are the top contributors in terms of duration. Speaker coverage remains uniformly distributed, preserving balance across languages.

The Noisy Train Set is characterized by challenging acoustic conditions and contains 1320.20 hours and 764,516 utterances. Despite filtering for quality, a sizable portion of the data remains usable. Notably, languages such as Hindi and Maithili still provide substantial noisy data, while Bengali (bn) contributes significantly less due to stringent filtering, offering only 5.77 hours.

Across all three slabs combined, the RESPIN training corpus comprises approximately 11,235 hours of audio spanning over 7.96 million utterances, 517,260 unique sentences, and 54,321 speakers (non-unique across slabs). This extensive and diverse training set enables the development of robust speech models capable of generalizing across noise conditions, dialectal variations, and speaker demographics.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Audio Data Analysis}
\label{appendix:audio_analysis}

The distributions of SNR values, durations and speaking rates for RESPIN audio data are specified in sections \ref{sect:snr_dist} and \ref{sect:dur_sr_dist}, respectively

\subsection{SNR-Based Audio Quality Analysis}
\label{sect:snr_dist}

\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/bn_snr.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/bh_snr.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/ch_snr.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/hi_snr.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/kn_snr.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/mg_snr.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/mt_snr.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/mr_snr.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/te_snr.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of SNR values for slab \textit{Clean}}
\label{fig:snr_clean}
\end{figure}


% \textbf{Slab Semi-noisy Audio}

\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/bn_50_snr.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/bh_50_snr.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/ch_50_snr.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/hi_50_snr.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/kn_50_snr.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/mg_50_snr.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/mt_50_snr.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/mr_50_snr.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/te_50_snr.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of SNR values for slab \textit{Semi-noisy}}
\label{fig:snr_semi}
\end{figure}


% \textbf{Slab Noisy Audio}

\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/bn_0_snr.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/bh_0_snr.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/ch_0_snr.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/hi_0_snr.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/kn_0_snr.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/mg_0_snr.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/mt_0_snr.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/mr_0_snr.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/te_0_snr.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of SNR values for slab \textit{Noisy}}
\label{fig:snr_noisy}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Language-wise audio statistics in the Semi-noisy slab of RESPIN. The table shows the number of files, count and percentage of utterances with SNR $<$ 4 dB, and speaking rate features.}
\label{tab:audio_stats_semi}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{lccccccccc}
\toprule
\textbf{LID} & bn & bh & ch & hi & kn & mg & mt & mr & te \\
\midrule
\textbf{\# Files}      & 107{,}920 & 66{,}966 & 98{,}095 & 238{,}370 & 129{,}533 & 65{,}546 & 304{,}410 & 223{,}938 & 133{,}286 \\
\textbf{\# Low SNR}    & 4{,}358   & 1{,}607  & 844      & 1{,}453   & 877      & 989      & 1{,}706   & 1{,}445   & 1{,}509 \\
\textbf{\% Low SNR}    & 4.04      & 2.40     & 0.86     & 0.61      & 0.68     & 1.51     & 0.56      & 0.65      & 1.13 \\
\textbf{Wds/Aud}       & 9.28      & 10.11    & 14.91    & 11.59     & 9.12     & 10.72    & 11.19     & 9.04      & 9.02 \\
\textbf{Dur (s)}       & 5.38      & 4.91     & 6.45     & 4.75      & 5.95     & 5.30     & 5.22      & 4.64      & 5.15 \\
\textbf{WPM}           & 118.90    & 134.19   & 149.81   & 157.58    & 100.05   & 136.48   & 137.63    & 124.74    & 112.55 \\
\bottomrule
\end{tabular}
\end{adjustbox}

\vspace{6pt}
\noindent
\small \textbf{Abbreviations:} LID = Language ID; \# Files = Number of audio files; \# Low SNR = Number of utterances with SNR $<$ 4 dB; \% Low SNR = Proportion of low-SNR utterances; Wds/Aud = Average words per utterance; Dur (s) = Average utterance duration in seconds; WPM = Words per minute.
\end{table}



\begin{table}[htbp]
\centering
\caption{Language-wise audio statistics in the Noisy slab of RESPIN, showing number of files, count and percentage of utterances with SNR $<$ 4 dB.}
\label{tab:audio_stats_noisy}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{lccccccccc}
\toprule
\textbf{LID} & bn & bh & ch & hi & kn & mg & mt & mr & te \\
\midrule
\textbf{\# Files} & 10{,}413 & 50{,}799 & 47{,}657 & 238{,}990 & 118{,}999 & 46{,}119 & 227{,}340 & 128{,}562 & 88{,}378 \\
\textbf{\# Low SNR} & 4{,}170 & 7{,}436 & 4{,}102 & 10{,}130 & 6{,}774 & 4{,}198 & 7{,}620 & 6{,}077 & 4{,}859 \\
\textbf{\% Low SNR} & 40.05 & 14.64 & 8.61 & 4.24 & 5.69 & 9.10 & 3.35 & 4.73 & 5.50 \\
\bottomrule
\end{tabular}
\end{adjustbox}

\vspace{6pt}
\noindent
\small \textbf{Abbreviations:} LID = Language ID; \# Files = Number of audio files; \# Low SNR = Number of utterances with SNR $<$ 4 dB; \% Low SNR = Proportion of low-SNR utterances.
\end{table}


We use Signal-to-Noise Ratio (SNR) as a proxy to quantify audio quality across the RESPIN dataset. SNR measures the ratio of speech signal energy to background noise energy, expressed in decibels (dB). A higher SNR indicates a cleaner, less noisy recording, making it a useful metric for characterizing the quality of speech data.

\paragraph{SNR Computation Methodology.}
As shown in Figure~\ref{fig:snr_computation}, we compute the SNR of each utterance using the pretrained \texttt{FB-Denoiser}\footnote{\url{https://github.com/facebookresearch/denoiser}} model, which is based on the DEMUCS architecture for speech enhancement. Given an original audio sample \( x_m \), we generate its denoised counterpart \( y_m \), and compute the estimated noise as \( n_m = x_m - y_m \). To ensure that the SNR reflects only the spoken content, non-speech segments at the start and end of the audio are trimmed using forced alignment timestamps generated by a Kaldi TDNN-HMM model trained on slab Clean.

The SNR value is calculated using the standard formula:
\begin{equation}
\text{SNR} = 10 \log_{10} \left( \frac{X}{N} \right)
\label{equ:snr}
\end{equation}
where \( X \) is the average signal power, and \( N \) is the average noise power. These are computed as:
\begin{equation}
X = \frac{1}{m} \sum_{i=1}^{m} (x_i)^2
\label{equ:sig_en}
\end{equation}
\begin{equation}
N = \frac{1}{m} \sum_{i=1}^{m} (n_i)^2
\label{equ:noise_en}
\end{equation}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.6\linewidth]{figures/SNR/snr_computation.jpeg}
\caption{Calculation of SNR values using FB-Denoiser}
\label{fig:snr_computation}
\end{figure}

\paragraph{SNR Distributions Across Slabs:}
Figures~\ref{fig:snr_clean}, \ref{fig:snr_semi}, and~\ref{fig:snr_noisy} visualize the distributions of SNR values for all nine languages across the Clean, Semi-noisy, and Noisy slabs, respectively. These histograms clearly show how signal quality varies across slabs.

In the Clean slab (Figure~\ref{fig:snr_clean}), SNR values are tightly concentrated between 25 and 40 dB, with minimal low-SNR outliers. Languages like Hindi, Kannada, and Maithili demonstrate particularly clean profiles with sharp histogram peaks and minimal long tails.

In the Semi-noisy slab (Figure~\ref{fig:snr_semi}), the SNR distributions are noticeably wider, and the peaks shift leftward. Table~\ref{tab:audio_stats_semi} reports that the percentage of utterances with SNR below 4 dB is still relatively low—ranging from 0.56\% for Maithili to 4.04\% for Bengali. Hindi and Maithili continue to exhibit stable audio quality, while Bengali, Bhojpuri, and Magahi show modest increases in low-SNR samples.

In the Noisy slab (Figure~\ref{fig:snr_noisy}), the degradation becomes more evident. Bengali has the most substantial drop in quality, with 40.05\% of its utterances falling below 4 dB, as shown in Table~\ref{tab:audio_stats_noisy}. Bhojpuri, Chhattisgarhi, and Magahi also show elevated noise levels. In contrast, Hindi and Maithili retain a relatively small proportion of low-SNR files even in the Noisy slab, suggesting better recording conditions or noise resilience during collection.

SNR variation is not only slab-dependent but also language-specific. Bengali consistently exhibits more low-SNR recordings, while Maithili and Hindi maintain relatively clean audio across all slabs. This difference may stem from differences in recording environments, devices used, or speaker demographics. Additionally, the long-tailed nature of the Noisy distributions reflects the presence of a small but significant number of extremely noisy files that may require additional filtering in downstream tasks.

\paragraph{Summary}
These SNR-based analyses validate the design of the slab-based data curation pipeline. The progressive leftward shift of SNR distributions from Clean to Semi-noisy to Noisy slabs confirms the intended stratification of audio quality. This tiered structure makes RESPIN suitable for benchmarking ASR models under varying levels of noise and enables controlled experimentation for noise-robust speech recognition in diverse Indian language dialects.


\subsection{Speech Duration and Speaking Rate Analysis}
\label{sect:dur_sr_dist}

% \textbf{Slab Clean Audio}
\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/bn_100_sr_plot.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/bh_100_sr_plot.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/ch_100_sr_plot.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/hi_100_sr_plot.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/kn_100_sr_plot.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/mg_100_sr_plot.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/mt_100_sr_plot.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/mr_100_sr_plot.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/te_100_sr_plot.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of WPM values for slab Clean}
\label{fig:wpm_clean}
\end{figure}


% \textbf{Slab Semi-noisy Audio}

\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/bn_50_sr_plot.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/bh_50_sr_plot.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/ch_50_sr_plot.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/hi_50_sr_plot.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/kn_50_sr_plot.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/mg_50_sr_plot.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/mt_50_sr_plot.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/mr_50_sr_plot.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/te_50_sr_plot.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of WPM values for slab Semi-noisy}
\label{fig:wpm_semi}
\end{figure}


% \textbf{Slab Clean Audio}
\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/bn_100_durs_plot.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/bh_100_durs_plot.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/ch_100_durs_plot.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/hi_100_durs_plot.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/kn_100_durs_plot.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/mg_100_durs_plot.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/mt_100_durs_plot.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/mr_100_durs_plot.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/te_100_durs_plot.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of audio durations for slab Clean}
\label{fig:dur_clean}
\end{figure}


% \textbf{Slab Semi-noisy Audio}
\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/bn_50_durs_plot.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/bh_50_durs_plot.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/ch_50_durs_plot.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/hi_50_durs_plot.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/kn_50_durs_plot.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/mg_50_durs_plot.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/mt_50_durs_plot.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/mr_50_durs_plot.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/te_50_durs_plot.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of audio durations for slab Semi-noisy}
\label{fig:dur_semi}
\end{figure}

We analyze two important prosodic features of the RESPIN audio corpus—utterance duration and speaking rate (in words per minute, WPM)—across the Clean and Semi-noisy slabs. These metrics help assess consistency in recording conditions and variability in natural speech across languages.

% \vspace{0.5em}
\textbf{Utterance Duration:}
Utterance duration is computed from forced alignment outputs obtained using a Kaldi TDNN-HMM model trained on Clean audio. We define the duration as the time interval between the start of the first word and the end of the last word, ignoring leading/trailing silences. Figures~\ref{fig:dur_clean} and~\ref{fig:dur_semi} show the distribution of utterance durations for each language in the Clean and Semi-noisy slabs, respectively.

In the Clean slab, most languages exhibit tightly peaked distributions around 4–6 seconds with minimal long tails, indicating consistent control during scripted recording. In contrast, the Semi-noisy slab shows greater variability and longer tails, especially for Kannada and Maithili, which display more spread-out patterns. These differences reflect the less constrained and more spontaneous nature of recordings in Semi-noisy conditions.

% \vspace{0.5em}
\textbf{Speaking Rate:}
Speaking rate is defined as the number of words spoken per minute in each utterance. We compute this by dividing the number of tokenized words by the utterance duration (in seconds) and scaling appropriately. Figures~\ref{fig:wpm_clean} and~\ref{fig:wpm_semi} present the WPM histograms for Clean and Semi-noisy slabs.

In the Clean slab, the distributions are largely Gaussian-shaped and fall between 110–150 WPM. Languages such as Hindi, Maithili, and Chhattisgarhi exhibit higher average rates (above 140 WPM), while Kannada and Bengali cluster toward the lower end. The Semi-noisy slab, however, reveals increased spread and irregularities in speaking rate. Notably, Bengali and Kannada show low WPM outliers (<50 WPM), whereas Hindi retains a relatively consistent and faster rate. These trends are quantitatively supported by Table~\ref{tab:audio_stats_semi}, which shows Hindi reaching 157.58 WPM, and Kannada at 100.05 WPM.

Together, these observations highlight how the Clean slab offers tightly controlled speech characteristics, while the Semi-noisy slab introduces useful variability for training robust ASR models suited to real-world speech.

\section{Benchmarking ASR performance}
\label{appendix:asr_performance}

\begin{table}[htbp]
    \centering
    \caption{Dialect-wise and overall CER and WER (\%) for different fairseq-based models across languages. \textbf{Pretrained models} refer to models fine-tuned on publicly available data other than RESPIN. \textbf{Fine-tuned models} are pretrained SSL models further fine-tuned on a subset of RESPIN. For pretrained SSL models, \textbf{bh}, \textbf{ch}, \textbf{mg}, and \textbf{mt} are evaluated using Hindi-tuned models.}
    \resizebox{\columnwidth}{!}{%
    % \begin{tabular}{lccccccccccccccccccc}
    \begin{tabular}{lccccc@{\hskip 8pt}ccccc|ccccccccc}
    \midrule
     & \multicolumn{1}{l}{} & \multicolumn{9}{c}{\textbf{CER (\%)}} & \multicolumn{9}{c}{\textbf{WER (\%)}} \\ \cmidrule(lr){3-11} \cmidrule(lr){12-20}
    \multicolumn{1}{c}{\textbf{Model}} & \textbf{Dialect} & \textbf{bh} & \textbf{bn} & \textbf{ch} & \textbf{hi} & \textbf{kn} & \textbf{mg} & \textbf{mr} & \textbf{mt} & \textbf{te} & \textbf{bh} & \textbf{bn} & \textbf{ch} & \textbf{hi} & \textbf{kn} & \textbf{mg} & \textbf{mr} & \textbf{mt} & \textbf{te} \\ \hline

    \multicolumn{7}{l}{\cellcolor[HTML]{FFFFFF}\textbf{pre-trained IndicW2V2 (fine-tuned on non-RESPIN public data)}} \\
    \rowcolor[HTML]{FFFFFF} & D1 & 17.69 & 15.01 & 23.17 & 9.70 & 8.32 & 22.14 & 16.86 & 24.44 & 8.94 & 52.69 & 45.45 & 65.38 & 25.75 & 38.89 & 55.66 & 63.59 & 68.47 & 37.63 \\
    \rowcolor[HTML]{F2F2F2} & D2 & 15.76 & 12.64 & 22.27 & 13.35 & 5.00 & 17.11 & 15.32 & 23.08 & 8.47 & 48.54 & 38.38 & 65.45 & 34.32 & 21.92 & 48.95 & 52.81 & 65.38 & 38.93 \\
    \rowcolor[HTML]{FFFFFF} & D3 & 17.65 & 10.89 & 22.94 & 9.44 & 16.36 & 19.01 & 12.54 & 23.86 & 8.17 & 53.28 & 33.25 & 67.51 & 25.07 & 59.02 & 52.82 & 44.87 & 67.12 & 36.25 \\
    \rowcolor[HTML]{F2F2F2} & D4 & - & 17.09 & 22.75 & 12.54 & 17.96 & 20.99 & 15.50 & 21.84 & 8.90 & - & 52.33 & 65.70 & 31.49 & 65.06 & 62.68 & 53.68 & 63.58 & 38.64 \\
    \rowcolor[HTML]{FFFFFF} & D5 & - & 16.08 & - & 8.48 & 5.66 & - & - & - & - & - & 45.88 & - & 21.99 & 27.27 & - & - & - & - \\
    \rowcolor[HTML]{F2F2F2} & Overall & 17.08 & 14.27 & 22.77 & 11.02 & 10.37 & 19.64 & 15.09 & 23.30 & 8.61 & 51.61 & 42.83 & 65.98 & 28.34 & 42.37 & 54.32 & 53.91 & 66.10 & 37.82 \\
    \multicolumn{7}{l}{\cellcolor[HTML]{FFFFFF}\textbf{pre-trained SPRING-W2V2 (fine-tuned on non-RESPIN public data) }} \\
     \rowcolor[HTML]{FFFFFF}& D1 & 15.75 & 11.81 & 20.70 & 8.68 & 9.50 & 18.30 & 9.38 & 22.11 & 6.57 & 42.38 & 26.03 & 54.07 & 23.66 & 39.48 & 44.48 & 40.43 & 57.98 & 33.05 \\
     \rowcolor[HTML]{F2F2F2}& D2 & 15.08 & 12.85 & 20.23 & 9.76 & 5.34 & 13.78 & 7.47 & 17.25 & 6.93 & 40.64 & 25.22 & 53.62 & 24.87 & 27.99 & 34.24 & 35.15 & 49.38 & 37.42 \\
     \rowcolor[HTML]{FFFFFF}& D3 & 14.48 & 11.39 & 20.88 & 8.23 & 16.79 & 15.74 & 5.43 & 20.12 & 6.80 & 40.86 & 21.82 & 55.89 & 20.98 & 56.79 & 41.67 & 25.78 & 54.19 & 35.31 \\
     \rowcolor[HTML]{F2F2F2}& D4 & - & 14.44 & 21.40 & 8.68 & 20.91 & 18.46 & 7.81 & 21.93 & 7.67 & \multicolumn{1}{l}{-} & 30.30 & 57.91 & 24.11 & 64.73 & 51.46 & 34.68 & 54.41 & 39.69 \\
     \rowcolor[HTML]{FFFFFF}& D5 & - & 12.03 & - & 8.33 & 6.07 & - & - & - & - & - & 26.65 & - & 20.55 & 32.71 & - & - & - & - \\
     \rowcolor[HTML]{F2F2F2}& Overall & 15.10 & 12.50 & 20.81 & 8.80 & 11.43 & 16.35 & 7.56 & 20.12 & 6.97 & 41.32 & 25.93 & 55.42 & 22.99 & 44.35 & 42.09 & 34.15 & 53.69 & 36.32 \\

    \multicolumn{9}{l}{\cellcolor[HTML]{FFFFFF}\textbf{pre-trained SPRING-Data2Vec-AQC (fine-tuned on non-RESPIN public data)}} \\
    \rowcolor[HTML]{FFFFFF}  & D1 &  15.83 & 11.17 & 21.16 & 6.31 & 8.73 & 17.14 & 9.60 & 20.77 & 5.90 &  43.67   & 23.99 & 55.35 & 20.74 & 37.37 & 44.33 & 40.64 & 56.27 & 31.01 \\
    \rowcolor[HTML]{F2F2F2}  & D2 & 14.88 & 12.59 & 20.78 & 8.62 & 4.63 & 13.43 & 7.54 & 18.10 & 6.60 &  42.00 & 24.18 & 54.73 & 22.85 & 25.90 & 34.69 & 34.34 & 50.42 & 34.90 \\
    \rowcolor[HTML]{FFFFFF}  & D3 &  14.38  & 11.36 & 21.39 & 6.09 & 15.07 & 15.49 & 5.22 & 20.73 & 6.43 &  41.37 & 21.55 & 56.26 & 18.86 & 55.92 & 42.68 & 24.85 & 55.95 & 32.50 \\
    \rowcolor[HTML]{F2F2F2}  & D4 & - & 13.78 & 21.71 & 7.01 & 21.15 & 18.02 & 7.43 & 20.59 & 7.30 & - & 27.00 & 58.19 & 21.43 & 64.04 & 51.60 & 33.13 & 52.91 & 37.75 \\
    \rowcolor[HTML]{FFFFFF}  & D5 & - & 10.80 & - & 7.25 & 5.57 & - & - & - & - & - & 22.08 & - & 19.89 & 30.65 & - & - & - & - \\ 
    \rowcolor[HTML]{F2F2F2}  & Overall & 15.02 \rowcolor[HTML]{F2F2F2}& 11.94 & 21.26 & 7.20 & 10.78 & 15.81 & 7.49 & 19.91 & 6.53 &  42.35 & 23.69 & 56.17 & 20.93 & 42.79 & 42.47 & 33.40 & 53.65 & 33.98 \\ \midrule
    \multicolumn{7}{l}{\cellcolor[HTML]{FFFFFF}\textbf{IndicW2V2 (fine-tuned on RESPIN subset) }}  \\
    \rowcolor[HTML]{FFFFFF} & D1 & 4.45 & 4.09 & 3.13 & 2.59 & 4.15 & 6.76 & 4.54 & 5.7 & 4.78 & 15.89 & 15.57 & 11.13 & 9.14 & 23.46 & 23.90 & 21.61 & 21.43 & 23.55 \\
    \rowcolor[HTML]{F2F2F2} & D2 & 4.22 & 3.55 & 2.96 & 4.23 & 2.12 & 5.52 & 3.08 & 5.49 & 4.07 & 15.64 & 15.02 & 10.40 & 12.93 & 12.08 & 17.99 & 16.19 & 19.26 & 22.87 \\
    \rowcolor[HTML]{FFFFFF} & D3 & 4.57 & 3.01 & 2.88 & 1.92 & 7.05 & 5.55 & 2.11 & 4.4 & 4.41 & 16.63 & 11.29 & 10.56 & 7.29 & 35.98 & 21.49 & 8.66 & 17.55 & 23.82 \\
    \rowcolor[HTML]{F2F2F2} & D4 & - & 6.1 & 3.89 & 4.01 & 8.03 & 6.43 & 2.92 & 5.07 & 4.91 & - & 22.53 & 13.16 & 13.20 & 37.29 & 23.76 & 13.51 & 18.35 & 25.93 \\
    \rowcolor[HTML]{FFFFFF} & D5 & - & 4.75 & - & 2.18 & 2.68 & - & - & - & - & - & 19.13 & - & 7.77 & 15.98 & - & - & - & - \\
    \rowcolor[HTML]{F2F2F2} & Overall & 4.42 & 4.28 & 3.24 & 3.16 & 4.68 & 6.02 & 3.19 & 5.19 & 4.54 & 16.07 & 16.65 & 11.36 & 10.47 & 24.86 & 21.51 & 15.13 & 19.19 & 24.03 \\
    \multicolumn{7}{l}{\cellcolor[HTML]{FFFFFF}\textbf{SPRING-W2V2 (fine-tuned on RESPIN subset)}}   \\
    \rowcolor[HTML]{FFFFFF} & D1 & 3.94 & 3.45 & 2.75 & 2.04 & 3.75 & 5.77 & 3.62 & 4.80 & 3.81 & 14.81 & 13.22 & 9.56 & 7.61 & 21.89 & 20.87 & 18.02 & 18.12 & 20.05 \\
    \rowcolor[HTML]{F2F2F2} & D2 & 3.76 & 3.05 & 2.73 & 3.17 & 1.75 & 4.71 & 2.48 & 4.57 & 3.70 & 14.02 & 13.89 & 9.85 & 10.28 & 10.96 & 15.97 & 14.10 & 16.85 & 21.61 \\
    \rowcolor[HTML]{FFFFFF} & D3 & 4.04 & 2.82 & 2.75 & 1.49 & 6.75 & 4.94 & 1.39 & 3.74 & 3.70 & 14.93 & 9.83 & 10.47 & 5.60 & 35.82 & 19.81 & 6.33 & 15.19 & 21.51 \\
    \rowcolor[HTML]{F2F2F2} & D4 & - & 5.63 & 3.64 & 2.90 & 7.60 & 5.53 & 2.41 & 4.32 & 4.23 & - & 20.89 & 12.86 & 9.60 & 36.81 & 22.17 & 12.05 & 16.25 & 24.62 \\
    \rowcolor[HTML]{FFFFFF} & D5 & - & 4.40 & - & 1.69 & 2.33 & - & - & - & - & - & 17.94 & - & 6.69 & 14.55 & - & - & - & - \\
    \rowcolor[HTML]{F2F2F2} & Overall & 3.92 & 3.86 & 2.99 & 2.37 & 4.30 & 5.20 & 2.49 & 4.37 & 3.85 & 14.61 & 15.12 & 10.74 & 8.22 & 23.90 & 19.40 & 12.75 & 16.64 & 21.92 \\
    \multicolumn{1}{r}{\cellcolor[HTML]{FFFFFF}\textbf{SPRING-Data2Vec-AQC }} & \multicolumn{5}{l}{\cellcolor[HTML]{FFFFFF}\textbf{(fine-tuned on RESPIN subset)}} \\
    \rowcolor[HTML]{FFFFFF} & D1 & 3.95 & 3.48 & 2.68 & 2.03 & 3.59 & 5.48 & 3.73 & 4.95 & 3.60 & 14.79 & 13.27 & 9.72 & 7.77 & 19.95 & 20.01 & 18.56 & 19.05 & 19.27 \\
    \rowcolor[HTML]{F2F2F2} & D2 & 3.81 & 2.74 & 2.55 & 2.90 & 1.71 & 4.41 & 2.32 & 4.46 & 3.61 & 14.34 & 12.34 & 9.20 & 9.13 & 11.04 & 15.26 & 13.25 & 16.22 & 21.48 \\
    \rowcolor[HTML]{FFFFFF} & D3 & 4.07 & 2.48 & 2.54 & 1.67 & 6.40 & 5.00 & 1.35 & 3.58 & 3.61 & 15.33 & 8.87 & 9.51 & 6.19 & 35.02 & 19.56 & 6.43 & 14.50 & 20.42 \\
    \rowcolor[HTML]{F2F2F2} & D4 & - & 5.14 & 3.49 & 2.49 & 7.22 & 5.16 & 2.04 & 4.17 & 4.10 & - & 19.33 & 12.37 & 8.68 & 35.73 & 20.09 & 10.36 & 15.78 & 23.64 \\
    \rowcolor[HTML]{FFFFFF} & D5 & - & 4.37 & - & 1.87 & 2.26 & - & - & - & - & - & 17.22 & - & 7.05 & 14.42 & - & - & - & - \\  
    \rowcolor[HTML]{F2F2F2}  & Overall & 3.95 & 3.63 & 2.84 & 2.27 & 4.11 & 4.98 & 2.38 & 4.30 & 3.72 & 14.84 & 14.15 & 10.25 & 7.91 & 23.13 & 18.50 & 12.28 & 16.41 & 21.17 \\ \bottomrule
    
    
    \end{tabular}%
    }
    \label{tab:consolidated_resultsfariseq}
\end{table}


\begin{table}[htbp]
	\centering
        \caption{Dialect-wise and overall CER and WER (\%) for different Whisper and traditional models across languages. \textbf{Fine-tuned models} are Whisper models further fine-tuned on a subset of RESPIN. \textbf{Traditional models} are trained from scratch on RESPIN.}
	\resizebox{\columnwidth}{!}{%
	% \begin{tabular}{lccccccccccccccccccc}
	\begin{tabular}{lccccc@{\hskip 8pt}ccccc|ccccccccc}
	\midrule
		& \multicolumn{1}{l}{} & \multicolumn{9}{c}{\textbf{CER (\%)}} & \multicolumn{9}{c}{\textbf{WER (\%)}} \\ \cmidrule(lr){3-11} \cmidrule(lr){12-20}
	\multicolumn{1}{c}{\textbf{Model}} & \textbf{Dialect} & \textbf{bh} & \textbf{bn} & \textbf{ch} & \textbf{hi} & \textbf{kn} & \textbf{mg} & \textbf{mr} & \textbf{mt} & \textbf{te} & \textbf{bh} & \textbf{bn} & \textbf{ch} & \textbf{hi} & \textbf{kn} & \textbf{mg} & \textbf{mr} & \textbf{mt} & \textbf{te} \\ \hline
	
		\multicolumn{8}{l}{\cellcolor[HTML]{FFFFFF}\textbf{Whisper-Tiny (fine-tuned on RESPIN subset)}} \\
		\rowcolor[HTML]{FFFFFF}  & D1 & 9.45 & 11.72 & 6.81 & 7.27 & 12.38 & 16.8 & 10.8 & 11.69 & 13.2 & 26.84 & 32.65 & 19.22 & 19.43 & 46.98 & 40.19 & 37.21 & 34.8 & 44.05 \\
		\rowcolor[HTML]{F2F2F2}  & D2 & 9.2 & 10.45 & 6.7 & 14.03 & 8.57 & 11.53 & 9.39 & 11.44 & 9.15 & 26.8 & 29.85 & 20.22 & 25.45 & 35.34 & 30.49 & 32.8 & 33.16 & 37.85 \\
		\rowcolor[HTML]{FFFFFF}  & D3 & 10.14 & 9.1 & 6.44 & 6.85 & 17.14 & 13.11 & 6.74 & 8.88 & 10.29 & 28.62 & 26.04 & 18.82 & 17.37 & 61.88 & 35.51 & 22.16 & 28.6 & 40.28 \\
		\rowcolor[HTML]{F2F2F2}  & D4 & - & 14.39 & 8.4 & 11.38 & 17.74 & 15.13 & 9.5 & 10.67 & 13.21 & - & 39.01 & 24.49 & 26.84 & 61.02 & 41.64 & 30.91 & 30.72 & 44.42 \\
		\rowcolor[HTML]{FFFFFF}  & D5 & - & 12.53 & - & 6.26 & 8.43 & - & - & - & - & - & 35.55 & - & 16.2 & 38.16 & - & - & - & - \\
		\rowcolor[HTML]{F2F2F2}  & Overall & 9.62 & 11.6 & 7.13 & 9.69 & 12.62 & 13.98 & 9.15 & 10.73 & 11.43 & 27.45 & 32.51 & 20.81 & 21.71 & 48.54 & 36.4 & 30.93 & 31.96 & 41.61 \\

		\multicolumn{8}{l}{\textbf{Whisper-Base (fine-tuned on RESPIN subset)}} \\
		\rowcolor[HTML]{FFFFFF} & D1 & 7.22 & 7.34 & 5.21 & 4.4 & 7.35 & 11.41 & 8.2 & 8.17 & 8.71 & 22.3 & 24.35 & 15.8 & 12.62 & 34.45 & 32.84 & 31.66 & 27.55 & 34.44 \\
		\rowcolor[HTML]{F2F2F2} & D2 & 6.73 & 7.05 & 4.81 & 7.9 & 4.68 & 8.94 & 6.51 & 8.02 & 6.52 & 22.11 & 22.52 & 15.48 & 19.27 & 23.38 & 26.05 & 26.28 & 25.09 & 30.88 \\
		\rowcolor[HTML]{FFFFFF} & D3 & 7.46 & 5.38 & 4.77 & 4.18 & 11.67 & 9.92 & 4.21 & 6.35 & 7.16 & 23.07 & 17.94 & 15.16 & 11.23 & 48.4 & 30.01 & 15.92 & 21.93 & 31.45 \\
		\rowcolor[HTML]{F2F2F2} & D4 & - & 10.19 & 6.48 & 7.24 & 12.69 & 12.04 & 5.84 & 7.35 & 7.62 & - & 30.92 & 19.89 & 18.45 & 49.88 & 35.06 & 22.59 & 24.38 & 35.38 \\
		\rowcolor[HTML]{FFFFFF} \multirow{-5}{*}{} & D5 & - & 8.62 & - & 3.72 & 5.03 & - & - & - & - & - & 28.27 & - & 11.48 & 26.93 & - & - & - & - \\
		\rowcolor[HTML]{F2F2F2} & Overall & 7.15 & 7.69 & 5.36 & 5.8 & 8.1 & 10.44 & 6.23 & 7.51 & 7.51 & 22.51 & 24.71 & 16.67 & 15.19 & 36.52 & 30.54 & 24.28 & 24.8 & 32.99 \\
		\multicolumn{8}{l}{\textbf{Whisper-Small (fine-tuned on RESPIN subset)}} \\
		\rowcolor[HTML]{FFFFFF} & D1 & 8.43 & 4.96 & 3.86 & 3.28 & 5.52 & 7.99 & 5.6 & 6.97 & 6.24 & 19.44 & 17.93 & 11.81 & 10.1 & 27.73 & 24.5 & 23.93 & 23.79 & 27.16 \\
		\rowcolor[HTML]{F2F2F2} & D2 & 8.8 & 4.38 & 3.52 & 5.39 & 2.79 & 6.55 & 3.8 & 6.34 & 4.74 & 18.19 & 16.18 & 11.78 & 14.44 & 15.41 & 20.43 & 18.31 & 20.6 & 25.91 \\
		\rowcolor[HTML]{FFFFFF} & D3 & 6.61 & 3.77 & 3.51 & 2.87 & 9.09 & 7.19 & 2.57 & 4.72 & 8.81 & 19.34 & 14.24 & 11.32 & 8.8 & 41.89 & 24.18 & 9.66 & 17.02 & 28.1 \\
		\rowcolor[HTML]{F2F2F2} & D4 & - & 7.92 & 4.43 & 5.53 & 9.86 & 8.47 & 3.64 & 5.57 & 6.17 & - & 25.63 & 14.29 & 14.49 & 43.46 & 28.21 & 15.29 & 19.43 & 30.14 \\
		\rowcolor[HTML]{FFFFFF} & D5 & - & 6.34 & - & 2.58 & 3.57 & - & - & - & - & - & 20.87 & - & 8.95 & 20.39 & - & - & - & - \\
		\rowcolor[HTML]{F2F2F2} & Overall & 7.9 & 5.46 & 3.85 & 4.16 & 6 & 7.46 & 3.93 & 5.94 & 6.54 & 19.02 & 18.91 & 12.36 & 11.78 & 29.66 & 23.94 & 16.95 & 20.28 & 27.82 \\
		\multicolumn{8}{l}{\textbf{E-Branchformer (trained from scratch on RESPIN subset)}} \\
		\rowcolor[HTML]{FFFFFF} & D1 & 4.97 & 3.90 & 3.60 & 2.98 & 4.14 & 7.44 & 4.84 & 6.18 & 4.51 & 14.91 & 13.90 & 9.98 & 8.71 & 22.90 & 21.89 & 21.05 & 19.64 & 21.58 \\
		\rowcolor[HTML]{F2F2F2} & D2 & 4.87 & 3.86 & 3.10 & 4.62 & 1.85 & 5.82 & 3.01 & 5.90 & 3.19 & 14.82 & 13.86 & 9.78 & 12.18 & 11.64 & 16.55 & 16.07 & 17.79 & 19.72 \\
		\rowcolor[HTML]{FFFFFF} & D3 & 5.00 & 2.53 & 3.37 & 2.25 & 7.43 & 6.16 & 1.86 & 4.82 & 3.80 & 15.86 & 9.72 & 10.35 & 7.36 & 36.96 & 21.10 & 7.69 & 15.90 & 20.78 \\
		\rowcolor[HTML]{F2F2F2} & D4 & - & 6.56 & 4.37 & 4.45 & 8.44 & 7.61 & 2.92 & 6.05 & 4.40 & - & 20.67 & 12.10 & 11.99 & 37.06 & 23.28 & 12.51 & 18.41 & 24.62 \\
		\rowcolor[HTML]{FFFFFF} & D5 & - & 4.90 & - & 2.41 & 2.11 & - & - & \multicolumn{1}{l}{} & - & - & 16.94 & - & 7.79 & 14.52 & - & - & - & - \\
		\rowcolor[HTML]{F2F2F2} & Overall & 4.95 & 4.33 & 3.63 & 3.52 & 4.62 & 6.68 & 3.19 & 5.75 & 3.97 & 15.21 & 14.96 & 10.59 & 9.94 & 24.50 & 20.38 & 14.48 & 17.95 & 21.64 \\
		\multicolumn{1}{r}{\textbf{TDNN-HMM}}  & \multicolumn{6}{l}{\textbf{(trained from scratch on RESPIN subset)}}  \\
		\rowcolor[HTML]{FFFFFF} & D1 & 5.80 & 5.04 & 4.25 & 2.57 & 4.04 & 8.87 & 4.52 & 6.90 & 4.35 & 18.02 & 15.22 & 11.88 & 7.56 & 20.71 & 24.30 & 17.94 & 21.52 & 19.72 \\
		\rowcolor[HTML]{F2F2F2} & D2 & 5.55 & 4.51 & 3.63 & 3.95 & 2.33 & 7.07 & 3.22 & 7.18 & 3.17 & 17.36 & 15.26 & 11.19 & 9.55 & 12.84 & 19.04 & 13.92 & 20.80 & 19.79 \\
		\rowcolor[HTML]{FFFFFF} & D3 & 5.66 & 3.46 & 3.80 & 2.31 & 7.42 & 7.05 & 2.44 & 5.47 & 4.10 & 17.31 & 11.71 & 11.41 & 6.24 & 33.34 & 22.91 & 8.97 & 18.24 & 21.47 \\
		\rowcolor[HTML]{F2F2F2} & D4 & - & 8.48 & 5.92 & 4.21 & 9.22 & 7.92 & 2.92 & 6.39 & 4.12 & - & 26.33 & 15.94 & 10.94 & 35.63 & 23.99 & 12.37 & 19.66 & 22.26 \\
		\rowcolor[HTML]{FFFFFF} & D5 & - & 4.80 & - & 2.40 & 2.18 & - & - & - & - & - & 16.26 & - & 7.89 & 12.57 & - & - & - & - \\ 
		\rowcolor[HTML]{F2F2F2} & Overall & 5.67 & 5.22 & 4.45 & 3.25 & 4.88 & 7.69 & 3.30 & 6.53 & 3.94 & 17.57 & 16.87 & 12.69 & 8.72 & 23.01 & 22.33 & 13.40 & 20.13 & 20.81 \\ \bottomrule

\end{tabular}%
}
\label{tab:consolidated_resultsothers}
\end{table}

Tables~\ref{tab:consolidated_resultsfariseq} and~\ref{tab:consolidated_resultsothers} present a detailed comparison of baseline ASR models evaluated on the RESPIN test set. The experiments span Fairseq-based, ESPnet-based, Whisper-based, and Kaldi-based systems, offering a broad perspective across different modeling frameworks.  These results offer insights into how different modeling paradigms perform when confronted with the dialectal, phonetic, and structural diversity embedded in RESPIN.

The results show that models fine-tuned on RESPIN consistently outperform those trained or fine-tuned solely on external corpora. This confirms that RESPIN’s dialect-rich coverage is essential for achieving reasonable performance across all nine languages. In particular, the differences in WER between fine-tuned and non-fine-tuned versions of the same model (e.g., SPRING-Data2Vec-AQC) illustrate how dialect-specific supervision significantly impacts accuracy.

Among the baselines, SPRING-Data2Vec-AQC (fine-tuned on RESPIN subset) performs best for almost all languages and dialects; E-Branchformer model trained from scratch on RESPIN performs competitively across many languages, especially those with high data coverage in the clean subset. Its performance in languages such as Hindi, Marathi and Telugu suggests that well-curated, supervised data can serve as a strong foundation even in the absence of large-scale pretraining.

Whisper models demonstrate robustness across languages, but tend to underperform compared to other RESPIN-finetuned models. In some languages, such as Chhattisgarhi and Magahi, the gap between Whisper and fine-tuned SSL baselines widens, indicating that Whisper’s general-purpose training may not adequately capture the nuances of dialectal variation.

The detailed language-wise breakdown also reveals non-uniform trends in CER and WER. For instance, Dravidian languages like Kannada and Telugu often yield lower CERs but relatively higher WERs, which may reflect challenges in tokenization or word boundary segmentation. On the other hand, languages like Hindi and Bhojpuri tend to show tighter CER–WER alignment.

Variability across dialects is also evident, with certain dialects (e.g., D2, D4 of Hindi, D4 of Bengali, D3, D4 of Kannada) consistently resulting in higher error rates. This highlights the importance of dialect-level metadata in training and evaluation, which RESPIN-S1.0 explicitly encodes.

% Overall, these results validate RESPIN-S1.0 ’s value as a benchmarking resource.
Overall, these results highlight the utility of RESPIN-S1.0 as a robust benchmarking resource. The diversity in performance across models, languages, and dialects underscores the need for future work on dialect-adaptive modeling and context-aware evaluation. The RESPIN-S1.0 dataset offers a controlled and consistent framework for such comparative analyzes and will continue to facilitate reproducible benchmarking of ASR systems aimed at capturing the linguistic diversity of Indian languages.

\subsection{ASR performance on public test sets}

\begin{table}[t]
\centering
\caption{Average public-test CER/WER (\%) across all languages. 
\textbf{Pretrained} = models fine-tuned on public data other than RESPIN; 
\textbf{Traditional} = trained from scratch on RESPIN; 
\textbf{Fine-tuned} = pretrained SSL/Whisper further fine-tuned on a RESPIN subset.}
\vspace{3pt}
\label{tab:public_avg_results_final}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrrrrrrrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{Model}} 
& \multicolumn{8}{c}{\textbf{CER (\%)}} 
& \multicolumn{8}{c}{\textbf{WER (\%)}} \\
\cmidrule(lr){2-9}\cmidrule(lr){10-17}
& \textbf{CV} & \textbf{FL} & \textbf{GV} & \textbf{IT} & \textbf{KB} & \textbf{KBN} & \textbf{MUCS} & \textbf{Avg}
& \textbf{CV} & \textbf{FL} & \textbf{GV} & \textbf{IT} & \textbf{KB} & \textbf{KBN} & \textbf{MUCS} & \textbf{Avg} \\
\midrule

\rowcolor{gray!10}
\multicolumn{17}{l}{\textbf{Pretrained Models (fine-tuned on non-RESPIN public data)}} \\
SeamlessM4T-v2-Large (PT) & \textbf{7.50} & 9.68 & 34.75 & 18.34 & 9.56 & 10.84 & 17.86 & 15.51
                         & \textbf{26.27} & 29.12 & 48.58 & 48.12 & 31.50 & 33.00 & 42.61 & 37.03 \\
IndicW2V2 (PT)            & 13.13 & 14.07 & 21.28 & 16.09 & 8.17 & 10.58 & \textbf{5.32} & 12.66
                         & 42.55 & 36.82 & 48.70 & 44.20 & 32.36 & 36.90 & \textbf{21.53} & 37.58 \\
SPRING-W2V2 (PT)          & 10.17 & 9.52 & 14.48 & \textbf{6.92} & 7.63 & 8.71 & 7.81 & 9.32
                         & 31.53 & 28.66 & 32.49 & \textbf{26.36} & 27.23 & 29.82 & 30.44 & 29.51 \\
SPRING-Data2Vec-AQC (PT)  & 8.88 & \textbf{8.94} & 13.47 & 7.03 & \textbf{6.91} & \textbf{7.27} & 7.31 & \textbf{8.55}
                         & 27.88 & \textbf{25.92} & 29.64 & 27.14 & \textbf{24.18} & \textbf{25.21} & 28.29 & \textbf{26.89} \\
\addlinespace

\rowcolor{gray!10}
\multicolumn{17}{l}{\textbf{Traditional Models (trained from scratch on RESPIN subset)}} \\
TDNN-HMM                  & 26.19 & 19.70 & 42.11 & 19.46 & 19.77 & 24.10 & 20.83 & 24.59
                         & 59.09 & 52.42 & 61.55 & 66.87 & 56.52 & 59.79 & 50.26 & 58.07 \\
E-Branchformer            & 19.86 & 14.75 & 35.74 & 11.67 & 12.93 & 15.81 & 14.10 & 17.84
                         & 52.80 & 42.24 & 58.97 & 47.31 & 42.73 & 47.15 & 42.54 & 47.68 \\
\addlinespace

\rowcolor{gray!10}
\multicolumn{17}{l}{\textbf{Fine-tuned Models (fine-tuned on RESPIN subset)}} \\
Whisper-Tiny              & 30.90 & 34.38 & 49.16 & 28.13 & 22.63 & 27.58 & 24.43 & 31.03
                         & 70.12 & 65.90 & 74.84 & 69.12 & 62.45 & 68.67 & 60.32 & 67.35 \\
Whisper-Base              & 23.90 & 20.39 & 43.14 & 18.46 & 17.40 & 20.47 & 18.86 & 23.23
                         & 61.23 & 52.29 & 68.18 & 59.97 & 53.47 & 58.31 & 51.99 & 57.92 \\
Whisper-Small             & 18.72 & 15.23 & 36.34 & 14.91 & 13.74 & 15.75 & 14.91 & 18.52
                         & 50.97 & 42.59 & 58.66 & 52.34 & 45.10 & 48.39 & 44.07 & 48.87 \\
IndicW2V2 (FT)            & 14.71 & 14.51 & 19.59 & 14.17 & 10.49 & 12.63 & 10.90 & 13.86
                         & 45.05 & 37.22 & 44.77 & 45.40 & 38.42 & 42.71 & 39.35 & 41.85 \\
SPRING-W2V2 (FT)          & 10.28 & 12.50 & 12.02 & 12.38 & 7.63 & 8.84 & 7.03 & 10.10
                         & 32.60 & 29.59 & 30.51 & 38.67 & 28.21 & 30.91 & 28.69 & 31.31 \\
SPRING-Data2Vec-AQC (FT)  & 8.98 & 11.99 & \textbf{10.78} & 12.36 & 7.21 & 7.57 & 7.02 & 9.42
                         & 28.99 & 27.66 & \textbf{27.79} & 38.40 & 26.26 & 27.22 & 27.66 & 29.14 \\
\bottomrule
\end{tabular}%
}
\vspace{2mm}
\footnotesize
\textbf{Test-set abbreviations:} 
\textbf{CV}: CommonVoice,\;
\textbf{FL}: FLEURS,\;
\textbf{GV}: GramVaani,\;
\textbf{IT}: IndicTTS,\;
\textbf{KB}: Kathbath,\;
\textbf{KBN}: Kathbath\_Noisy,\;
\textbf{MUCS}: MUCS.
\vspace{-3mm}
\end{table}

\newcommand{\cmark}{\ding{51}} % ✓
\newcommand{\nmark}{---}       % em-dash style placeholder
\begin{table}[h]
\centering
\caption{Language availability per public test set (\cmark = available). 
Averages in Table~\ref{tab:public_avg_results_final} are computed only over languages marked as available for each test set.}
\label{tab:public_lang_coverage}
\small
\vspace{2pt}
\resizebox{0.7\textwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Test set} & \textbf{bn} & \textbf{hi} & \textbf{kn} & \textbf{mr} & \textbf{te} \\
\midrule
CommonVoice      & \cmark & \cmark & \nmark & \cmark & \nmark \\
FLEURS           & \cmark & \cmark & \cmark & \cmark & \cmark \\
GramVaani        & \nmark & \cmark & \nmark & \nmark & \nmark \\
IndicTTS         & \cmark & \cmark & \cmark & \cmark & \cmark \\
Kathbath         & \cmark & \cmark & \cmark & \cmark & \cmark \\
Kathbath\_Noisy  & \cmark & \cmark & \cmark & \cmark & \cmark \\
MUCS             & \nmark & \cmark & \nmark & \cmark & \cmark \\
\bottomrule
\end{tabular}%
}
\vspace{1.5mm}
\parbox{0.7\textwidth}{
\centering
\footnotesize
\textbf{Language codes:} bn = Bengali,\; hi = Hindi,\; kn = Kannada,\; mr = Marathi,\; te = Telugu.
}
\end{table}


Table~\ref{tab:public_lang_coverage} summarizes the language coverage of each public test set, indicating that not all corpora include all five languages. Therefore, the average CER/WER values reported in Table~\ref{tab:public_avg_results_final} are computed over the available languages for each test set.  
Pretrained models generally achieve the best performance across most test sets, with SPRING-Data2Vec-AQC (PT) and SPRING-W2V2 (PT) yielding the lowest average CER and WER. However, RESPIN fine-tuned models perform comparably well, despite being trained on data from only two domains—\textit{agriculture} and \textit{banking}. This demonstrates the linguistic diversity and robustness of RESPIN, which enables effective transfer learning and generalization to unseen corpora. SPRING-Data2Vec-AQC (FT) shows particularly strong cross-corpus performance, underscoring RESPIN’s value as a benchmark for multidialectal, low-resource ASR. Implementation details and Zipformer results will be made available at: \url{https://github.com/labspire/respin_baselines.git}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ethical Protocols and Data Governance}
\label{appendix:non_tech}

\subsection{Contributor Onboarding and Validation Structure}

A multi-tier validation framework was established to ensure linguistic and ethical quality throughout text data collection. The team comprised \textbf{Language Consultants (LCs)}, \textbf{Language Resource Managers (LRMs)}, \textbf{Senior LRMs}, and \textbf{technical reviewers} including speech scientists and computational linguists. LCs conducted dialect-level validation, while LRMs and Senior LRMs—linguists with advanced academic or field experience—oversaw inter-dialect consistency and annotation quality. Final reviews by technical experts verified linguistic accuracy, dialectal fidelity, and corpus-wide consistency. Figure~\ref{fig:slab_def_fig1} illustrates the validation team hierarchy.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.65\linewidth]{Styles/figures_appendix/slab_def_fig1.png}
\caption{Validation team hierarchy used during data collection and quality control.}
\label{fig:slab_def_fig1}
\end{figure}

\subsection{Recruitment Frameworks and Data Collection Phases}

\paragraph{Phase~1: Domain Expert Recruitment and Collection.}
Phase~1 targeted domain-specific question generation by \textbf{Domain Experts (DEs)} selected for their native fluency, geographic authenticity, and professional experience in agriculture or finance. Typical DEs included agricultural officers, field workers, and local farmers—often the end users of the envisioned ASR systems. After recruitment, DEs received verbal and written orientation via regional coordinators. They were trained to compose natural, contextually relevant questions in their dialect using appropriate local terminology (e.g., crop names, diseases, loan schemes).

Submissions were made through Google Forms distributed via messaging platforms, each covering a specific subdomain such as \textit{weather}, \textit{irrigation}, or \textit{savings}. DEs were required to contribute seven unique questions per topic, avoiding repetition or translation artifacts. Compensation was tied to the number of accepted entries. Contributors received installation guides for regional keyboards, handwriting input tools, and short tutorial videos. Common issues—English mixing, transliteration errors, or fragments—were flagged for correction before final submission. Each DE typically had one to two weeks to complete assignments, followed by a seven-day technical validation before transfer to the LC-led review pipeline.

\paragraph{Phase~2: Dialect Expert Recruitment for Text Expansion.}
Phase~2 aimed to expand linguistic and domain coverage by collecting an additional 15,000 dialect- and domain-rich sentences per language. Online and offline drives recruited \textbf{Dialect Experts (DEs)} for two roles: \textit{Sentence Composition} and \textit{Sentence Translation}. Applicants completed pilot tasks assessing dialect knowledge, grammar, and writing fluency, followed by structured training on syntax, domain terminology, and submission protocols.

\textbf{Sentence Composers} generated original, grammatically accurate sentences (8–15 words) in their native dialects for agriculture and finance domains. Minimum requirements included undergraduate-level education, native dialect proficiency, and digital literacy (Google Docs and basic tools). They worked remotely and were encouraged to maximize lexical diversity while avoiding copied or machine-translated text.

\textbf{Sentence Translators} converted standard dialect sentences into regional variants, maintaining semantic equivalence and grammatical integrity. They demonstrated control over both formal and informal registers and were evaluated through pilot tasks before onboarding. Translators worked remotely using collaborative documents, submitting regular batches for validation.

\subsection{Validation and Review Workflow}

All Phase~1 and Phase~2 submissions passed through a unified validation pipeline. Data were first standardized and grouped by language, dialect, and domain, then reviewed by LCs for correctness, dialectal fidelity, and semantic clarity. Each validated entry included a normalized sentence, English translation, and feature tag (e.g., crop, pest, finance), enabling structured downstream filtering. LC-reviewed files were escalated to LRMs and Senior LRMs for secondary review.

Automated scripts assisted validators by flagging anomalies such as spelling errors, punctuation inconsistencies, English insertions, or unsupported characters. These flags appeared as columns in shared Google Sheets, allowing validators to filter and resolve issues efficiently.

\subsection{Contributor Communication and Management}

\paragraph{Coordination and Communication.}
Contributors working on under-represented dialects received templated instruction sheets specifying translation rules, formatting conventions, and turnaround expectations. Dedicated communication channels via email and WhatsApp ensured real-time clarification.

\paragraph{Compensation and Incentives.}
Payment was performance-based and transparent. For composers and translators, remuneration was proportional to the number of accepted entries, with adjustments for complexity or length. Validators were compensated by validated entry volume and review quality. Compensation slabs were communicated during onboarding.

\paragraph{Recruitment and Outreach.}
Outreach materials—including WhatsApp banners, email flyers, and community posters—were customized by region and dialect to reach both rural and semi-urban contributors effectively.

\paragraph{Contributor Metadata and Demographics.}
Contributor details such as district, dialect, role, and language were collected and anonymized for audit and demographic analysis. These records also informed dialectal balance across the corpus.

\paragraph{Domain Preparation Resources.}
All contributors received curated resources, including domain sublists (e.g., irrigation, pest control, loan schemes), topic-specific vocabulary sets, and question-framing guidelines to support consistency in linguistic style.

\paragraph{Pilot Task Evaluation.}
Before onboarding, each contributor completed pilot tasks reflecting actual assignment types. Evaluations measured grammar, dialect usage, and formatting adherence. Personalized feedback ensured quality alignment prior to full-scale contribution.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ethics and Participant Onboarding}
\label{sec:ethics}

\subsection{Demographic Statistics}
We summarize high-level participant demographics to highlight geographic and socio-linguistic diversity. Table \ref{tab:demo-details} shows the gender, age-group, and regional distributions derived from verified metadata. All languages include contributions from both male and female speakers spanning 18–60 years of age, with representation from 24 states and Union Territories.

\begin{table}[h!]
\centering
\caption{Aggregated participant demographics in the RESPIN-S1.0 corpus.}
\label{tab:demo-details}
\begin{tabular}{lcc}
\toprule
\textbf{Attribute} & \textbf{Coverage} & \textbf{Notes} \\
\midrule
Gender & 57\% Male / 43\% Female & Balanced across languages \\
Age Group & 18–25 (22\%), 26–40 (54\%), 41–60 (24\%) & Mean ≈ 31 yrs \\
Regional Coverage & 24 States + UTs & Derived from postal codes \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Compensation and Wage Compliance}
Participants were paid a task-based honorarium ranging from ₹550 – ₹750 per completed task (≈ 557 utterances), corresponding to an \textbf{effective rate of ₹275 per hour}. This rate is \textbf{3–5× higher} than the regional minimum wages prescribed by the Government of India, ensuring fair remuneration across skill levels and locations. To verify compliance, we refer to the official notification issued by the \emph{Office of the Chief Labour Commissioner (Central), Ministry of Labour and Employment}, effective April 1, 2025. Table \ref{tab:minwage} lists the published daily wage rates.

\begin{table}[h!]
\centering
\caption{Regional minimum daily wage rates (April 1 2025) — Government of India.}
\label{tab:minwage}
\begin{tabular}{lccc}
\toprule
\textbf{Worker Category} & \textbf{Area A (Metro)} & \textbf{Area B (Urban)} & \textbf{Area C (Rural)} \\
\midrule
Unskilled & ₹514/day (₹64.25/hr) & ₹470/day (₹58.75/hr) & ₹465/day (₹58.13/hr) \\
Skilled & ₹610/day (₹76.25/hr) & ₹562/day (₹70.25/hr) & ₹515/day (₹64.38/hr) \\
Highly Skilled & ₹675/day (₹84.38/hr) & ₹628/day (₹78.50/hr) & ₹562/day (₹70.25/hr) \\
\bottomrule
\end{tabular}
\end{table}

Hourly equivalents assume 8-hour workdays. RESPIN’s ₹275/hr rate is therefore substantially higher than the highest prescribed category in Area A, ensuring ethical compensation.

\subsection{Participant Instructions and Consent}
The onboarding workflow combined automated and manual steps: (i) field coordinators provided \textbf{task instructions} and explained compensation, privacy, and consent requirements in local languages; (ii) participants viewed an onboarding video tutorial: \url{https://www.youtube.com/watch?v=7uZjAE3uRS0}; (iii) within the Bolo App, users completed a \textbf{digital consent form} and accepted a click-wrap privacy policy before recording; and (iv) only participants who digitally consented could proceed to tasks. Screenshots of the mobile interfaces illustrating onboarding and payments are shown in Figures \ref{fig:bolo_home} and \ref{fig:bolo_payment}. The consent form text is reproduced below.

\paragraph{Excerpt from Digital Consent Form.}
\begin{quote}\small
By proceeding, I confirm that I am participating voluntarily. I understand that my recordings may be used for research and model development and that my identity will not be disclosed. I consent to data storage and processing under the RESPIN project and may withdraw my participation at any time.
\end{quote}

\subsection{Consent Withdrawal and Data Rights}
Participants retained the right to revoke consent at any time. The Privacy Policy contained the following clause:
\begin{quote}\small
\textbf{“C. Your Choices — Access to Personal Data or Information.”} You may request to access, modify, or delete information held by us. You may withdraw consent by contacting \texttt{operations@navanatech.in}. Any withdrawal request will be honored and processed by the data team, though it may disable access to certain app features.
\end{quote}
All such requests received via email were logged and resolved by the data-collection team within 72 hours.

\subsection{Ethics Approval and PII Safeguards}
The RESPIN project received institutional ethics clearance from the \emph{Indian Institute of Science (IISc) Bangalore}. No personally identifiable information (names, phone numbers, addresses, or photographs) was retained in the released corpus. Only anonymized IDs and coarse location codes were stored. All data were encrypted during transfer and hosted on secure IISc servers.

\subsection{Summary}
RESPIN-S1.0 adhered to the principles of transparency, fair compensation, and informed consent. All ethical documentation, including the consent form, task instructions, and the wage-compliance table, is released alongside the dataset for reproducibility.



\medskip
{
\small

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{NeurIPS Paper Checklist}

% %%% BEGIN INSTRUCTIONS %%%
% The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and follow the (optional) supplemental material.  The checklist does NOT count towards the page
% limit. 

% Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
% \begin{itemize}
%     \item You should answer \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item \answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
%     \item Please provide a short (1-2 sentence) justification right after your answer (even for NA). 
%    % \item {\bf The papers not including the checklist will be desk rejected.}
% \end{itemize}

% {\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

% The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "\answerYes{}" is generally preferable to "\answerNo{}", it is perfectly acceptable to answer "\answerNo{}" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "\answerNo{}" or "\answerNA{}" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.

% IMPORTANT, please:
% \begin{itemize}
%     \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS Paper Checklist"},
%     \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
%     \item {\bf Do not modify the questions and only use the provided macros for your answers}.
% \end{itemize} 
 

% %%% END INSTRUCTIONS %%%


\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    % \item[] Justification: \justificationTODO{}

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justification See Section 6{}

\item {\bf Theory assumptions and proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justification{This is a database paper and we do not claim any theoretical results.}

    \item {\bf Experimental result reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    % \item[] Justification: \justificationTODO{}

\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    % \item[] Justification: \justificationTODO{}


\item {\bf Experimental setting/details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    % \item[] Justification: \justificationTODO{}

\item {\bf Experiment statistical significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper reports standard ASR evaluation metrics (CER and WER) to benchmark model performance on RESPIN-S1.0. Since the focus is on dataset release and not on statistically comparing methods across multiple runs or random seeds, statistical significance testing or error bars were not applicable.

\item {\bf Experiments compute resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    % \item[] Justification: \justificationTODO{}
    
    
\item {\bf Code of ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    % \item[] Justification: \justificationTODO{}


\item {\bf Broader impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    % \item[] Justification: \justificationTODO{}
    
\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The RESPIN-S1.0 dataset consists of curated, read speech collected with informed consent for research use in Indian languages. It does not contain personally identifiable information or content with high risk for misuse. Hence, specific safeguards were not required beyond standard ethical data collection practices.

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    % \item[] Justification: \justificationTODO{}

\item {\bf New assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The RESPIN-S1.0 corpus introduces new language and dialect-level speech and text resources for 9 Indian languages. Detailed documentation is provided alongside the assets, including data format descriptions, speaker metadata, train/dev/test splits, validation procedures, and usage instructions, hosted at \url{https://github.com/saurabhk0317/respin_data_neurips25}.

\item {\bf Crowdsourcing and research with human subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
    \item[] Answer: \answerNo{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The RESPIN corpus was created using contributions from trained dialect experts and volunteers. While informed consent was obtained and contributors were compensated, the paper does not include the full set of instructions, screenshots, or detailed compensation information.

\item {\bf Institutional review board (IRB) approvals or equivalent for research with human subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: All participants contributed voluntarily with informed consent for the public release of their anonymized data. The project received ethics clearance through an internal review process at IISc Bangalore, and no personally identifiable information (PII) was collected. The risks to participants were minimal and clearly communicated.

\item {\bf Declaration of LLM usage}
    \item[] Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.
    %this research? 
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: LLMs were used only for improving the writing and editing of the manuscript. They were not involved in data creation, modeling, evaluation, or any part of the research methodology.

\end{enumerate}

\end{document}

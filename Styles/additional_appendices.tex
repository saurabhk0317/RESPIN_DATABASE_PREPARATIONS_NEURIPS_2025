\documentclass{article}

% NeurIPS style
\usepackage[final]{neurips_2025}

% Encoding
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{enumitem}
% Math & Symbols
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{polyglossia} % Automatically loads fontspec
\setmainlanguage{english}
\setotherlanguages{hindi}
\newfontfamily\hindifont{Noto Sans Devanagari}[Script=Devanagari] % Use any Devanagari font on your system

% Figures & Tables
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{makecell}
\usepackage{colortbl}
% \usepackage{float}
\usepackage{longtable}
\usepackage{colortbl}
\usepackage[table]{xcolor}
\usepackage{geometry} % optional: for custom margins
\usepackage{enumitem}
\usepackage{fontspec}
\usepackage{xunicode} %% loading this first to avoid clash with bidi/arabic

%%% For language switching -- like babel, but for xelatex
\usepackage{polyglossia}

%%% For the xelatex (and other LaTeX friends) logos
\usepackage{hologo}

%%% For the awesome fontawesome icons!
\usepackage{fontawesome}

\usepackage[hyphens]{url}

\usepackage{algorithm}       % Provides the algorithm environment
\usepackage{algpseudocode}   % Provides support for pseudocode
% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\setmainlanguage{english}
\setotherlanguages{arabic,hindi,sanskrit,greek,thai} %% or other languages
\newfontfamily\devanagarifont[Script=Devanagari]{Noto Serif Devanagari}

% Links
\usepackage{hyperref}
\usepackage{url}

% Layout & Formatting
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

% Title
\title{\textbf{RESPIN-S1.0 Corpus: A Read Speech Corpus of 10,000+ Hours in Dialects of Nine Indian Languages}}

\author{}

\begin{document}

\maketitle
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\section*{Appendices}
\section{Languages and Dialect Selection}
This section details the systematic process used for selecting representative dialects across nine Indian languages (Bhojpuri, Magahi, Maithili, Bengali, Kannada, Chhattisgarhi, Telugu, Marathi, and Hindi). As shown in Table\ref{tab:dialect_selection}, our methodology followed a four-step process. For each language, we aimed to identify 3-5 dialects that collectively represent 70-90\% of native speakers while ensuring linguistic diversity. Expert linguists validated our selections to ensure comprehensive coverage and representativeness of the chosen dialects and districts.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.5} % Increased row height
\setlength{\tabcolsep}{6pt} % Adjusted cell padding
\begin{tabular}{|l|p{3cm}|p{7cm}|}
\hline
\textbf{Step} & \textbf{Description} & \textbf{Criteria} \\ \hline

1. Literature Survey & 
Identify dialects and their geographic distribution & 
A comprehensive review of linguistic literature to identify the major dialects of each language followed by the identification of districts in which each dialect is spoken. \\ \hline

2. Dialect Selection & 
Choose representative dialects & 
Dialects that cover 70-90\% of native speakers of the language must be chosen based on their distinctiveness and speaker population size. \\ \hline

3. District Selection & 
Identify representative districts & 
Standard form of dialect, minimal overlap with other dialects and regions to avoid unique speakers, and operational feasibility. \\ \hline

4. Expert Validation & 
Verify selections & 
Consultation with language experts/linguists to validate districts chosen. \\ \hline
\end{tabular}
\caption{Dialect selection criteria}
\label{tab:dialect_selection}
\end{table}
\begin{table}[htbp]
\vspace{-1cm}
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.2}
\begin{adjustbox}{max width=\linewidth,max height=0.9\textheight}
\begin{tabular}{|l|l|p{3.5cm}|p{4cm}|}
\hline
\textbf{Language} & \textbf{Dialect} & \textbf{Core Districts} & \textbf{Justification} \\ \hline

% Bhojpuri-Maithili Section
\multirow{3}{*}{Bhojpuri} & Southern Standard & Saran & Representative of standard variety \\ \cline{2-4} 
& Northern & East Champaran, Deoria & Captures sub-varieties in Bihar and UP \\ \cline{2-4} 
& Western & Varanasi & Major urban center for Western dialect \\ \hline

\multirow{4}{*}{Magahi} & Central Variety & Gaya, Jahanabad & Representative of standard Magahi \\ \cline{2-4} 
& Southern & Jamui, Lakhisaray, Nawada & Distinct from central variety \\ \cline{2-4} 
& Western & Vaishali & Shows Bhojpuri influence \\ \cline{2-4} 
& NE (Surjapuri) & Kishanganj, Purnia, Katihar & Distinct northern variety \\ \hline

\multirow{4}{*}{Maithili} & Standard (Sotipura) & Darbhanga, Madhubani & Representative of standard variety \\ \cline{2-4} 
& Bajjika & Samastipur, Saharsa & Distinct morphological features \\ \cline{2-4} 
& Eastern (Thēthi) & Araria, Madhepura & Eastern variety with distinct features
 \\ \cline{2-4} 
& Angika & Bhagalpur & Originally classified under Magahi \\ \hline

% Bengali-Kannada Section
\multirow{5}{*}{Bengali} & Western & Purba/Paschim Medinipur & Shows Odia influence \\ \cline{2-4} 
& Varendri/Pundra & Malda (Core), Dakshin Dinajpur & Northern variety \\ \cline{2-4} 
& Rajbangsi & Jalpaiguri, Cooch Behar & Distinct northern dialect \\ \cline{2-4} 
& Jharkhandi & Purulia, Bankura & Variety spoken in Jharkhand region \\ \cline{2-4} 
& Standard & Kolkata, Nadia/Hooghly & Standard variety \\ \hline

\multirow{5}{*}{Kannada} & Hyderabad & Bellary & Formerly classified as Central \& Hyderabad Karnataka \\ \cline{2-4} 
& Mangalore & Dakshin Kannada (Mangalore) & Coastal variety \\ \cline{2-4} 
& Dharwad & Dharwad, Uttar Kannada & North Western variety\\ \cline{2-4} 
& NE & Gulbarga & Shows strong Urdu influence \\ \cline{2-4} 
& Mysore & Mysore Rural, Mandya & Southern standard variety \\ \hline

% Other Languages Section
\multirow{4}{*}{Chhattisgarhi} & Kedri (Central) & Bilaspur, Durg & Central standard variety \\ \cline{2-4} 
& Utti (Eastern) & Raigarh & Eastern variety \\ \cline{2-4} 
& Budati/Khatahi & Kabirdham, Balaghat & Western variety \\ \cline{2-4} 
& Bhandar & Sarguja & Northern variety \\ \hline

\multirow{4}{*}{Telugu} & Mid-Coastal & Guntur, Krishna & Central variety \\ \cline{2-4} 
& Rayalseema & Chittoor, Anantpur & Southern variety \\ \cline{2-4} 
& Telangana & Karimnagar, Nalgonda & Northern variety \\ \cline{2-4} 
& Utterandhra & Vishakapattanam, Srikakulam & Eastern variety \\ \hline

\multirow{4}{*}{Marathi} & S Konkan & Sindhudurga & Coastal south \\ \cline{2-4} 
& N Konkan & Dhule, Nashik & Coastal north \\ \cline{2-4} 
& Varhadi & Nagpur Rural & Eastern variety \\ \cline{2-4} 
& Standard & Pune Rural & Standard variety \\ \hline

\multirow{5}{*}{Hindi} & Hindustani+Malvi & Muzaffarnagar & Phonological similarities \\ \cline{2-4} 
& Kannauji+Braj & Etah & Transitional district with speakers of both dialects \\ \cline{2-4} 
& Awadhi+Bundeli & Hamirpur & Transitional district with speakers of both dialects
 \\ \cline{2-4} 
& Marwari+Dhundhari & Nagaur & Phonological similarities \\ \cline{2-4} 
& Garhwali & Tehri Garhwal & Distinct variety requiring separate collection \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Language-wise Dialect and Districts Selected}
\label{tab:dialect_selected}
\end{table}
Based on the dialect selection criteria, Table\ref{tab:dialect_selected} lists the dialects selected along with the district names where corresponding dialects and spoken. For each selected dialect Table \ref{tab:dialect_selected} also gives the reasoning to why the dialect was considered, and how it enriches dialectal variety in the corpus. \\\\
Some notes which were considered while finalization of dialects in Table\ref{tab:dialect_selected}:
\begin{itemize}
    \item Bhojpuri: Nagpuri was excluded as it is now classified as a separate language.
    \item Magahi: Eastern Magahi cluster was excluded due to internal variation. Maithili-mixed Magahi ("Angika") was reclassified under Maithili.
    \item Maithili: Bajjika and Angika could potentially be classified as separate languages.
    \item  Bengali: Eastern and South-eastern dialects were excluded as they are primarily spoken in Bangladesh.
    \item Chhattisgarhi: Rakshahun (Southern) dialect was excluded due to the smaller speaker population.
    \item Marathi:  Zadi Boli was excluded due to operational difficulties in data collection.
    \item Hindi: Due to the large number of dialects in Hindi (50+ according to the 2011 census), a different approach was adopted. Instead of capturing dialectal variations, accent variations were prioritized to create a dataset that could support speech recognition for 70-90\% of Hindi speakers.
\end{itemize}

\section{Text Data Preparation and Validation}
\label{appendix:text_validation}
Text data collection process was completed in two phases: 1) In phase 1, minimum of 5k sentences were collected per language. These sentences were primarily interrogative, 2) In phase 2, we targeted to collect  $\sim$15K sentences, which resulted in atleast 20K sentences per language uniformly distributed over language dialects and domains (agriculture and finance). In phase 2, sentences all types of sentences were collected with primary focus on covering as much as dialectal variations.

\subsection{Phase 1: Preparation and Validation}
\subsubsection{Identification of Domains for Data collection}
For the development of the ASR corpus, data was systematically collected from financial and agriculture domains, following a structured topic selection process as detailed below. The finalised topics were then used to create topic-specific google forms with a set template of questions in the 9 languages chosen for data collection.\\\\
\textbf{Mapping Agricultural \& Financial Services in India:}
\begin{enumerate}
    \item Comprehensive market research to identify existing agricultural and financial service providers
    \item Systematic cataloging of features, services, and data structures in current applications
    \item Analysis of products or services relevant to the Indian market context
    \item Evaluation of mobile and web applications designed for low-literacy populations in the finance and agriculture domains
    \item Assessment of interaction topics and advisory systems within the products to identify relevant market topics for which agents may be deployed as a final-use case from the models created and open-sourced
\end{enumerate}\\
\textbf{Lexical Resources Development:}
\begin{enumerate}
    \item Keyword mining through domain-specific Internet research
    \item Identification of topic clusters relevant to the Indian agricultural and financial ecosystem
    \item Development of semantic categorization frameworks for collected terminology into topics for google form creation
\end{enumerate}
Table\ref{tab:agricultural_categories} and Table\ref{tab:financial_categories} enumerate the subtopics which were chosen for the agriculture and finance domain respectively.
\input{Styles/tables/topics_agri}
\input{Styles/tables/topics_finance}
\subsubsection{Text data collection from Domain experts}
Text data were collected from Domain Experts via Google Forms and then submitted to the Validation Team for verification and finalization before being uploaded for Voice Collection.  The figure below shows a sample form for the subtopic “Climate and Weather” in Hindi, completed by a dialect-specific Domain Expert.  Figure\ref{fig:nt_form_comp} shows a sample form shared the dialect experts for sentence composition.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{Styles/figures_appendix/NT_composition_form.jpg}
    \caption{Google form used for phase1 text data collection}
    \label{fig:nt_form_comp}
\end{figure}

\subsubsection{Text Data Validation and Sanitisation}
Following the initial collection of textual data via standardized Google Forms completed by domain experts, the Validation Team performs a rigorous review to verify and finalize every submission. Only once an entry has been approved does it move forward to the Voice Collection phase.\\\\
\textbf{Step 1: Preliminary linguistic validation of Google Forms data} \\
Submissions that do not meet our linguistic quality standards are rejected with targeted feedback. Entries that pass this stage advance to the Sentence Validation and Sanitization process.

\begin{itemize}
    \item Authenticity check by LRMs
    \begin{itemize}
        \item Domain-Relevancy: Representation of genuine domain-specific sentences relevant to the sub-topic in each form.
        \item Vocabulary-Richness: Inclusion of general and specialized vocabulary representative of spoken-language interactions under each sub-topic.
        \item Conversationally Natural: Coverage of typical expert-user communication patterns.
    \end{itemize}
    \item Linguistic Diversity Check by Computational Linguist
    \begin{itemize}
        \item Domain-Relevancy: Representation of domain-specific terminology variations
        \item Dialectal Coverage: Samples from regional and socio-geographic varieties to reflect pronunciation and lexical differences.
        \item Demographic Balance: Balanced representation across demographic variables such as age, literacy level and gender etc.
        \item Sociolinguistic Documentation: Documentation of socio-linguistic variations and patterns relevant to ASR development.
    \end{itemize}
\end{itemize} \\\\
\textbf{Step-2: Systematic Text Data Validation Protocol} \\
\begin{itemize}
    \item Validator Preparation: Linguistic consultants (LCs) were briefed comprehensively before commencing validation tasks. The orientation included:
    \begin{itemize}
        \item Data collection methodology and form structure used in text data collection
        \item Expected response format specifications from domain experts
        \item Domain-specific terminological requirements
        \item Dialectal variations across regional data sources
    \end{itemize}
    \item Preliminary Validation Criteria: During initial assessment, validators identified typical error patterns including:
    \begin{itemize}
        \item Non-interrogative responses (statements rather than questions)
        \item Script violations (non-native script usage)
        \item Repetitive submissions across respondents
        \item Transliteration inconsistencies in native language content
    \end{itemize}
    \item{Preliminary Validation Procedures: Upon completion of domain-specific forms, validators executed a systematic verification process that resulted in grammatically accurate regional language sentences along with their translations in English:}
    \begin{itemize}
        \item Verified response format compliance (questions vs. statements)
        \item Assessed domain relevance and eliminated non-agricultural / non-financial content
        \item Identified duplicate submissions across participants
        \item Evaluated dialectal authenticity and regional representation
        \item Performed grammatical and orthographic normalization
        \item Flagged ambiguous content for expert resolution
        \item Translated validated content into reference language.
    \end{itemize}
    \item{Quality Assessment Metrics: The validation team conducted comprehensive evaluation including:}
    \begin{itemize}
        \item Verification of validator annotations and corrections
        \item Cross-validation of automated and manual translations
        \item Documentation of error patterns for methodological refinement
        \item Compensation calculations based on validated submissions
    \end{itemize}
\end{itemize}
\\\\
\textbf{Step-3: Systematic Text data sanitisation Protocol:}\\
The text data received from domain experts after systematic text validation, further undergoes a multi-stage text validation, sanitisation and review process by the Validation Team. It is first split by domain, dialect, and language, then assigned to dialect-specific language consultants. Each consultant then carries out sanitisation of the text corpus.

\begin{itemize}
    \item Corpus Standardization: The validated corpus underwent systematic processing to ensure consistency:
    \begin{itemize}
        \item Feature categorization and keyword extraction from the sentences
        \item Alternative categorization for cross-domain content by providing feature categories that were more relevant
        \item Standardization of punctuation and special characters
        \item Normalization of numerical expressions and units
        \item Expansion of abbreviations and acronyms
        \item Standardization of capitalization patterns
    \end{itemize}
    
    \item Transliteration Management: Specialized handling was implemented for multilingual elements:
    \begin{itemize}
        \item Consistent transliteration of non-native terms
        \item Separation of inflectional morphology from borrowed terms
        \item Documentation of transliteration alternatives for ambiguous terms
        \item Phonological Coverage Analysis
    \end{itemize}
    
    \item Phonetic Distribution Verification:
    \begin{itemize}
        \item Computational linguists performed comprehensive phonological analysis:
        \begin{itemize}
            \item Monophone distribution compared against language reference data
            \item Diphone coverage verification through expert validation
            \item Triphone frequency assessment against corpus benchmarks
            \item Corpus Augmentation Strategy
        \end{itemize}
        \item Targeted corpus enrichment was conducted to address phonological gaps:
        \begin{itemize}
            \item Identification of domain-relevant vocabulary for augmentation
            \item Systematic creation of phonologically balanced supplementary content
            \item Integration of regional variants to ensure dialectal representation
        \end{itemize}
    \end{itemize}    
\end{itemize}
In Table\ref{tab:data_entries_val} we have a couple of Marwari-region Hindi sentences from the agricultural domain, as reviewed and corrected by the Validation Team.

\begin{table}[htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|>{\raggedright}p{1.2cm}|c|c|>{\raggedright}p{1.8cm}|>{\raggedright\arraybackslash}p{2.8cm}|>{\raggedright\arraybackslash}p{2.8cm}|>{\raggedright\arraybackslash}p{2.5cm}|}
\hline
\textbf{DE Name} & \textbf{Pin Code} & \textbf{District} & \textbf{Feature Category} & \textbf{Sentence provided by DE} & \textbf{Corrected sentence in Hindi by LC} & \textbf{Translation provided by LC} \\ \hline
XXXX & 341319 & Nagaur & 
1 - Crop Names \& Seasons & 
\texthindi{अगेती रबी मौसम के अंतर्गत कौन से महीने आते हैं?} & 
\texthindi{अगेती रबी मौसम के अंतर्गत कौनसे महीने आते हैं?} & 
Which months fall under early rabi season? \\ \hline
XXXX & 341319 & Nagaur & 
2 - Seeds, Varieties \& Hybrids & 
\texthindi{मक्का की सबसे अच्छी उपज वाली किस्म कोनसी है?} & 
\texthindi{मक्का की सबसे अच्छी उपज वाली किस्म कौनसी है?} & 
Best yielding variety of maize? \\ \hline
\end{tabular}
\end{adjustbox}
\caption{ Illustration of Text data validation}
\label{tab:data_entries_val}
\end{table}
We programmed scripts to automatically tag issues, helping the Validation Team with their preliminary screening. Table\ref{tab:data_entries_autotag} shows an example for a single sentence to illustrate this process. In the actual validation workflow, the items generated for each field in every sentence are presented as columns in a Google Sheet.
In the example provided in the table, the script tagged the following:
\begin{enumerate}
    \item Special symbol: “?”
    \item Abbreviation: “HP”
    \item Numeric translation: “\texthindi{आठ}” (meaning “eight”)
    \item Letter transliteration: “\texthindi{के}” (this could be the English letter “K” or the Hindi postposition “\texthindi{के}”)
\end{enumerate}
The data underwent multiple rounds of automatic issue tagging, followed by manual corrections by language consultants and a subsequent multi-stage review by the Validation Team.

\begin{table}[htbp]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.5} % Increase row height
\setlength{\tabcolsep}{8pt} % Increase cell padding
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|l|p{4cm}|p{4cm}|}
\hline 
\textbf{Field Type} & \textbf{Example Sentence 1} & \textbf{Example Sentence 2} \\ \hline

LC Sentences & 
\texthindi{अगेती रबी मौसम के अंतर्गत कौनसे महीने आते हैं ?} & 
\texthindi{आठ} H.P \texthindi{की मोटर चलाने के लिए बिजली का कौनसा कनेक्शन लेना होगा ?} \\ \hline

Transliteration & 
ageetii rabii mousam kee atargat koun see mahiinee aatee hei & 
aaṭh eecapii kii moṭar calaanee kee liee bijalii kaa kounasaa kaneekśan leenaa hogaa \\ \hline

Translations & 
Which months come under the early rabi season? & 
Which connectivity of electricity we should use to run eight H.P motor? \\ \hline

Special Symbols & 
{[}'?'{]} & 
{[}'.', '?'{]} \\ \hline

Acronyms & 
{[}{]} & 
{[}{]} \\ \hline

Roman Numerals & 
{[}{]} & 
{[}{]} \\ \hline

Alphanumerics & 
{[}{]} & 
{[}{]} \\ \hline

Numerics & 
{[}{]} & 
{[}{]} \\ \hline

English Words & 
{[}{]} & 
{[}{]} \\ \hline

Abbreviations & 
{[}{]} & 
{[}H.P{]} \\ \hline

Letters & 
{[}{]} & 
{[}{]} \\ \hline

Letter Transliteration & 
{[}'\texthindi{के}'{]} & 
{[}'\texthindi{के}'{]} \\ \hline

Numeric Translation & 
{[}{]} & 
{[}'\texthindi{आठ}'{]} \\ \hline

Non Whitespaces & 
{[}{]} & 
{[}{]} \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Illustration of automatic tagging of text for text data validation}
\label{tab:data_entries_autotag}
\end{table}

\subsection{Phase 2: Preparation and Validation}
\subsubsection{Domain and Topic Coverage}
In phase 2, we take a more diverse and extensive collection approach. Which starts with curating a comprehensive set of topics. This initiative aimed to pinpoint specific areas within these domains, facilitating the generation of ideas for individuals unfamiliar with the subject matter. Careful consideration was given to encompassing the primary facets of each domain, leaving no stone unturned.The compilation process involved manual curation utilizing a variety of sources including magazines, Wikipedia, diverse websites, and pertinent literature. Magazines sourced from various online platforms, academic institutions, and research facilities contributed to the breadth of topics. Additionally, the outline articles page on Wikipedia served as a valuable resource, providing a structured approach to identifying relevant subjects and retrieving corresponding web links for reference purposes. Upon completion, the finalized list comprised approximately 1500 topics, each accompanied by pertinent web links for further exploration. Compared to Phase 1, Phase 2 has almost 100x topics, which makes the composed data more diverse. Beginning with the identification of key fields within agriculture and finance, the list then delved into more granular subtopics, covering everything from the nuances of sugarcane cultivation to the intricacies of UPI pin setup and transaction history checking in UPI apps.

\subsubsection{Sentence Preparation/Composition}
Primary source for corpus creation was composition of sentences by selected language experts with reference to curated topics' lists. In some cases where target of \~15K sentences per language was not achieved, strategies like scraping web (for high resource language) and  translating to low resource dialects/languages, translating sentences from sentence rich dialect to rest of the dialects were adopted. \\\\
    
\textbf{Sentence Composition}\\
The digital data accessible in standard language format, while extensive in quantity, often lacks the essence of colloquial style or local dialect. Despite comprising millions of sentences in major languages, it falls short of capturing the regional nuances and variations crucial for authenticity. Thus, the endeavor to capture colloquial expressions specific to each region arises.

Recognizing this gap, our approach has shifted towards sourcing sentences from individuals native to the respective districts. By engaging native speakers, we aim to gather text imbued with the distinct local flavor, ensuring authenticity and resonance with the targeted audience. This strategy not only enriches our linguistic database but also fosters a deeper understanding of regional linguistic diversity and cultural nuances. 
Given the designated topics, the composers were tasked with crafting conversational and colloquial sentences aimed at enhancing the Automatic Speech Recognition (ASR) performance of the language model. However, navigating the intricacies of dialect variability posed significant challenges, particularly considering the subtle linguistic differences even within a small radius of 5 kilometers within a district.

Despite these challenges, the inherent diversity of language presented an opportunity rather than a hindrance. Recognizing the inherent versatility of language and the absence of standardized norms in certain dialects, our approach embraced the diverse range of linguistic expressions and variations within the dialect. This inclusive stance allowed for the incorporation of a wide spectrum of dialectal nuances, ensuring that the resultant dataset reflected the rich tapestry of regional linguistic diversity.
Certainly, adhering to these rules while composing sentences was paramount to ensure consistency and usability:
\begin{enumerate}
    \item Character Length Limit: Each sentence should not exceed 120 characters to maintain readability and consistency.
    \item Avoid Pronoun Start: Sentences should refrain from commencing with a pronoun to ensure coherence, as each sentence is treated independently.
    \item Numerical Representation: Numbers must be written in word format to enhance readability and comprehension.
    \item Special Character Usage: Only dot (.), comma (,), and question mark (?) are permissible; refrain from using other special characters to maintain uniformity.
    \item Avoid Controversial Statements: Steering clear of controversial statements to uphold neutrality and objectivity in the dataset.
    \item Stick to Given Topic: Ensure sentences remain relevant to the assigned topic to maintain focus and coherence.
    \item Maintain Balance: Strive for equilibrium between agriculture and finance sentences to provide a well-rounded dataset.
    \item Acronym Formatting: Acronyms should adhere to a specific format, such as A.T.M, ensuring consistency and clarity in representation.
\end{enumerate}
Adhering to these guidelines guarantees the creation of a cohesive and standardized dataset conducive to enhancing the performance and reliability of the language model.\\\\

\textbf{Translation}\\
To achieve the target of 15K sentence per language, some percentage of sentences were translated across multiple dialects by language experts. The source from which the sentences were translated was either a language's own standard dialect or a different language entirely.

\subsubsection{Text Corpus Validation}
After the data composition is completed for each language, the data is passed through a pipeline consisting of several automated and manual checks by language validators. 

As the sentences are composed by multiple language personnels, the data at hand is bound to have some errors and deviations from the correct version. As this text corpus is to be utilized as stimuli for a crowd-sourced recording application for audio data collection. Sentences need to be accurate, un-ambiguous, coherent and compatible with the recording application. Thus making the use of the validation pipeline imperative. 

The validation pipeline consists of several automatic and manual checks, targeted to specific categories of errors. These categories define the architecture of the validation pipeline, which is almost similar for all languages but is adapted to each language and includes some language level checks as well.\\\\
Major categories of checks/errors are as follows:
\begin{enumerate}
    \item Duplicate sentence removal (Automatic) :\\
    Given the set of raw sentences, we run a pair wise Word Error Rate (WER) analysis to remove any duplicate sentences. Implementing this check in the early stages reduces unnecessary stress in the validation pipeline. \\
    
    \item Invalid character check and correction (Manual) :\\
    Given the set of sentences after previous checks, here we remove any non-printable, new line and extra white space characters.Then the list of characters in the given sentences is generated and given to the language validator. Sentences with flagged characters are filtered and given to the validator for corrections and merged with the database. We re-iterate this process till all corrections are done properly and no invalid characters are present in the database. Validators are pre-informed that only language characters (alphabets and numerals) and special characters(comma, full stop and question mark) are valid characters.\\
    
    \item Sentence pruning to specified length (Manual) :\\
    Due to the restriction in the recording application, sentence length was restricted to 90 characters. All sentences exceeding the threshold were given to validators for pruning/rejection.\\
    
    \item Acronyms standardization (Manual) :\\
    A standard format ("A.B.C" where A, B and C are characters of the given acronym) is followed for all acronyms occurring in the corpus. To validate this, all words with full stop are filtered and flagged by validator if its either not a acronym or not following the standard format. Sentences with flagged words are filtered and given for correction to the validator. Apart from correcting the given acronyms (containing full stop), we also filter out the sentences where acronyms without full stop are present. This is achieved by detecting transliterated English characters and also searching with existing acronym list.\\
    
    \item Invalid matra check and correction (Manual) :\\
    In this check, consecutive usage of matras in words are detected and given to the validators for correction. Flagged cases include incorrect usage of matras and  multiple overlayed usage of matras (appearance remains same).\\
    
    \item Interchangeably used characters' words check and correction (Manual) :\\
    In this check, we take a set of characters from the language validators which can be interchangeably used by mistake. Word lists with characters are given to the validators and sentences with flagged words are corrected. This check resolves spelling mistakes in the corpus.\\
    
    \item Similar sentences check (Manual) :\\
    This check is an extension of check 1. Here sentence pairs with 0$<$WER$<$0.3 are given to validators and check if any sentence needs to be rejected.\\
    
    \item Homophones check (Manual) :\\
    Using the phonetic spellings of vocabulary (given by Navana Tech), WER analysis on the word pairs' phonetic spellings is done to get all the homophones present in the corpus. Validators check these homophone pairs and flag the word with correct spellings.\\
    
    \item Language specific checks :\\
    Checks 1 to 8 almost cover all the validation pipeline. In some languages extra checks were required, those are explained in respective langauge sections.
\end{enumerate}
Checks in validation pipeline were conducted in a versioning setup i.e., after each set of checks and manual validation, a new version of database is created to maintain backups and history in case of rollbacks in checks. Checks which are independent are done in a single version whereas in case where checks are dependent, they are performed in a sequential manner.

\input{Styles/tables/text_sources}
Table \ref{tab:sentence_statistics} gives Phase2 Text-Corpus details like number of sentences with their split in agriculture and finance domains, along with details on the method of creation like composition or translation. Details on number of dialect experts who either composed or translated the sentences are also provided with their pincode distribution.
\input{Styles/tables/text_stats}
The overall text corpus dialect level details like total samples, sample collected in Phase 1, vocab size and average words in a sentence are given in Table \ref{tab:text_stats}.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Voice Data Acquisition}

% \subsection{Onboarding Human Resources}

% The voice data providers are referred to as \textbf{Voice Participants}. Preliminary Audio Validators review sample recordings by comparing the audio against the provided transcript to ensure accuracy, provide early feedback, and remove any Voice Participants who fail to meet basic quality standards from the collection process.

% The Validation Team follows the same structure used for text data validation and sanitization, as described in the previous section. The only difference is that Language Consultants (LCs) undergo extensive screening tests, interviews, and specialized training; they are also provisioned with custom audio-validation tools and supported by a comprehensive training and feedback loop before being onboarded for audio data validation tasks.

% \paragraph{Voice Participants and Preliminary Audio Validators:}

% \subparagraph{Voice Participant Qualification Criteria}
% \begin{itemize}
% \item \textbf{Geographic Authenticity}: Resident of the district/village where the target dialect is natively spoken.
% \item \textbf{Literacy Criteria}: Must have the ability to read and speak the dialect.
% \item \textbf{Smartphone usage}: Have the ability to navigate a voice collection app on a smartphone and have the ability to read and record the sentences provided.
% \item \textbf{Age and Gender requirements}: Must be above the legal age of 18 and must fall into the age and gender buckets specified for the data collection process.
% \end{itemize}

% \subparagraph{Preliminary Audio Validators Qualification Criteria}
% Preliminary Audio validators response is used to determine the extent of payments that can be disbursed to each participant. A voice validator must meet the following requirements:
% \begin{itemize}
% \item \textbf{Linguistic Expertise}: Native-level proficiency in the target language and its dialectal variations
% \item \textbf{Phonetic Training}: Demonstrated ability to recognize phonetic variations and pronunciation errors
% \item \textbf{Audio Accuracy Check}: Advanced ability to verify text-to-speech alignment in the target language
% \item \textbf{Technical Proficiency}: Competence in using smartphones
% \item \textbf{Concentration Stamina}: Ability to maintain focused attention during extended validation sessions
% \item \textbf{Auditory Acuity}: Excellent hearing capabilities for detecting subtle audio quality issues
% \item \textbf{Consistency}: Demonstrated ability to apply standardized validation criteria uniformly
% \item \textbf{Dialectal Knowledge}: Familiarity with regional accents and acceptable pronunciation variations
% \end{itemize}

% \subsection{Voice Data Collection}

% \paragraph{Technical Implementation}
% The data collection platform incorporated several technical components:
% \begin{enumerate}
% \item WhatsApp-based authentication and integration system
% \item “Bolo” Mobile application with guided recording interface
% \item Manual Validation of voice data
% \item Server-based validation algorithms
% \end{enumerate}

% \paragraph{WhatsApp Integration System}
% To ensure participant verification, a dedicated authentication system was implemented through WhatsApp messaging platform. The "Bolo Code Bot" served as the primary verification gateway for voice collection participants through a multi-stage process. Only the participants who were vetted by the partner team were provided with a code to enroll themselves on the “Bolo App” along with an application link.

% \begin{itemize}
% \item \textbf{Pre-registration Validation}: Voice collection partners submitted participant phone numbers for database registration prior to recording sessions
% \item \textbf{Secure Access Management}: The WhatsApp-based bot provided unique access codes exclusively to registered participants
% \item \textbf{Authentication Gating}: Unregistered users attempting to access the system received error notifications directing them to contact project coordinators
% \item \textbf{Multilingual Interface}: The bot supported communications in regional languages to accommodate participants with varying language preferences
% \item \textbf{Technical Onboarding}: Successfully authenticated participants received automated instructions for mobile application installation and configuration
% \item \textbf{Tutorial Distribution}: The system distributed video tutorials demonstrating proper recording techniques and application usage
% \item \textbf{Protocol Enforcement and Security}: This authentication mechanism ensured that only vetted participants meeting demographic and dialectal criteria could contribute to the corpus
% \end{itemize}

% \begin{figure}[htbp]
% \centering
% \resizebox{0.65\linewidth}{!}{%
% \begin{tabular}{cc}
% \begin{subfigure}[b]{0.48\linewidth}
%     \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image1.jpg}
%     \caption{Failure scenario — No code sent to a user who is not registered with us.}
% \end{subfigure} &
% \begin{subfigure}[b]{0.48\linewidth}
%     \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image2.jpg}
%     \caption{Success scenario — Code sent to a user whose number is registered on the database.}
% \end{subfigure}
% \end{tabular}%
% }
% \caption{WhatsApp code delivery scenarios via Bolo Code Bot: Left shows failure for unregistered users; right shows successful code generation for verified users.}
% \end{figure}

% \paragraph{Data Collection Architecture}
% The voice data collection process followed a structured implementation framework:
% \begin{enumerate}
% \item Regional partners identified and trained voice collection participants (VCPs)
% \item Registered participants received access credentials via WhatsApp messaging system
% \item Participants utilized a dedicated mobile application ("Bolo App") for recording contributions
% \item Server-side validation assessed recording quality parameters and read-speech accuracy
% \item Compensation was calculated based on validated submissions
% \end{enumerate}

% \paragraph{Recording Task Distribution}
% The corpus design for voice collection included multiple speech collection categories totaling approximately 1152 hours per language:
% \begin{itemize}
% \item 556 hours of phonetically balanced sentences from academic sources from IISc
% \item 556 hours of domain-specific sentences developed by the field data collection team and Domain Experts by Navana Tech
% \item 10 hours of agricultural domain common sentences to serve as a test set
% \item 10 hours of banking domain common sentences to serve as a test set
% \item 22 hours of spontaneous speech tasks with open ended questions to collect free-form speech from the participants
% \end{itemize}

% \begin{table}[htbp]
% \centering
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Task Type} & \textbf{Total hours} & \textbf{3 dialects} & \textbf{4 dialects} & \textbf{5 dialects} \\\hline
% IISc & 556 & 185.33 & 139.00 & 111.20 \\\hline
% Navana & 556 & 185.33 & 139.00 & 111.20 \\\hline
% Common Agri & 10 & 3.33 & 2.50 & 2.00 \\\hline
% Common Bank & 10 & 3.33 & 2.50 & 2.00 \\\hline
% Spontaneous & 22 & 7.33 & 5.50 & 4.40 \\\hline
% \end{tabular}
% \caption{Dialect-wise split of the total number of hours required per task in languages on the basis of the total number of dialects.}
% \end{table}

% \paragraph{Read Speech Collection}
% Each participant contributed to read speech collection through systematically presented prompts:
% \begin{itemize}
% \item 279 sentences from phonetically balanced academic sources and internet scraping (IISc dataset)
% \item 278 sentences from domain-specific corpus from agricultural and banking experts (Navana dataset)
% \item 5 common sentences from agricultural domain
% \item 5 common sentences from banking domain
% \end{itemize}

% \paragraph{Spontaneous Speech Collection}
% Supplementary to read speech, spontaneous speech was elicited through 10 open-ended questions presented to each participant. Responses were intended to capture natural speech patterns and prosodic features.

% Finalised questions for spontaneous data collection that were translated into the respective languages and added to the application towards the end of the collection process in order to not confuse the participant with the read speech approach.

% \begin{itemize}
% \item What are the different dishes you usually eat in a day?
% \item Could you please describe a festival that is very famous in your locality?
% \item What is your favorite vegetable and what dishes are made using it?
% \item What tasks do you perform in the day after you wake up?
% \item What activities do you like to do in your free time? What are the religious places you visit and where are they located?
% \item How has COVID affected you and your family?
% \item What places would you recommend to a traveller visiting your hometown?
% \item Is there any folklore associated with your hometown? Can you describe it for us?
% \item What do you usually do on your smartphone, what apps do you use?
% \item What did you like about this project you have participated in?
% \item What did you dislike about this project you have participated in?
% \item Could you please describe any story you tell a child to put them to sleep?
% \item Who are the different members of your family, and what do they do?
% \item What is your favorite place? Can you describe any fond memory you have from this place?
% \item How has your neighbourhood changed in the last ten years?
% \item What is your favorite movie or serial? Please describe the story.
% \item What was your favorite game to play as a child. Can you tell us the rules of the game?
% \item Can you describe a fond memory from your childhood?
% \item If you had to travel to a nearby big city, how would you plan the travel?
% \item What are the qualities about your friends that you like and why?
% \end{itemize}

% \paragraph{Mobile Application Voice Recording}

% \subparagraph{Bolo Application Workflow}
% A specialized mobile application ("Bolo App") was developed to facilitate structured voice data collection from authenticated participants. The application implemented a comprehensive user journey designed for accessibility and data quality:

% \textbf{Authentication Process}
% \begin{enumerate}
% \item Mobile number verification through two-factor authentication (SMS OTP)
% \item Participants entered their unique 16-digit access code provided via the WhatsApp bot
% \item System cross-validated participant credentials against the registered database
% \item Privacy policy acceptance required before proceeding with data collection
% \end{enumerate}

% \begin{figure}[htbp]
% \centering
% \resizebox{0.65\linewidth}{!}{%
% \begin{tabular}{cc}
% \begin{subfigure}[b]{0.48\linewidth}
%     \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image3.jpg}
%     \caption{Mobile number entry screen: Participants input their mobile number to initiate the verification process.}
% \end{subfigure} &
% \begin{subfigure}[b]{0.48\linewidth}
%     \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image4.jpg}
%     \caption{OTP input screen: Participants enter the verification code received via SMS.}
% \end{subfigure}
% \end{tabular}%
% }
% \caption{Two-step authentication workflow on the Bolo App using mobile number and OTP verification.}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \resizebox{0.65\linewidth}{!}{%
% \begin{tabular}{cc}
% \begin{subfigure}[b]{0.48\linewidth}
%     \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image5.jpg}
%     \caption{Access code authentication from WhatsApp bot: Screen where participants input the received code to proceed.}
% \end{subfigure} &
% \begin{subfigure}[b]{0.48\linewidth}
%     \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image6.jpg}
%     \caption{Privacy policy acceptance screen: User must agree before starting the voice data collection process.}
% \end{subfigure}
% \end{tabular}%
% }
% \caption{Screens showing secure onboarding: Access code entry and privacy policy confirmation in the Bolo App.}
% \end{figure}

% \textbf{Participant Profile Addition} — Involved the collection of demographic metadata including:
% \begin{itemize}
% \item Profile photo for participant identification – Optional for participants to respect their privacy.
% \item Gender specification for corpus balancing – to ensure that the demographic requirements are met.
% \item Birth year for age distribution documentation to ensure that all age groups are meeting the minimum
% \item Geographic location through pincode entry.
% \end{itemize}

% \begin{figure}[htbp]
% \centering
% \resizebox{0.65\linewidth}{!}{%
% \begin{tabular}{cc}
% \begin{subfigure}[b]{0.48\linewidth}
%     \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image7.jpg}
%     \caption{Profile screen: Participants optionally upload a profile photo for identification.}
% \end{subfigure} &
% \begin{subfigure}[b]{0.48\linewidth}
%     \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image8.jpg}
%     \caption{Gender selection screen: Used to ensure demographic balance in corpus collection.}
% \end{subfigure}
% \end{tabular}%
% }
% \caption{Screens for profile and gender selection during participant onboarding.}
% \end{figure}


% \begin{figure}[htbp]
% \centering
% \resizebox{0.65\linewidth}{!}{%
% \begin{tabular}{cc}
% \begin{subfigure}[b]{0.48\linewidth}
%     \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image9.jpg}
%     \caption{Birth year input: Helps capture age group information of the participant.}
% \end{subfigure} &
% \begin{subfigure}[b]{0.48\linewidth}
%     \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image10.jpg}
%     \caption{Pincode input: Used to log participants' geographic origin for dialect validation.}
% \end{subfigure}
% \end{tabular}%
% }
% \caption{Screens to collect age and location metadata during participant registration.}
% \end{figure}


% \textbf{Task Management Interface}

% Participants were presented with categorized recording assignments:
% \begin{itemize}
% \item IISc Sentences
% \item NavanaTech Sentences
% \item Common sentences across agricultural domain (5 sentences)
% \item Common sentences across banking domain (5 sentences)
% \item Spontaneous speech elicitation prompts
% \end{itemize}

% Visual progress tracking displayed completion metrics such as:
% \begin{itemize}
% \item Available sentences within each category
% \item Completed recording count
% \item Submission status indicators
% \item Verification status feedback
% \end{itemize}

% % \begin{figure}[htbp]
% % \centering
% % \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image11.jpg}
% % \caption{Task management interface and progress dashboard}
% % \end{figure}

% \textbf{Recording Interface features}
% \begin{itemize}
% \item Sentence presented in native script
% \item Intuitive microphone activation control
% \item Audio waveform visualization during recording
% \item Playback capability for participant self-verification
% \item Navigation controls for sequential task progression
% \item Session management with task resumption capability
% \end{itemize}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.4\linewidth]{Styles/figures_appendix/audio_image11.jpg}
% \caption{Recording screen interface}
% \end{figure}

% \paragraph{Audio Data Validation}
% The recorded audio data underwent a structured validation pipeline combining manual and hybrid manual-automated checks.

% Initially, about 5\% of the utterances per dialect were manually validated to assess quality of recorded audio with respect to the corresponding text.

% \subparagraph{Manual Data Validation Process (Work in Progress)}
% Submitted recordings underwent multi-dimensional quality verification:
% \begin{itemize}
% \item Transcription accuracy assessment
% \item Audio volume level evaluation
% \item Recording clarity and environmental noise analysis
% \end{itemize}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.4\linewidth]{Styles/figures_appendix/audio_image12.jpg}
% \caption{Manual app-based validation interface}
% \end{figure}

% \subparagraph{Hybrid manual-automated Process (Work in Progress)}
% Dialectwise datasets were partitioned into three quality slabs – Clean, Semi-noisy, and Noisy. The slab assignment reflects the proportion of audio-text pairs that are exact matches: the clean slab contains the highest percentage of perfectly aligned utterances, while the noisy slab contains the least. This slab-based categorization allows downstream tasks to select data based on quality requirements and robustness needs.

% \textbf{Definition of slab:}
% \begin{itemize}
% \item For non-Bengali and non-Bhojpuri language:
%   \begin{itemize}
%   \item Clean: Expected error: ~5\%
%   \item Semi-noisy: Expected error: ~12\%
%   \item Noisy: Expected error: at least 15\%
%   \end{itemize}
% \item Bengali and Bhojpuri language expected to have higher error (~5\%+) in each of the slab
% \end{itemize}

% \textbf{Definition of error:}
% \begin{itemize}
% \item 5\% error means, if one samples 100 audio files, they are likely to observe that 5 out of 100 audio files would have mismatch between audio and the sentence
% \end{itemize}

% The voice data from the voice collection platform was stored in cloud storage. A task management platform connected to cloud storage allowed validators to connect to it and review audios for validations. The task management platform supported both Manual as well as Hybrid Manual-automated voice data validation, also enabling Multi stage review process.

% A validation model leveraged neural network based acoustic model to generate phone level and word level scores representing how well phones and words are spoken for a prompt sentence provided which is then used to generate a per utterance score.

% The details have been provided in Section~\ref{subsec:slab_score} on how a score per utterance is calculated. The multi stage validation and review process has been detailed in Section~\ref{subsec:slab_valid}.

% Post the multi stage review process that takes into consideration Neural Network model generated scores and a minimally manually validated audios sets to bucketise data into multiple slabs. The pseudocode to generate slabs have been provided in Section~\ref{subsec:slab_pseudocode}.

% \newpage
\section{Voice Data Acquisition}

\subsection{Human Resource Onboarding}

Voice data providers are referred to as \textbf{Voice Participants (VPs)}. Before contributing, they are screened by \textbf{Preliminary Audio Validators}, who compare recordings against text prompts to check for audio quality and reading accuracy. Participants failing to meet quality standards are removed early from the process.

The validation workflow parallels that of text validation, with a key distinction: \textbf{Language Consultants (LCs)} undergo rigorous screening, interviews, and specialized training. They are provided with custom audio validation tools and integrated into a continuous training and feedback loop before handling audio validation tasks.

\paragraph{Voice Participant Criteria}
\begin{itemize}
\item \textbf{Geographic Authenticity}: Must reside in the district or village where the target dialect is natively spoken.
\item \textbf{Literacy}: Must be able to read and speak the dialect.
\item \textbf{Smartphone Proficiency}: Able to navigate the collection app and record sentences.
\item \textbf{Demographic Compliance}: Must be over 18 years old and meet predefined age-gender quotas.
\end{itemize}

\paragraph{Preliminary Audio Validator Criteria}
Validators assess submission quality and influence payment decisions. Each validator must possess:
\begin{itemize}
\item Native-level dialectal fluency
\item Training in phonetic error detection
\item Strong text-audio alignment skills
\item Proficiency with smartphones
\item High concentration and consistency
\item Excellent auditory skills
\item Familiarity with regional accent variations
\end{itemize}

\subsection{Voice Data Collection Framework}

\paragraph{System Components}
\begin{enumerate}
\item WhatsApp-based authentication
\item Mobile app ("Bolo App") for guided recordings
\item Manual and hybrid validation pipelines
\item Cloud-based backend and task management
\end{enumerate}

\subsubsection{Participant Authentication via WhatsApp}

A dedicated WhatsApp bot, \textbf{Bolo Code Bot}, served as the primary interface for authenticating voice participants (see Figure~\ref{fig:whatsapp-auth}). Prior to participation, phone numbers were submitted by field partners and registered in a secure database. Only these pre-registered users received a unique 16-digit access code via WhatsApp, enabling them to log in to the Bolo application. If an unregistered user attempted access, the bot returned an error and redirected them to the project coordinator.

The authentication system supported multilingual communication to ensure accessibility for participants across various language backgrounds. Once authenticated, participants received onboarding materials including app installation instructions and video tutorials demonstrating the recording workflow. This gatekeeping mechanism ensured that only vetted participants who met dialectal and demographic criteria could proceed to record speech data, thereby enhancing both the security and quality of the collected corpus.

\begin{figure}[htbp]
\centering
\begin{tabular}{cc}
\begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image1.jpg}
    \caption{Failure scenario: unregistered user receives no code.}
\end{subfigure} &
\begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[width=\linewidth]{Styles/figures_appendix/audio_image2.jpg}
    \caption{Success scenario: registered user receives a code.}
\end{subfigure}
\end{tabular}
\caption{WhatsApp-based authentication via Bolo Code Bot.}
\label{fig:whatsapp-auth}
\end{figure}


\subsubsection{Data Collection Process}

The voice data collection pipeline involved close collaboration with regional partners, who were responsible for identifying, training, and onboarding voice participants (VPs). Once registered, participants were authenticated using the WhatsApp-based Bolo Code Bot and were granted access to the Bolo mobile application for guided speech recording. The submitted recordings were processed through a server-side validation system to assess both technical quality and alignment with the provided text. Compensation for each participant was calculated based on the number of validated contributions.

Each language corpus was designed to comprise 1152 hours of audio, distributed across five task categories. These included 556 hours of phonetically balanced sentences under Phase-2 and 556 hours of domain-specific content categorized under Phase-1. An additional 10 hours each were dedicated to a small set of common agricultural and common banking domain sentences. These sentences were designed to be spoken by a larger number of participants in order to capture broader speaker variation within the corpus. The final 22 hours were allocated to spontaneous speech tasks intended to elicit natural prosody and informal speaking styles.

The dialect-wise distribution of hours is summarized in Table~\ref{tab:task-split}. In cases where the language included five dialects, each dialect contributed approximately 111.20 hours per phase, whereas languages with three dialects allocated about 185.33 hours per dialect per phase.

\begin{table}[htbp]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Task Type} & \textbf{Total} & \textbf{3 Dialects} & \textbf{4 Dialects} & \textbf{5 Dialects} \\\hline
Phase-2 & 556 & 185.33 & 139.00 & 111.20 \\\hline
Phase-1 & 556 & 185.33 & 139.00 & 111.20 \\\hline
Common Agri & 10 & 3.33 & 2.50 & 2.00 \\\hline
Common Bank & 10 & 3.33 & 2.50 & 2.00 \\\hline
Spontaneous & 22 & 7.33 & 5.50 & 4.40 \\\hline
\end{tabular}
\caption{Dialect-wise task distribution per language.}
\label{tab:task-split}
\end{table}

Each participant was assigned 279 Phase-2 sentences and 278 Phase-1 sentences, along with five common agricultural and five common banking domain sentences. These common prompts were shared across all participants and served to increase the number of unique speakers per sentence, thereby enhancing speaker diversity in the dataset. 

In addition to read speech, participants were presented with 10 open-ended prompts selected from a pool of 20 to encourage spontaneous speech. Example prompts included: “What local dishes do you typically eat in a day?”, “Can you describe a well-known festival celebrated in your area?”, “What did you like or dislike about this project?”, “What stories do you narrate to children at bedtime?”, and “How has your neighborhood changed in the last ten years?”

Despite the intent to collect free-form, conversational speech, many participants ended up reading the questions aloud rather than responding naturally. Consequently, this subset of the data did not capture the desired spontaneous characteristics and was not included in the open-source release of the corpus.

\subsection{Bolo App Workflow}

\paragraph{Authentication Process}
As shown in Figures~\ref{fig:bolo-auth1} and~\ref{fig:bolo-auth2}, the app required:
\begin{enumerate}
\item OTP verification of mobile number
\item Entry of 16-digit access code from WhatsApp bot
\item Acceptance of the privacy policy
\end{enumerate}

\begin{figure}[htbp]
\centering
\resizebox{0.65\linewidth}{!}{%
\begin{tabular}{cc}
\includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image3.jpg} &
\includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image4.jpg}
\end{tabular}%
}
\caption{Bolo App: Mobile number and OTP verification screens.}
\label{fig:bolo-auth1}
\end{figure}

\begin{figure}[htbp]
\centering
\resizebox{0.65\linewidth}{!}{%
\begin{tabular}{cc}
\includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image5.jpg} &
\includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image6.jpg}
\end{tabular}%
}
\caption{Bolo App: Access code entry and privacy policy acceptance.}
\label{fig:bolo-auth2}
\end{figure}

\paragraph{Participant Metadata Collection}

Figures~\ref{fig:bolo-profile1} and~\ref{fig:bolo-profile2} show profile setup screens that collected:
\begin{itemize}
\item Optional profile photo
\item Gender
\item Year of birth
\item Pincode for dialect verification
\end{itemize}

\begin{figure}[htbp]
\centering
\resizebox{0.65\linewidth}{!}{%
\begin{tabular}{cc}
\includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image7.jpg} &
\includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image8.jpg}
\end{tabular}%
}
\caption{Bolo App: Profile photo and gender selection.}
\label{fig:bolo-profile1}
\end{figure}

\begin{figure}[htbp]
\centering
\resizebox{0.65\linewidth}{!}{%
\begin{tabular}{cc}
\includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image9.jpg} &
\includegraphics[width=0.48\linewidth]{Styles/figures_appendix/audio_image10.jpg}
\end{tabular}%
}
\caption{Bolo App: Year of birth and pincode entry.}
\label{fig:bolo-profile2}
\end{figure}

\paragraph{Recording Interface}

The app presented categorized tasks, tracked progress, and supported features such as waveform display and playback (see Figure~\ref{fig:bolo-recording}).

\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\linewidth]{Styles/figures_appendix/audio_image11.jpg}
\caption{Bolo App: Guided recording interface with waveform and playback.}
\label{fig:bolo-recording}
\end{figure}

\subsection{Audio Validation Pipeline}

\paragraph{Manual Validation (5\% sample)}

As shown in Figure~\ref{fig:bolo-validation}, manually validated recordings were reviewed for:
\begin{itemize}
\item Audio-text alignment
\item Volume and background noise
\item Overall clarity
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\linewidth]{Styles/figures_appendix/audio_image12.jpg}
\caption{App-based manual audio validation interface.}
\label{fig:bolo-validation}
\end{figure}

\paragraph{Hybrid Manual-Automated Validation}

Recordings were bucketed into slabs based on quality:
\begin{itemize}
\item \textbf{Clean}: $\sim$5\% error
\item \textbf{Semi-noisy}: $\sim$12\% error
\item \textbf{Noisy}: ≥15\% error
\end{itemize}

Bengali and Bhojpuri had slightly higher baseline error in all slabs.

\textbf{Error Definition}: A 5\% error rate means $\sim$5 mismatches in 100 audio-text pairs.

A neural acoustic model computed phone- and word-level match scores, with full methodology described in Section~\ref{subsec:slab_score}. Multi-stage validation and slab pseudocode are detailed in Sections~\ref{subsec:slab_valid} and~\ref{subsec:slab_pseudocode}, respectively.

The task management platform, integrated with cloud storage, enabled both manual and hybrid validation across stages.








% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Slab Definitions}
% \label{sec:slab_def}

% \subsection{Steps for Score Computation}
% \label{subsec:slab_score}

% \begin{enumerate}
%   \item Consider Dialect Z, Validation Model VZO (Model iteration O, dialect Z)

%   \item Let us say we have Dataset $D = [U, W, P]$ in Dialect Z:
%   \begin{enumerate}[label=\alph*.]
%     \item $U = [u_1, \ldots, u_i, \ldots, u_N]$ — utterances set.\\
%     $\#$ $u_i$ is audio / utterance $i$

%     \item $W = [W_1, \ldots, W_i, \ldots, W_N]$ — corresponding sentence prompt (word level)
%     \begin{itemize}
%       \item $W_i = [W_{i1}, \ldots, W_{ij}, \ldots, W_{iJ}]$ — sentence prompt i.e., word sequence of length $J$ corresponding to utterance $i$
%     \end{itemize}

%     \item $P = [P_1, \ldots, P_i, \ldots, P_N]$ — corresponding sentence prompt (phone level)
%     \begin{itemize}
%       \item $P_i = [P_{i1}, \ldots, P_{ik}, \ldots, P_{iK}]$ — phone sequence of length $K$ corresponding to sentence prompt $i$
%       \item Phone sequence $P_i$ for word sequence $W_i$ are generated using a pronunciation dictionary/lexicon that translates a given word into a sequence of phones.\\
%       $\text{Lexicon}[W_i] = [P_{i1}, \ldots, P_{iH}]$ (assuming $H$ number of phones for word $W_i$)
%     \end{itemize}
%   \end{enumerate}

%   \item For $i = 1$ to $N$:
%   \begin{enumerate}[label=\alph*.]
%     \item For the $i$-th utterance $u_i$, extract acoustic feature vector sequence $F_i$ of length $T$, i.e.,\\
%     $F_i = [F_{i0}, \ldots, F_{it}, \ldots, F_{iT}]$

%     \item Pass $(W_i, P_i, F_i)$ as input to the validation model VZO

%     \item The VZO carries out alignment of acoustic feature vector sequence $F_i$ with respect to sequence of phones $P_i$ and sequence of words $W_i$ in the sentence prompt, and generates scores:
%     \begin{itemize}
%       \item $S_{wi} = [S_{wi0}, \ldots, S_{wij}, \ldots, S_{wiJ}]$
%       \item $S_{pi} = [S_{pi0}, \ldots, S_{pik}, \ldots, S_{piK}]$
%     \end{itemize}
%     These scores represent how well every word $W_{ij}$ in word sequence $W_i$ or every phone $P_{ik}$ in the phone sequence $P_i$ in the sentence prompt was spoken by a user.

%     \item Apply statistical measure $Y$ on $(S_{wi}, S_{pi})$ on the phone-level and word-level scores to generate a per utterance score:
%     \\ $SO_i = Y[S_{wi}, S_{pi}]$

%     \item Return score $SO_i$
%   \end{enumerate}
% \end{enumerate}


% \subsection{Validation Process}
% \label{subsec:slab_valid}

% \subsection*{1. Initial Setup and Delegation of Manual Validation Tasks}

% \paragraph{a. Identification and Review}
% \begin{enumerate}[label=\roman*.]
%     \item From the validation team (Figure~\ref{fig:slab_def_fig1}), the Language Resource Manager (LRM) identifies $L$ bins, each of size 0.02, from which $M$ samples will be randomly selected for manual validation.
%     \item The Senior LRM reviews and signs off on the selected $L$ bins and the number of samples ($M$) to be manually validated.
% \end{enumerate}

% \paragraph{b. Delegation to Language Consultants}
% \begin{enumerate}[label=\roman*.]
%     \item Following the Senior LRM's approval, the LRM delegates the task of manually validating the $L$ bins and the number of samples per bin $M$ to Dialect-Specific Language Consultants (LCs).
% \end{enumerate}

% \subsection*{2. Manual Validation Process for LCs}
% \begin{itemize}
%     \item LCs are provided with the Validation Model VZO Model ID, information on $L$ bins (or score intervals), and the number of samples ($M$) per bin to be evaluated.
%     \item LCs must label each sample as \textbf{MATCH} or \textbf{MISMATCH} based on predetermined validation criteria.
% \end{itemize}

% \subsection*{3. Comprehensive Validation Process for LCs}

% \paragraph{a. LC Login and Model Selection}
% \begin{enumerate}[label=\roman*.]
%     \item LCs access the Validation Tool Portal using their unique login ID and password.
%     \item LCs select the Validation Model VZO that corresponds to Version O of the model for their specific dialect Z (Figure~\ref{fig:slab_def_fig2}).
% \end{enumerate}

% \paragraph{b. Entering Validation Details}
% \begin{enumerate}[label=\roman*.]
%     \item Upon selecting the appropriate model, LCs are prompted to input details such as the start and end of the score interval and the number of samples ($M$) to be validated manually (Figure~\ref{fig:slab_def_fig3}).
% \end{enumerate}

% \paragraph{c. Manual Validation Workflow}
% \begin{enumerate}[label=\roman*.]
%     \item The portal directs LCs to the Validation Page, displaying $M$ randomly sampled utterances from the chosen score interval corresponding to the VZO Model ID (Figure~\ref{fig:slab_def_fig4}).
%     \item For each utterance, LCs:
%     \begin{enumerate}[label=\arabic*.]
%         \item View the associated sentence prompt (Figure~\ref{fig:slab_def_fig4})
%         \item Use a Play button to listen to the corresponding audio file (Figure~\ref{fig:slab_def_fig4})
%         \item Make a MATCH or MISMATCH decision based on the following validation criteria:
%         \begin{itemize}
%             \item If the audio is completely incorrect or contains missing words, additions, deletions, or substitutions not in the text: \textbf{MISMATCH}.
%             \item If the audio is completely correct or has 1--2 character errors that are valid phonetic variations: \textbf{MATCH}.
%             \item \textit{Exception}: If a minor sound change alters the meaning, mark as \textbf{MISMATCH}.
%             \begin{itemize}
%                 \item E.g., \textit{mujhe kaam karna hain} vs. \textit{mujhe kam karna hain}.
%             \end{itemize}
%         \end{itemize}
%         \item Press the submit button to upload decisions to the system.
%         \item If uncertain:
%         \begin{itemize}
%             \item Consult via Slack or WhatsApp with LRM, Senior LRM, or Speech Scientist.
%             \item After clarification, finalize and submit decisions.
%         \end{itemize}
%     \end{enumerate}
% \end{enumerate}

% \subsection*{4. Review of Validation Process}

% \paragraph{a. Reviewer Privileges}
% LRMs, Senior LRMs, and Speech Scientists have admin-level reviewer privileges:
% \begin{itemize}
%     \item Access utterances validated by dialect-specific LCs.
%     \item Filter and sort by MATCH/MISMATCH labels, score intervals, or dates.
%     \item Amend labels to ensure validation accuracy and integrity.
% \end{itemize}

% \paragraph{b. Review Process by LRMs}
% \begin{enumerate}[label=\roman*.]
%     \item Log in and select the corresponding Validation Model VZO.
%     \item Filter utterances validated the previous day, apply additional filters.
%     \item For each utterance:
%     \begin{itemize}
%         \item View the sentence prompt, listen to the audio.
%         \item Compare LC decision with criteria (Section 3c.ii.3, Figures~\ref{fig:slab_def_fig5}, \ref{fig:slab_def_fig6}).
%         \item Record any disagreements and consult via Slack or WhatsApp.
%     \end{itemize}
%     \item Finalize the review: uphold or revise the label.
% \end{enumerate}

% \paragraph{c. Senior LRM and Speech Scientist Review}
% \begin{itemize}
%     \item Have same access as LRMs.
%     \item Provide regular reviews and feedback.
%     \item Ensure early detection and resolution of validation issues.
% \end{itemize}

% \subsection*{References}
% \begin{itemize}
%     \item Figure~\ref{fig:slab_def_fig1}: Validation Team Hierarchy Diagram
%     \item Figure~\ref{fig:slab_def_fig2}: Entering Validation Model ID
%     \item Figure~\ref{fig:slab_def_fig3}: Selecting Score Interval and Sample Count
%     \item Figure~\ref{fig:slab_def_fig4}: Manual Validation Interface
%     \item Figure~\ref{fig:slab_def_fig5}: Review Screen for MATCH Utterances
%     \item Figure~\ref{fig:slab_def_fig6}: Review Screen for MISMATCH Utterances
% \end{itemize}

% \textbf{Note:}
% \begin{itemize}
%     \item LCs: Native speakers of the target dialect, proficient in spoken and written language.
%     \item LRMs / Senior LRMs: Hold advanced degrees (PhD/Masters) in linguistics and/or have significant linguistic experience.
% \end{itemize}

% % Figures
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.65\linewidth]{Styles/figures_appendix/slab_def_fig1.png}
% \caption{Validation Team Hierarchy Diagram}
% \label{fig:slab_def_fig1}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.65\linewidth]{Styles/figures_appendix/slab_def_fig2.png}
% \caption{Screenshot - Entering Validation Model ID to select validation model VZO of dialect Z}
% \label{fig:slab_def_fig2}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.65\linewidth]{Styles/figures_appendix/slab_def_fig3.png}
% \caption{Screenshot - Select Score interval begin and end score, number of samples to be sampled}
% \label{fig:slab_def_fig3}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.65\linewidth]{Styles/figures_appendix/slab_def_fig4.png}
% \caption{Screenshot - Manual validation of sampled audios as MATCH (Valid) or MISMATCH (Invalid)}
% \label{fig:slab_def_fig4}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.65\linewidth]{Styles/figures_appendix/slab_def_fig5.png}
% \caption{Screenshot - Reviewing MATCH utterances validated for a specific Date range}
% \label{fig:slab_def_fig5}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.65\linewidth]{Styles/figures_appendix/slab_def_fig6.png}
% \caption{Screenshot - Reviewing MISMATCH utterances validated for a specific Date range}
% \label{fig:slab_def_fig6}
% \end{figure}


% \subsection{Pseudocode to Generate Slabs}
% \label{subsec:slab_pseudocode}

% \begin{enumerate}
%   \item \textbf{Initialize:}
%   \begin{enumerate}[label=\alph*.]
%     \item Consider Dialect Z  
%     \item Audio files in Dialect Z: $N$ files  
%     \item Score computation for each file according to Appendix-B1 with validation model VZO for dialect Z:  
%     \begin{itemize}
%       \item $SO_i$ for $i=1$ to $N$  
%     \end{itemize}
%     \item Rank order $N$ files by $SO$  
%     \item Determine $min\_SO$ and $max\_SO$ to cover $\sim90\%+$ of data  
%     \item Divide score range into $L$ bins, each of range $0.02$  
%   \end{enumerate}

%   \item \textbf{Create Voice Validation Dataset:}
%   \begin{enumerate}[label=\alph*.]
%     \item Initialize empty dataset $D$  
%     \item For each bin in $L$, randomly select $M$ files, resulting in a total of $M \\times L$ files for validation  
%     \item Perform validation as outlined in Appendix-B2, generating MATCH/MISMATCH decision for each file  
%     \item Append the validated subset to the dataset $D$  
%   \end{enumerate}

%   \item \textbf{Analysis-I:}
%   \begin{enumerate}[label=\alph*.]
%     \item For each bin, compute the percentage of MATCH results: $P_l$ for $l=1$ to $L$.  
%     \begin{itemize}
%       \item \textbf{Note:} Dataset $D$ is validation set with which percentage MATCH results is measured across bins  
%     \end{itemize}
%     \item Visually inspect score range to identify score thresholds $SO_{clean}$ and $SO_{seminoisy}$ where:  
%     \begin{itemize}
%       \item $P_l > 0.95$ and $P_l > 0.88$ consistently, respectively  
%       \item \textbf{Note:} Since all $N$ audio files and bins are rank ordered using scores, we will observe higher accuracies as we move up in the score range  
%     \end{itemize}
%   \end{enumerate}

%   \item \textbf{Additional Sampling:}
%   \begin{enumerate}[label=\alph*.]
%     \item Based on $SO_{clean}$ and $SO_{seminoisy}$, additionally validate $K$ bins before and after each threshold if recommended by the validation team  
%     \item Based on Step-4a, repeat Step-2b to Step-4a as given below until no further sampling is recommended by the validation team:  
%     \begin{enumerate}[label*=\arabic*.]
%       \item Run Step-2b to Step-2d for $K$ bins instead  
%       \item Run Step-3a to Step-4a  
%     \end{enumerate}
%   \end{enumerate}

%   \item \textbf{Analysis-II:}
%   \begin{enumerate}[label=\alph*.]
%     \item Compute a new score for each audio file: $SN_i$ for $i=1$ to $N$, as per Appendix-B1 with a new refined validation model VZN for all utterances $N$ in dialect Z  
%     \item Rank order files by $SN$  
%     \item Divide score range into $L'$ bins, each of range $0.02$  
%     \item Compute percentage MATCH for each $L'$ bin: $P'_l$ for $l=1$ to $L'$  
%     \item Validation team to visually inspect stats and scores for clarity and accuracy. Next, identify ($X$ out of $L$) bins for additional validation, if necessary (specifically, those bins where the validation team seeks to gain more clarity, such as how certain pronunciation variants are penalized). If so:  
%     \begin{enumerate}[label*=\arabic*.]
%       \item For each bin $X$, randomly select $M$ files, resulting in a total of $M \\times X$ files for validation  
%       \item Perform validation as outlined in Appendix-B2, generating MATCH / MISMATCH decision for each file and append this to dataset $D$  
%       \item For each bin, compute the percentage of MATCH results: $P'_l$ for $l=1$ to $L'$  
%       \item Visually inspect score range to identify thresholds $SN_{clean}$ and $SN_{seminoisy}$ where:  
%       \begin{itemize}
%         \item $P'_l > 0.95$ and $P'_l > 0.88$ consistently, respectively  
%       \end{itemize}
%       \item Repeat Step-5e until validation team does not recommend further bins to be validated  
%     \end{enumerate}
%   \end{enumerate}

%   \item \textbf{Final Thresholds and Labeling:}  
%   The latest $SN_{clean}$ is the slab-clean threshold and $SN_{seminoisy}$ is the slab-seminoisy threshold. Now label all utterances of dialect Z as follows:  
%   \begin{enumerate}[label=\alph*.]
%     \item $SN > SN_{clean}$: slab-clean  
%     \item $SN_{seminoisy} < SN < SN_{clean}$: slab-seminoisy  
%     \item $SN < SN_{seminoisy}$: slab-noisy  
%   \end{enumerate}
% \end{enumerate}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Slab Definitions}
\label{sec:slab_def}

This section describes the comprehensive process used to categorize utterances into three quality slabs: \textbf{Clean}, \textbf{Seminoisy}, and \textbf{Noisy}. The slab assignment is driven by automatic scoring models and verified through manual validation across multiple stages.

\subsection{Score Computation for Slab Assignment}
\label{subsec:slab_score}

To begin, consider a target dialect $Z$ and its corresponding validation model $V_{Z,O}$, where $O$ indicates the model iteration.

Let $D = [U, W, P]$ denote the dataset for dialect $Z$:
\begin{itemize}
    \item $U = [u_1, u_2, ..., u_N]$: the set of $N$ utterances.
    \item $W = [W_1, ..., W_N]$: word-level sentence prompts, where each $W_i = [W_{i1}, ..., W_{iJ}]$.
    \item $P = [P_1, ..., P_N]$: phone-level transcriptions. Each $P_i = [P_{i1}, ..., P_{iK}]$ is derived from $W_i$ using a pronunciation lexicon.
\end{itemize}

For each utterance $u_i$:
\begin{enumerate}[label=\alph*.]
    \item Extract acoustic feature sequence $F_i = [F_{i0}, ..., F_{iT}]$.
    \item Pass $(W_i, P_i, F_i)$ into $V_{Z,O}$ for alignment.
    \item The model produces alignment-based scores:
    \[
    S_{wi} = [S_{wi1}, ..., S_{wiJ}], \quad S_{pi} = [S_{pi1}, ..., S_{piK}]
    \]
    representing word- and phone-level pronunciation quality.
    \item Apply a statistical aggregation function $Y$ to get the utterance score:
    \[
    SO_i = Y[S_{wi}, S_{pi}]
    \]
\end{enumerate}

\subsection{Manual Validation Workflow}
\label{subsec:slab_valid}

Manual validation ensures the reliability of score-based slab thresholds. The process is led by a multi-tiered team comprising Language Resource Managers (LRMs), Senior LRMs, and Dialect-Specific Language Consultants (LCs).

\subsubsection*{1. Bin Selection and Task Delegation}
\begin{itemize}
    \item The LRM selects $L$ score bins (width 0.02) and randomly chooses $M$ utterances from each bin.
    \item The Senior LRM reviews and approves the bin set.
    \item The validated bins are then assigned to LCs for manual annotation.
\end{itemize}

\subsubsection*{2. Manual Annotation by Language Consultants}
\begin{itemize}
    \item LCs receive the validation model ID, bin range, and $M$ utterances per bin.
    \item Each sample is labeled as \texttt{MATCH} or \texttt{MISMATCH}:
    \begin{itemize}
        \item \texttt{MATCH}: Audio is correct or has minor phonetic variations.
        \item \texttt{MISMATCH}: Audio is incorrect, has insertions/deletions, or meaning-altering substitutions.
    \end{itemize}
    \item Difficult cases are resolved through discussions with LRMs or scientists.
\end{itemize}

\subsubsection*{3. Validation Portal Process}
LCs perform validation using a structured interface:
\begin{enumerate}[label=\roman*.]
    \item Login and model selection (VZO).
    \item Input score interval and number of samples ($M$).
    \item Review each utterance with playback and transcription.
    \item Submit final MATCH/MISMATCH label.
\end{enumerate}

\subsubsection*{4. Review Mechanism}
LRMs and Senior LRMs:
\begin{itemize}
    \item Access all LC decisions with filters.
    \item Overrule incorrect labels with justifications.
    \item Coordinate feedback loops for quality control.
\end{itemize}

\textbf{Note:}
\begin{itemize}
    \item LCs are native dialect speakers.
    \item LRMs and Senior LRMs are experienced linguists or domain experts.
\end{itemize}

\subsection{Slab Generation Pseudocode}
\label{subsec:slab_pseudocode}

The slab generation involves multiple stages of automatic scoring and manual validation:

\begin{enumerate}
    \item \textbf{Initialization:}
    \begin{itemize}
        \item Dialect $Z$ has $N$ utterances scored using $V_{Z,O}$ as described earlier.
        \item All $SO_i$ scores are ranked. The range covering 90\%+ of data is binned into $L$ intervals of width 0.02.
    \end{itemize}

    \item \textbf{Voice Validation Dataset Creation:}
    \begin{itemize}
        \item Sample $M$ utterances from each bin.
        \item Manually annotate these using the LC process.
    \end{itemize}

    \item \textbf{Analysis-I:}
    \begin{itemize}
        \item For each bin $l$, compute $P_l = \%$ of MATCH labels.
        \item Identify thresholds:
        \[
        SO_{\text{clean}}: P_l > 0.95, \quad SO_{\text{seminoisy}}: P_l > 0.88
        \]
    \end{itemize}

    \item \textbf{Additional Validation (Optional):}
    \begin{itemize}
        \item Validate $K$ bins before/after identified thresholds.
        \item Update scores, rerun Analysis-I.
    \end{itemize}

    \item \textbf{Analysis-II with Refined Model:}
    \begin{itemize}
        \item Compute updated scores $SN_i$ for all $N$ utterances using refined model $V_{Z,N}$.
        \item Repeat binning, annotation, and thresholding steps to refine:
        \[
        SN_{\text{clean}}, \quad SN_{\text{seminoisy}}
        \]
    \end{itemize}

    \item \textbf{Final Slab Assignment:}
    \begin{itemize}
        \item Label each utterance:
        \[
        \text{Clean: } SN_i > SN_{\text{clean}}, \quad
        \text{Seminoisy: } SN_{\text{seminoisy}} < SN_i \leq SN_{\text{clean}}, 
        \]
        \[
        \text{Noisy: } SN_i \leq SN_{\text{seminoisy}}
        \]

    \end{itemize}
\end{enumerate}

In summary, the slab generation process combines automatic scoring with expert manual validation to ensure that speech quality annotations are both reliable and reproducible. This categorization is crucial for downstream training, evaluation, and dataset release pipelines.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Speaker Bucketization and ID Verification}
% \label{appendix:speaker_bucket}
% In a scenario where audio data is collected in a crowd-sourced framework, acquiring correct meta-data of speakers is a challenging task. Speaker Identity meta-data of the speaker is the one of most relevant and with high use-case for speech corpuses. Deviations of speaker identity correctness in speech corpuses can be categorized into two classes:
% \begin{enumerate}
%     \item Intra-Speaker ID errors:\\
%     This class of speaker ID deviation encapsulates those scenarios where given speaker id is not of the actual speaker.
%     \item Inter-Speaker ID errors:\\
%     This class of speaker ID deviation encapsulates those scenarios where  atleast two given speaker ids are actually of the same speaker.
% \end{enumerate}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/speaker_id_issue.png}
%     \caption{Intra speaker and Inter speaker issues}
%     \label{fig:spk_id_issue}
% \end{figure}
% Figure \ref{fig:spk_id_issue} gives a pictorial representation of intra and inter speaker id error. Speaker 1 has two subsets Speaker 1 (correct) and Speaker 3 (incorrect), is an example of intra error. Whereas Speaker 3 (subset of subset Speaker 1) and Speaker3 shows an inter error.
% \subsection{Speaker Meta-Data Error Prevalence and Degree}
% To estimate the degree to which intra and inter speaker ID errors are present in our corpus, we extract speaker embeddings with pre-trained TDNN model using speech brain\footnote{https://huggingface.co/speechbrain/spkrec-xvect-voxceleb}. For intra errors, using the given speaker meta-data, cosine similarity across combinatorial pairs of each speakers' embeddings is calculated.  As for inter errors, we caluclate the cosine similarity across combinatorial pairs of 2 distinct speakers' embedding in a district. Fig\ref{fig:spk_id_issue}(left) and (right) shows the distribution of cosine similarities for intra and inter embedding pairs in Bengali language respectively. To determine the threshold where probabilty of finding intra and inter errors becomes significant, we sample few audio files in each cosine similarity window of size 0.1 and manually listen descending order in case of intra and ascending in inter's case. The point where errors are detected, that cosine similarity is considered as the idea of threshold and data after that is assumed as corrupted with intra and inter errors. For intra we find 0.92 as the threshold and the data in $<$0.90 is assumed to have intra speakers issues. As for inter we also find 0.92 as the threshold and the corrupted data range is $>$0.92.


% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/cosine_similarity.pdf}
%     \caption{Example for cosine similarity distribution}
%     \label{fig:cosine_similarity}
% \end{figure}
% \subsection{Speaker Meta-Information Recovery via Multi-Stage Clustering}
% The proposed approach addresses two fundamental challenges: (1) Determining the precise number of unique speakers within incorrectly labeled same speaker groups (intra-speaker clustering), and (2) Identifying identical speakers across different label groups (inter-speaker clustering). Our method achieves this through calibrated threshold optimization, hierarchical clustering, and multi-stage verification processes. Figure\ref{fig:bucketization} gives the overview the Speaker Meta-Information Recovery via Multi-Stage Clustering framework.
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/Bucketization.pdf}
%     \caption{Bucketization Algorithm}
%     \label{fig:bucketization}
% \end{figure}
% \subsubsection{Dialect-Specific Calibration}
% The effectiveness of speaker verification systems depends critically on appropriate decision thresholds. We established dialect-specific thresholds using manually validated calibration sets for each of the 38 dialects in our corpus.\\\\
% \textbf{ Equal Error Rate Optimization}\\
% For threshold determination, we constructed a verification dataset comprising: 1)500 speakers × 4 samples/speaker, 2)50 speakers × 20 samples/speaker. From these recordings, we generated approximately 20,000 trial pairs (10,000 genuine and 10,000 impostor comparisons). For each dialect, we computed speaker embeddings and corresponding similarity metrics to establish the threshold value at which the false acceptance rate equals the false rejection rate (Equal Error Rate, EER). These dialect-specific thresholds were subsequently employed in the clustering processes.\\\\
% \textbf{Speaker Selection and Validation}\\
% The calibration process begins with:
% \begin{itemize}[noitemsep, topsep=0pt, partopsep=0pt]
%     \item \textit{SelectSpeakers(N): Choose N speakers, emphasizing gender and age diversity.}
%     \item \textit{ValidateSpeakers(C): Manually validate the speaker selection to compile the calibration dataset.}
% \end{itemize}

% Where N represents the total number of speakers selected to ensure diversity across gender and age, and C is the Calibration Dataset comprising manually validated audio samples from these N speakers, used to establish baseline metrics for speaker bucketization.
% \subsubsection{Intra-Speaker Clustering Methodology}
% For each incorrect speaker label group, we applied an iterative clustering process using the calibrated, dialect-specific thresholds:
% \begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt]
%     \item Randomly select a seed utterance as the base for clustering within a speaker audio set
%     \item Identify all utterances exceeding the similarity threshold when compared with the seed
%     \item Calculate the centroid of the resulting cluster
%     \item Iteratively refine cluster membership through centroid-based comparisons
%     \item Remove the clustered utterances from the candidate pool
%     \item Repeat until all utterances are assigned
% \end{enumerate}
% This process is formalized as:
% \begin{itemize}[noitemsep, topsep=0pt, partopsep=0pt]
%     \item \textit{ClusterAudio(): Organize audio files into clusters based on their perceived similarity.}
%     \item \textit{EvaluateSimilarity(Clusters): Determine the level of similarity within and between speaker clusters.}
% \end{itemize}
% Despite the dialect-optimized thresholds, we observed fragmentation of utterances from the same speaker across multiple clusters. To address this issue, we implemented a post-processing consolidation step:
% \begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt]
%     \item Compute cross-cluster centroid similarities
%     \item For potentially matching clusters, calculate pairwise similarities between randomly selected utterances from each cluster
%     \item Merge clusters when both centroid and utterance-level similarities    exceed the threshold
% \end{enumerate}
% \textit{MergeClusters(Clusters): Combine clusters when significant similarity is detected, considering speaker voice variability and other factors.}\\
% Experimental validation of our intra-speaker clustering methodology demonstrated accuracy exceeding 95\% across multiple dialects.\\\\
% \textbf{Confidence-Based Refinement:}\\
% To further improve cluster purity, we developed a confidence scoring mechanism incorporating: Utterance-to-centroid similarity, Within-cluster consistency metrics and Cross-validation scores from the post-processing step. This confidence scoring enabled the identification and removal of boundary cases and outliers, resulting in higher-quality speaker clusters. \\

% \textit{AssignConfidenceScores(Clusters): Allocate confidence scores (S\_confidence) to each audio file reflecting the clustering analysis.} \\
% \textit{FilterByConfidence(Scores): Exclude audio files with scores indicating low confidence in cluster assignment.}\\\\
% \textbf{Gender Consistency Verification:}\\
% The proposed method integrates gender classification information from an auxiliary pipeline to provide an additional verification layer. Utterances with gender classifications inconsistent with the dominant cluster gender are flagged for review or exclusion, further enhancing cluster homogeneity.\\
% \textit{MergeGenderData(Clusters, GenderData): Merge speaker identity clusters with gender information for comprehensive metadata correction.}

% \subsubsection{Inter-Speaker Clustering Framework}
% The inter-speaker clustering process identifies identical speakers across different speaker label groups by extending the methodology used in intra-speaker cluster consolidation. The process involves:
% \begin{enumerate}
%     \item Computing similarity scores between cluster centroids from different label groups
%     \item For candidate matches, calculating similarities between multiple randomly sampled utterance pairs
%     \item Merging clusters when both centroid and utterance-level similarities consistently exceed the threshold
% \end{enumerate}
% The inter-speaker clustering incorporates both initial filtering based on centroid-based similarity metrics and refined similarity assessment using sampling-based metrics, with special consideration for dialect-specific characteristics.\\\\
% \textbf{Hierarchical Verification Protocol:}\\
% Given the increased consequences of errors in inter-speaker clustering, we implemented a hierarchical verification protocol:
% \begin{enumerate}
%     \item Initial inter-speaker clustering as described above
%     \item Re-application of intra-speaker clustering to the merged clusters
%     \item Confidence-based filtering of the resulting clusters
%     \item Final verification through selective manual auditing of boundary cases
% \end{enumerate}
% This multi-stage approach ensures the reliability of the final speaker partitioning across the entire dataset.Once the clustering processes are complete, we update the dataset with corrected speaker identities based on the refined clusters and integrate gender labels into the final speaker metadata, enriching the dataset for downstream applications.
% \subsection{Testing Bucketization Algorithm}
% To evaluate the efficacy of the proposed bucketization algorithm, we test the algorithm on an in-house assembled test corpus.
% To create a test set which accurately evaluates the performance of the algorithm, we took speakers from various in-house and standard speech corpuses. As the algorithm is tuned for Indian speakers, corpuses for test set with Indian speakers were chosen. The original speaker labels of the test corpus are taken as ground truth. And new speaker labels are assigned to the audio samples of the test corpus so that several instances of intra and inter speaker errors are included in the test set. This test set with synthetic speaker labels is given to the algorithm which generates new speaker labels and  speaker label pairs resolving the intra and inter speaker errors respectively. Figure\ref{fig:bucketization_test} shows the process of creating the test corpus.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/Bucktezisation_Test.pdf}
%     \caption{Blind Test Generation for Bucketization}
%     \label{fig:bucketization_test}
% \end{figure}

% Bucketization algorithm takes the Blind test's samples along with synthetic speaker labels and generates a set of new speaker labels and speaker label pairs. New set of speakers tries to eliminate the intra speaker errors so that the clusters with new speaker labels are atomic and cannot be further divided into multiple speakers. And the speakers label pairs tries to resolve the inter speaker errors in the blind test corpus, which essentially merges the atomic clusters of the same speaker. Figure \ref{fig:bucketization_comp} shows the inference of the bucketization algorithm on the blind test corpus. 

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\textwidth,trim={0cm 0 0 2cm},clip]{figures/Bucketization_test_composition.pdf}
%     \caption{Running Bucketization algorithm on Blind Test Corpus}
%     \label{fig:bucketization_comp}
% \end{figure}

% To evaluate the algorithm's performance on intra speaker issues, an alignment score is calculated for each speaker bucket in the blind test corpus. For alignment score, we create a confusion matrix of samples between predicted and ground-truth speaker labels. Algorithm to evaluate alignment score is given as \ref{intra_align_score}. Higher the alignment score of a bucket, better are the predicted speaker labels resolving intra speaker error. Figure \ref{fig:bucketization_res} (left) shows the confusion matrix and maximum score alignment between predicted and ground-truth speakers for Bucke 1.

% \begin{algorithm}[H]
%     \caption{Calculating Alignment Score}
%     \label{intra_align_score}
%     \For{each bucket in Blind Test Corpus}{
%         Create a confusion matrix (cm) between predicted and ground-truth speakers\;
%         Initialize alignment score to 0\;\\
%         \While{cm is not empty}{
%             Find the maximum value in cm\;\\
%             Delete the row and column corresponding to the maximum value\;\\
%             Increment alignment score by the maximum value\;
%         }
%         Normalize the alignment score\;
%     }
% \end{algorithm}

% For inter speaker issues, bucketization algorithm generates the new speaker pair list indicating pair speaker labels are to be merged and essentially belong to the same speaker.Figure \ref{fig:bucketization_inter_eval} gives the workflow of the inter speaker evaluation. To evaluate the performance of the algorithm here, we use Intersection-Over-Union (IOU) of the ground-truth speakers involved in the paired speaker buckets given by the algorithm. Figure \ref{fig:bucketization_eval} (right) shows the inter speaker interaction between bucket 1 and bucket 2 along with the ground-truth speakers in each bucket. 
% \begin{figure}[htbp]
%     \begin{minipage}{.45\textwidth}
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/bucketization_test_eval.pdf}
%     \caption{Blind Test Intra Speaker Evaluation of Bucketization}
%     \label{fig:bucketization_intra_eval}
%     \end{minipage}
%     \hspace{1cm}
%     \begin{minipage}{.45\textwidth}
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/Bucketization_Inter_speaker_eval.pdf}
%     \caption{Blind Test Inter Speaker Evaluation of Bucketization}
%     \label{fig:bucketization_inter_eval}
% \end{minipage}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/Bucketization_Evaluation_Metrics.pdf}
%     \caption{Bucketization Evaluation Metrics}
%     \label{fig:bucketization_eval}
% \end{figure}
% Using alignment score and IOU as the metrics for evaluating the performance of bucketization algorithm. Average alignment score was 99.28\% implying the resolution of almost all intra speaker errors. Whereas algorithm's average IOU was only 52.91\% indicating algorithm's shortcoming in resolving inter speaker errors. And about 10\% of blind test corpus couldn't be bucketized by the algorithm.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/Bucketization_Eval_Res.pdf}
%     \caption{Bucketization Evaluation Results}
%     \label{fig:bucketization_res}
% \end{figure}


\section{Speaker Bucketization and ID Verification}
\label{appendix:speaker_bucket}

In crowdsourced speech data collection frameworks, accurate speaker metadata is crucial but difficult to guarantee. Among various metadata types, speaker identity plays a pivotal role in training robust models and enabling tasks like speaker adaptation and verification. Errors in speaker IDs broadly fall into two categories:

\textbf{Intra-Speaker ID Errors:} A single speaker is assigned multiple distinct speaker IDs.

\textbf{Inter-Speaker ID Errors:} Multiple speakers are incorrectly assigned the same speaker ID.

Figure~\ref{fig:spk_id_issue} illustrates both these error types using simplified speaker ID relationships.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/speaker_id_issue.png}
    \caption{Illustration of intra-speaker and inter-speaker ID inconsistencies}
    \label{fig:spk_id_issue}
\end{figure}

\subsection{Speaker Meta-Data Error Prevalence and Degree}

To assess the prevalence of speaker ID inconsistencies, we extracted speaker embeddings using the pretrained SpeechBrain TDNN model\footnote{\url{https://huggingface.co/speechbrain/spkrec-xvect-voxceleb}}. For intra-speaker ID analysis, cosine similarity was computed across all utterances with the same speaker label. For inter-speaker ID errors, similarity was measured between utterances from different speaker IDs within the same district.

The distributions of cosine similarities for intra- and inter-speaker pairs in Bengali are shown in Figure~\ref{fig:cosine_similarity}. Based on manual inspection across cosine similarity intervals, we empirically identified a threshold of 0.92. Utterance pairs with cosine similarity below 0.90 are likely to be affected by intra-speaker error, while inter-speaker error is suspected for similarity values above 0.92.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/cosine_similarity.pdf}
    \caption{Cosine similarity distributions for intra- and inter-speaker embeddings in Bengali}
    \label{fig:cosine_similarity}
\end{figure}

\subsection{Speaker Meta-Information Recovery via Multi-Stage Clustering}

To correct speaker labeling inconsistencies, we propose a multi-stage clustering framework consisting of calibrated threshold estimation, intra- and inter-speaker clustering, post-processing, and validation. An overview is shown in Figure~\ref{fig:bucketization}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Bucketization.pdf}
    \caption{Overview of the speaker clustering and correction (bucketization) pipeline}
    \label{fig:bucketization}
\end{figure}

\subsubsection{Dialect-Specific Calibration}

We computed dialect-specific similarity thresholds using an Equal Error Rate (EER)-based calibration method. For each dialect, we curated a speaker verification set with 500 speakers (4 samples each) and 50 speakers (20 samples each), yielding 20k trials equally divided into genuine and impostor pairs. The similarity threshold at EER was adopted for clustering.

Speakers for calibration were selected to maximize demographic diversity (gender, age). Manual validation ensured reliability of these calibration samples.

\subsubsection{Intra-Speaker Clustering Methodology}

Given a potentially inconsistent speaker label, we clustered the associated utterances using a similarity threshold:
\begin{enumerate}
    \item A random utterance is chosen as the seed.
    \item Utterances exceeding the threshold are clustered.
    \item A centroid is computed for the cluster.
    \item The cluster is refined using the centroid similarity.
    \item Clustered utterances are removed; process repeats.
\end{enumerate}

To address over-fragmentation, post-clustering merges were performed if cross-cluster centroid and utterance similarities both exceeded the threshold.

A confidence scoring mechanism was implemented using centroid similarity, intra-cluster consistency, and post-validation cross-similarity scores. Low-confidence utterances were filtered to improve cluster purity.

Gender metadata was also integrated to flag inconsistencies. Clusters with mixed-gender predictions were reviewed or split.

\subsubsection{Inter-Speaker Clustering Framework}

To merge clusters representing the same speaker across speaker labels:
\begin{enumerate}
    \item Compute centroid similarities between different speaker clusters.
    \item If similar, sample utterances from both clusters and compute pairwise similarities.
    \item Merge clusters only if both conditions are satisfied.
\end{enumerate}

A hierarchical verification protocol was adopted to ensure reliability:
\begin{enumerate}
    \item Initial inter-speaker cluster proposals
    \item Re-run intra-clustering on merged sets
    \item Filter by confidence score
    \item Manual review of borderline cases
\end{enumerate}

\subsection{Sampling Uncontaminated Speakers for Dev and Test Sets}

Following speaker clustering and verification, we compiled a list of \emph{uncontaminated speakers}—those not involved in any intra- or inter-speaker ID errors. This uncontaminated speaker pool formed the basis for dev and test set sampling, ensuring no overlap with the training data. This separation is critical to maintain dataset partition integrity and enables reliable benchmarking.

\subsection{Testing the Bucketization Algorithm}

To evaluate the effectiveness of our clustering-based bucketization, we designed a synthetic blind test corpus. This test set was constructed by combining samples from standard and internal Indian speech corpora with ground-truth speaker labels. New synthetic speaker labels were introduced to simulate known intra- and inter-speaker ID errors.

Figure~\ref{fig:bucketization_test} outlines the creation of this test corpus.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Bucktezisation_Test.pdf}
    \caption{Creating the blind test corpus by simulating intra and inter speaker label errors}
    \label{fig:bucketization_test}
\end{figure}

Figure~\ref{fig:bucketization_comp} shows the application of the bucketization algorithm to recover correct speaker IDs.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth,trim={0cm 0 0 2cm},clip]{figures/Bucketization_test_composition.pdf}
    \caption{Inference of bucketization algorithm on blind test set}
    \label{fig:bucketization_comp}
\end{figure}

\textbf{Intra-Speaker Evaluation:} Alignment score is used to measure the purity of predicted speaker clusters.

\begin{algorithm}[H]
    \caption{Calculating Alignment Score}
    \label{intra_align_score}
    \For{each bucket in Blind Test Corpus}{
        Create a confusion matrix (cm) between predicted and ground-truth speakers\;
        Initialize alignment score to 0\;\\
        \While{cm is not empty}{
            Find the maximum value in cm\;\\
            Delete the row and column corresponding to the maximum value\;\\
            Increment alignment score by the maximum value\;
        }
        Normalize the alignment score\;
    }
\end{algorithm}

\textbf{Intra-Speaker Evaluation:} To evaluate the ability of the bucketization algorithm to resolve intra-speaker ID errors, we compute an \textit{alignment score} for each speaker bucket. This metric quantifies how well the predicted speaker clusters align with the ground-truth speaker labels.

Figure~\ref{fig:bucketization_intra_eval} illustrates the evaluation workflow for intra-speaker analysis. The alignment score is computed using the confusion matrix between predicted and actual speaker labels, as formalized in Algorithm~\ref{intra_align_score}. Higher alignment scores indicate better cluster purity and minimal fragmentation.

Figure~\ref{fig:bucketization_eval} (left) shows a sample confusion matrix for one speaker cluster, where the maximum alignment path is highlighted. The algorithm achieved an average alignment score of \textbf{99.28\%}, confirming that intra-speaker identity inconsistencies were almost completely resolved.

\textbf{Inter-Speaker Evaluation:} For inter-speaker errors, where multiple labels are used for the same speaker, the algorithm proposes pairs of speaker clusters to merge. We evaluate this using \textit{Intersection over Union (IoU)} between the sets of ground-truth speaker IDs represented in each proposed cluster pair.

The evaluation process is illustrated in Figure~\ref{fig:bucketization_inter_eval}, while Figure~\ref{fig:bucketization_eval} (right) depicts how IoU is calculated between the overlapping speaker identity sets. The algorithm achieved an average inter-speaker IoU of \textbf{52.91\%}, suggesting moderate success in merging duplicated speaker identities. The lower IoU relative to the alignment score indicates that resolving inter-speaker ID inconsistencies remains more challenging.

\textbf{Final Evaluation Summary:} Overall performance is summarized in Figure~\ref{fig:bucketization_res}, which visualizes intra and inter evaluation metrics across the test corpus. Although intra-speaker resolution was highly accurate, approximately \textbf{10\% of utterances} in the blind test corpus remained unassignable due to confidence score thresholds or conflicting metadata.

\begin{figure}[htbp]
    \begin{minipage}{.45\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/bucketization_test_eval.pdf}
    \caption{Blind Test Intra-Speaker Evaluation}
    \label{fig:bucketization_intra_eval}
    \end{minipage}
    \hspace{1cm}
    \begin{minipage}{.45\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Bucketization_Inter_speaker_eval.pdf}
    \caption{Blind Test Inter-Speaker Evaluation}
    \label{fig:bucketization_inter_eval}
    \end{minipage}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Bucketization_Evaluation_Metrics.pdf}
    \caption{Evaluation metrics: (left) Confusion matrix alignment for intra-speaker analysis; (right) IoU for inter-speaker cluster overlap}
    \label{fig:bucketization_eval}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Bucketization_Eval_Res.pdf}
    \caption{Bucketization performance results across the test corpus}
    \label{fig:bucketization_res}
\end{figure}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Dev, Test and Train Sets Creation}

% \subsection{Dev and test sets creation}
% Creating the dev and test sets from the dataset $D_{Z}$ of Dialect Z involves careful selection of speakers and utterances, ensuring diversity and avoiding speaker contamination between sets. The dataset $D_{Z}$ consists of audio files categorized into three slabs: $A_{c}$ (\textit{clean} slab), $A_{sn}$ (\textit{seminoisy} slab), and $A_{n}$ (\textit{noisy} slab), as detailed in Appendix-A1. The audio files are divided into dev, test, and train sets, ensuring no overlap between the sets regarding speakers and text IDs.

% As the dev and test sets are relatively small compared to the train set and need to be balanced across dialects, domains and speakers, we first create the dev and test sets, keeping all the parameters balanced to the extent possible. The following pseudocode summarizes the key steps in creating the dev, test sets:

% \begin{algorithm}[H]
% \SetAlgoLined
% \caption{Test Set and Dev Set Creation}
% \begin{algorithmic}[1]

% \State \textbf{Initialize:}
% \State Define $A$ as the total number of audio files in $D_{Z}$.
% \State Define subsets $A_{c}$, $A_{sn}$, and $A_{n}$ corresponding to the three slabs (see Section~\ref{sec:slab_def} for slab definitions).
% \State Define $S$ as the total number of speakers, and $S_{uncont}$ as the set of uncontaminated speaker IDs (see Section~\ref{appendix:speaker_bucket} for contamination criteria).
% \State Let max\_rep represent the maximum allowed repetitions of any text ID across the test and dev sets.

% \State \textbf{Diversity Constraints:}
% \State Maintain balance across task (Question, Statement), domain (Banking, Agriculture), and gender (male, female).
% \State Ensure minimal loss of utterances during test and dev set creation.
    
% \end{algorithmic}
% \end{algorithm}

% \subsubsection{Test Set Creation}
% The test set is created by filtering out uncontaminated speakers and selecting utterances from $A_{c}$ for each speaker. Gender diversity is ensured by forming two subsets: $S_{ms}$ (sampled male speakers) and $S_{fs}$ (sampled female speakers). For each selected speaker, one utterance from $A_{c}$ is chosen for each combination of task (Question/Statement) and domain (Banking/Agriculture). After selection, all utterances containing the selected text IDs are removed once max\_rep is reached.

% Once the selection is complete, a diversity analysis is performed to ensure that the balance between tasks, domains, and genders is maintained. The test set $A_{test}$ is finalized if the diversity constraints are satisfied.

% \begin{algorithm}[H]
% \caption{\textbf{Test Set Creation:}}
% \begin{algorithmic}[1]
% \State Filter speakers based on $S_{\textit{uncont}}$.
% \State Select $A_c$ from $S_{\textit{uncont}}$ for test set creation.
% \State Form subsets $S_{ms}$ and $S_{fs}$, ensuring balance in tasks and domains.
% \State \textbf{for} each selected speaker per gender \textbf{do}
% \State \hspace{1em} Select a single $A_c$ utterance for each of the following:
% \State \hspace{2em} Task: Question, Domain: Agriculture
% \State \hspace{2em} Task: Statement, Domain: Agriculture
% \State \hspace{2em} Task: Question, Domain: Banking
% \State \hspace{2em} Task: Statement, Domain: Banking
% \State \hspace{1em} Delete all utterances with text IDs selected \texttt{max\_rep} times.
% \State \textbf{end for}
% \State Perform diversity analysis. If constraints are met, freeze the test set $A_{\textit{test}}$.
% \end{algorithmic}
% \end{algorithm}

% \subsubsection{Dev Set Creation}

% The dev set is created similarly to the test set but with the additional condition that no speaker or text ID from the test set is included in the dev set. Speakers are selected similarly, ensuring gender balance and diversity analysis are performed to confirm that the dev set satisfies the required constraints.

% \begin{algorithm}[H]
% \caption{\textbf{Dev Set Creation:}}
% \begin{algorithmic}[1]
% \State Filter speakers based on $S_{\textit{uncont}}$, excluding those from $A_{\textit{test}}$.
% \State Select $A_{\textit{slab1}}$ from $S_{\textit{uncont}}$ for dev set creation.
% \State Form subsets $S_{ms}$ and $S_{fs}$, ensuring balance in tasks and domains.
% \State \textbf{for} each selected speaker per gender \textbf{do}
% \State \hspace{1em} Select a single $A_c$ utterance for each of the following:
% \State \hspace{2em} Task: Question, Domain: Agriculture
% \State \hspace{2em} Task: Statement, Domain: Agriculture
% \State \hspace{2em} Task: Question, Domain: Banking
% \State \hspace{2em} Task: Statement, Domain: Banking
% \State \hspace{1em} Delete all utterances with text IDs selected \texttt{max\_rep} times.
% \State \textbf{end for}
% \State Perform diversity analysis. If constraints are met, freeze the dev set $A_{\textit{dev}}$.
% \end{algorithmic}
% \end{algorithm}


% \subsection{Training Set Creation}

% The training set is created by removing all speakers and text IDs present in the test and dev sets. A diversity analysis ensures that the distribution of sentences, domains, and genders in the training set is not skewed compared to the original dataset $D_{Z}$. Once the diversity constraints are validated, the training set $A_{train}$ is finalized.

% \begin{algorithm}[H]
% \SetAlgoLined
% \begin{algorithmic}[1]

% \State \textbf{Training Set Creation:}
% \State Filter out utterances belonging to any speaker or text ID from $A_{test}$ and $A_{dev}$.
% \State Perform diversity analysis to verify that the training set $A_{train}$ reflects the diversity of $D_{Z}$.
% \State Freeze the training set $A_{train}$.

% \end{algorithmic}

% \end{algorithm}

% \subsection{Handling High Loss Dialects}

% For dialects with high loss or failure to meet diversity constraints, such as Mt\_D2, Hi\_D2, and Hi\_D4, a similar process is followed. However, in these cases, both $A_{slab1}$ and $A_{slab0.5}$ are used, in addition to uncontaminated speakers, to ensure the creation of valid test and dev sets.

% \begin{algorithm}[H]
% \SetAlgoLined
% \begin{algorithmic}[1]

% \State \textbf{Sampling for High Loss Dialects:}
% \State Follow the same steps as for the test and dev sets, but include both $A_{slab1}$ and $A_{slab0.5}$ for speaker selection.
    
% \end{algorithmic}
% \end{algorithm}

% Furthermore, to provide the RESPIN corpora to a wider audience, we have also open-sourced a smaller version of the training set for each of the 9 languages. These sets are referred to as \textit{train\_$<$lang$>$\_small}.

% \begin{algorithm}[H]
% \caption{Balanced Sampling of Text IDs Across Dialects and Domains}
% \label{alg:balanced_sampling}
% \textbf{Input:} 
% \begin{itemize}
%     \item $\mathcal{T}$: Set of all text IDs
%     \item $\mathcal{D}_{\text{did}}$: Set of dialect IDs
%     \item $\mathcal{D}_{\text{dom}}$: Set of domain IDs
%     \item $N_{\text{total}}$: Total number of text IDs to sample
% \end{itemize}

% \textbf{Output:} 
% \(\mathcal{T}_{\text{sampled}}\): Balanced sampled text IDs

% \begin{algorithmic}[1]
%     \State \textbf{Filter Valid Text IDs:}
%     \[
%     \mathcal{T}_{\text{valid}} = \{t \in \mathcal{T} \mid |\text{utterances}(t)| \geq 5\}
%     \]

%     \State \textbf{Set Sampling Targets:}
%     \[
%     N_{\text{dialect}} = \frac{N_{\text{total}}}{|\mathcal{D}_{\text{did}}|}
%     \]
%     \[
%     N_{\text{target}}(d, o) = \frac{N_{\text{dialect}}}{|\{o \in \mathcal{D}_{\text{dom}} \mid d \text{ fixed}\}|}
%     \]

%     \State \textbf{Sample Text IDs:}
%     For each $(d, o) \in \mathcal{D}_{\text{did}} \times \mathcal{D}_{\text{dom}}$:
%     \[
%     \mathcal{T}_{\text{sampled}}(d, o) = 
%     \begin{cases} 
%         \text{Compensate from other domains}, & \text{if } |\text{available}(d, o)| < N_{\text{target}}(d, o) \\
%         \text{Sample within domain}, & \text{otherwise}
%     \end{cases}
%     \]
%     Append \(\mathcal{T}_{\text{sampled}}(d, o)\) to \(\mathcal{T}_{\text{sampled}}\)

%     \State \textbf{Save Results:}
%     \[
%     \text{Save } \mathcal{T}_{\text{sampled}} \text{ to output files}
%     \]
% \end{algorithmic}
% \end{algorithm}

% The purpose of this algorithm is to perform balanced sampling of text IDs across different dialects and domains. The main steps are:

% \paragraph{1. Filter Valid Text IDs}
% The algorithm filters the set of all text IDs (\(\mathcal{T}\)) to retain only those that have at least five associated utterances, ensuring sufficient representation.

% \paragraph{2. Set Sampling Targets}
% The total number of text IDs to sample (\(N_{\text{total}}\)) is divided equally among all dialects (\(\mathcal{D}_{\text{did}}\)). Within each dialect, the target number of text IDs is further divided among its associated domains (\(\mathcal{D}_{\text{dom}}\)).

% \paragraph{3. Sample Text IDs}
% For each dialect-domain pair \((d, o)\):
% \begin{itemize}
%     \item If the available text IDs are fewer than the target (\(N_{\text{target}}(d, o)\)), the shortfall is compensated by sampling additional text IDs from other domains within the same dialect.
%     \item Otherwise, text IDs are sampled within the domain to meet the target.
% \end{itemize}

% \paragraph{4. Save Results}
% The sampled text IDs (\(\mathcal{T}_{\text{sampled}}\)) are saved, ensuring that the dataset is balanced across dialects and domains.

% This approach ensures a fair distribution of text IDs across dialects and domains while maintaining sufficient representation for each pair.

% \begin{algorithm}[H]
% \caption{Sample 5 Utterances for Each Text ID}
% \label{alg:sample_utterances}
% \textbf{Input:} 
% \begin{itemize}
%     \item $\mathcal{T}_{\text{sampled}}$: Set of sampled text IDs from the previous algorithm
%     \item $\mathcal{U}$: Set of all utterances with associated text and speaker IDs
%     \item $\mathcal{U}_{\text{train}}$: Set of utterances in the training set
% \end{itemize}

% \textbf{Output:} 
% \begin{itemize}
%     \item $\mathcal{S}_{\text{sampled}}$: Table of speaker IDs sampled for each text ID
%     \item $\mathcal{U}_{\text{sampled}}$: Table of utterance IDs sampled for each text ID
% \end{itemize}

% \begin{algorithmic}[1]
%     \State \textbf{Filter Utterances:}
%     \[
%     \mathcal{U}_{\text{filtered}} = \{u \in \mathcal{U} \mid \text{tid}(u) \in \mathcal{T}_{\text{sampled}} \text{ and } u \in \mathcal{U}_{\text{train}}\}
%     \]

%     \State \textbf{Compute Speaker Frequencies:}
%     \[
%     f_s(s) = \text{Count of occurrences of each speaker ID } s \text{ in } \mathcal{U}_{\text{filtered}}
%     \]

%     \State \textbf{Sample Speaker IDs:}
%     For each $t \in \mathcal{T}_{\text{sampled}}$:
%     \[
%     \mathcal{S}_t = \text{Select 5 speakers with the lowest } f_s(s) \text{ for } t
%     \]

%     \State \textbf{Sample Utterances:}
%     For each $t \in \mathcal{T}_{\text{sampled}}$:
%     \[
%     \mathcal{U}_t = \text{Select utterances associated with } \mathcal{S}_t
%     \]

%     \State \textbf{Save Results:}
%     \[
%     \mathcal{S}_{\text{sampled}} = \{(t, s_1, s_2, \dots, s_5) \mid t \in \mathcal{T}_{\text{sampled}}, s_i \in \mathcal{S}_t\}
%     \]
%     \[
%     \mathcal{U}_{\text{sampled}} = \{(t, u_1, u_2, \dots, u_5) \mid t \in \mathcal{T}_{\text{sampled}}, u_i \in \mathcal{U}_t\}
%     \]
% \end{algorithmic}
% \end{algorithm}

% The purpose of this algorithm is to sample five utterances for each text ID in the set of sampled text IDs (\(\mathcal{T}_{\text{sampled}}\)) from the previous algorithm. The main steps are as follows:

% \paragraph{1. Filter Utterances}
% The algorithm filters the set of all utterances (\(\mathcal{U}\)) to include only those that belong to the sampled text IDs (\(\mathcal{T}_{\text{sampled}}\)) and are part of the training set (\(\mathcal{U}_{\text{train}}\)).

% \paragraph{2. Compute Speaker Frequencies}
% The frequency of each speaker ID in the filtered set of utterances (\(\mathcal{U}_{\text{filtered}}\)) is calculated.

% \paragraph{3. Sample Speaker IDs}
% For each text ID in \(\mathcal{T}_{\text{sampled}}\), five speakers with the lowest frequency are selected.

% \paragraph{4. Sample Utterances}
% For the selected speakers of each text ID, the corresponding utterances are sampled.

% \paragraph{5. Save Results}
% The sampled speaker IDs (\(\mathcal{S}_{\text{sampled}}\)) and sampled utterance IDs (\(\mathcal{U}_{\text{sampled}}\)) are saved in tabular format. Each text ID is associated with exactly five speaker IDs and five utterance IDs.

% This ensures a balanced and fair sampling of utterances for each text ID.

\section{Dev, Test, and Train Sets Creation}

\subsection{Dev and Test Sets Creation}
Creating the dev and test sets from the dataset $D_{Z}$ of Dialect Z involves careful selection of speakers and utterances, ensuring diversity and avoiding speaker contamination between sets. The dataset $D_{Z}$ consists of audio files categorized into three slabs: $A_{c}$ (Clean), $A_{sn}$ (Semi-noisy), and $A_{n}$ (Noisy), as described in Appendix~\ref{sec:slab_def}. Each utterance belongs to one of these slabs, and no speaker or text ID is repeated across dev, test, and train splits.

Since the dev and test sets are relatively small compared to the training set, and need to be balanced across dialects, domains, and genders, we prioritize their creation. Balance is maintained wherever possible in the number of utterances across domains (Agriculture and Banking), task types (Question and Statement), and speaker gender.

The pseudocode below outlines the initialization and constraints applied to construct these sets:

\begin{algorithm}[H]
\SetAlgoLined
\caption{Test Set and Dev Set Creation}
\begin{algorithmic}[1]
\State \textbf{Initialize:}
\State Define $A$ as the total number of audio files in $D_{Z}$.
\State Define subsets $A_{c}$, $A_{sn}$, and $A_{n}$ corresponding to the three slabs (see Section~\ref{sec:slab_def} for slab definitions).
\State Define $S$ as the total number of speakers, and $S_{uncont}$ as the set of uncontaminated speaker IDs (see Section~\ref{appendix:speaker_bucket} for contamination criteria).
\State Let max\_rep represent the maximum allowed repetitions of any text ID across the test and dev sets.
\State \textbf{Diversity Constraints:}
\State Maintain balance across task (Question, Statement), domain (Banking, Agriculture), and gender (male, female).
\State Ensure minimal loss of utterances during test and dev set creation.
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsubsection{Test Set Creation}
The test set is constructed by filtering the list of uncontaminated speakers and selecting utterances from the clean slab $A_c$. Gender balance is enforced by sampling from both male and female speaker pools, denoted $S_{ms}$ and $S_{fs}$. For each selected speaker, exactly one utterance is chosen for each of the four combinations of task and domain.

To avoid repeated evaluation content, utterances with overused text IDs are discarded after reaching the \texttt{max\_rep} limit. A final diversity analysis ensures the test set $A_{test}$ is balanced. The process is summarized below:

\begin{algorithm}[H]
\caption{\textbf{Test Set Creation:}}
\begin{algorithmic}[1]
\State Filter speakers based on $S_{\textit{uncont}}$.
\State Select $A_c$ from $S_{\textit{uncont}}$ for test set creation.
\State Form subsets $S_{ms}$ and $S_{fs}$, ensuring balance in tasks and domains.
\State \textbf{for} each selected speaker per gender \textbf{do}
\State \hspace{1em} Select a single $A_c$ utterance for each of the following:
\State \hspace{2em} Task: Question, Domain: Agriculture
\State \hspace{2em} Task: Statement, Domain: Agriculture
\State \hspace{2em} Task: Question, Domain: Banking
\State \hspace{2em} Task: Statement, Domain: Banking
\State \hspace{1em} Delete all utterances with text IDs selected \texttt{max\_rep} times.
\State \textbf{end for}
\State Perform diversity analysis. If constraints are met, freeze the test set $A_{\textit{test}}$.
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsubsection{Dev Set Creation}
The development set is generated using a similar strategy to the test set, with the added constraint that neither speakers nor text IDs from the test set are reused. As before, gender and task-domain balance is ensured through careful sampling. The finalized dev set $A_{dev}$ is frozen once diversity constraints are satisfied.

\begin{algorithm}[H]
\caption{\textbf{Dev Set Creation:}}
\begin{algorithmic}[1]
\State Filter speakers based on $S_{\textit{uncont}}$, excluding those from $A_{\textit{test}}$.
\State Select $A_{\textit{slab1}}$ from $S_{\textit{uncont}}$ for dev set creation.
\State Form subsets $S_{ms}$ and $S_{fs}$, ensuring balance in tasks and domains.
\State \textbf{for} each selected speaker per gender \textbf{do}
\State \hspace{1em} Select a single $A_c$ utterance for each of the following:
\State \hspace{2em} Task: Question, Domain: Agriculture
\State \hspace{2em} Task: Statement, Domain: Agriculture
\State \hspace{2em} Task: Question, Domain: Banking
\State \hspace{2em} Task: Statement, Domain: Banking
\State \hspace{1em} Delete all utterances with text IDs selected \texttt{max\_rep} times.
\State \textbf{end for}
\State Perform diversity analysis. If constraints are met, freeze the dev set $A_{\textit{dev}}$.
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsection{Training Set Creation}\vspace{-3mm}
The training set $A_{train}$ is created by filtering out any utterances associated with speakers or text IDs from the test and dev sets. Diversity analysis is then performed to ensure the distribution of sentences, domains, and gender is consistent with the full dataset $D_Z$. The final training set is frozen after confirming that diversity constraints are met.

\begin{algorithm}[H]
\SetAlgoLined
\begin{algorithmic}[1]
\State \textbf{Training Set Creation:}
\State Filter out utterances belonging to any speaker or text ID from $A_{test}$ and $A_{dev}$.
\State Perform diversity analysis to verify that the training set $A_{train}$ reflects the diversity of $D_{Z}$.
\State Freeze the training set $A_{train}$.
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsection{Handling High Loss Dialects}
Some dialects, such as Mt\_D2, Hi\_D2, and Hi\_D4, suffer from low availability of clean and semi-noisy samples or fail to meet the diversity constraints due to skewed distributions. For such cases, we expand speaker selection to include both slabs $A_{slab1}$ and $A_{slab0.5}$. This ensures sufficient representation while preserving uncontaminated speaker quality.

\begin{algorithm}[H]
\SetAlgoLined
\begin{algorithmic}[1]
\State \textbf{Sampling for High Loss Dialects:}
\State Follow the same steps as for the test and dev sets, but include both $A_{slab1}$ and $A_{slab0.5}$ for speaker selection.
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsection{Balanced Sampling for Small Train Subsets}
To support broader usage and evaluation of the RESPIN corpus, we release a compact version of the training set for each of the nine languages, named \texttt{train\_<lang>\_small}. This subset is constructed using a principled balanced sampling strategy that ensures fair representation across dialects and domains while maintaining sufficient speaker and utterance diversity.

The first step in this process is to perform balanced sampling of text IDs from the available training data. The goal is to distribute the sampling quota equally across all dialects (\(\mathcal{D}_{\text{did}}\)), and then within each dialect, further divide it among its constituent domains (\(\mathcal{D}_{\text{dom}}\)). To ensure meaningful inclusion, only those text IDs with at least five utterances are considered valid for sampling. If a particular domain lacks enough valid text IDs to meet its quota, the deficit is compensated by drawing additional IDs from other domains within the same dialect. This ensures the target size is met without sacrificing dialectal balance.

The detailed logic is formalized in Algorithm~\ref{alg:balanced_sampling}, which outputs a set of sampled text IDs, \(\mathcal{T}_{\text{sampled}}\), balanced across dialect-domain pairs. These sampled IDs serve as the foundation for constructing the final training subset.

Once the text IDs are sampled, the next step is to select corresponding utterances. For each sampled text ID, we aim to include five utterances from distinct speakers. To avoid speaker imbalance, speakers with the lowest frequency of participation in the training set are prioritized.

Algorithm~\ref{alg:sample_utterances} outlines the procedure to filter utterances, compute speaker frequencies, and perform speaker-aware sampling. This guarantees both text diversity and speaker variation in the final subset.

\begin{algorithm}[H]
\caption{Balanced Sampling of Text IDs Across Dialects and Domains}
\label{alg:balanced_sampling}
\textbf{Input:} 
\begin{itemize}
    \item $\mathcal{T}$: Set of all text IDs
    \item $\mathcal{D}_{\text{did}}$: Set of dialect IDs
    \item $\mathcal{D}_{\text{dom}}$: Set of domain IDs
    \item $N_{\text{total}}$: Total number of text IDs to sample
\end{itemize}

\textbf{Output:} 
\(\mathcal{T}_{\text{sampled}}\): Balanced sampled text IDs

\begin{algorithmic}[1]
    \State \textbf{Filter Valid Text IDs:}
    \[
    \mathcal{T}_{\text{valid}} = \{t \in \mathcal{T} \mid |\text{utterances}(t)| \geq 5\}
    \]
    \State \textbf{Set Sampling Targets:}
    \[
    N_{\text{dialect}} = \frac{N_{\text{total}}}{|\mathcal{D}_{\text{did}}|}
    \quad\quad
    N_{\text{target}}(d, o) = \frac{N_{\text{dialect}}}{|\{o \in \mathcal{D}_{\text{dom}} \mid d \text{ fixed}\}|}
    \]
    \State \textbf{Sample Text IDs:}
    For each $(d, o) \in \mathcal{D}_{\text{did}} \times \mathcal{D}_{\text{dom}}$:
    \[
    \mathcal{T}_{\text{sampled}}(d, o) = 
    \begin{cases} 
        \text{Compensate from other domains}, & \text{if } |\text{available}(d, o)| < N_{\text{target}}(d, o) \\
        \text{Sample within domain}, & \text{otherwise}
    \end{cases}
    \]
    Append \(\mathcal{T}_{\text{sampled}}(d, o)\) to \(\mathcal{T}_{\text{sampled}}\)
    \State \textbf{Save Results:}
    \[
    \text{Save } \mathcal{T}_{\text{sampled}} \text{ to output files}
    \]
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\begin{algorithm}[H]
\caption{Sample 5 Utterances for Each Text ID}
\label{alg:sample_utterances}
\textbf{Input:} 
\begin{itemize}
    \item $\mathcal{T}_{\text{sampled}}$: Set of sampled text IDs from the previous algorithm
    \item $\mathcal{U}$: Set of all utterances with associated text and speaker IDs
    \item $\mathcal{U}_{\text{train}}$: Set of utterances in the training set
\end{itemize}

\textbf{Output:} 
\begin{itemize}
    \item $\mathcal{S}_{\text{sampled}}$: Table of speaker IDs sampled for each text ID
    \item $\mathcal{U}_{\text{sampled}}$: Table of utterance IDs sampled for each text ID
\end{itemize}

\begin{algorithmic}[1]
    \State \textbf{Filter Utterances:}
    \[
    \mathcal{U}_{\text{filtered}} = \{u \in \mathcal{U} \mid \text{tid}(u) \in \mathcal{T}_{\text{sampled}} \text{ and } u \in \mathcal{U}_{\text{train}}\}
    \]
    \State \textbf{Compute Speaker Frequencies:}
    \[
    f_s(s) = \text{Count of occurrences of each speaker ID } s \text{ in } \mathcal{U}_{\text{filtered}}
    \]
    \State \textbf{Sample Speaker IDs:}
    For each $t \in \mathcal{T}_{\text{sampled}}$:
    \[
    \mathcal{S}_t = \text{Select 5 speakers with the lowest } f_s(s) \text{ for } t
    \]
    \State \textbf{Sample Utterances:}
    For each $t \in \mathcal{T}_{\text{sampled}}$:
    \[
    \mathcal{U}_t = \text{Select utterances associated with } \mathcal{S}_t
    \]
    \State \textbf{Save Results:}
    \[
        \mathcal{S}_{\text{sampled}} = \{(t, s_1, s_2, \dots, s_5) \mid t \in \mathcal{T}_{\text{sampled}}, s_i \in \mathcal{S}_t\}
    \]
    \[
        \mathcal{U}_{\text{sampled}} = \{(t, u_1, u_2, \dots, u_5) \mid t \in \mathcal{T}_{\text{sampled}}, u_i \in \mathcal{U}_t\}
    \]
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsection{Training Set Statistics and Insights}

\begin{table}[htbp]
\centering
\caption{Language-wise statistics for Clean, Semi-noisy, and Noisy train sets including duration, utterances, unique sentences, and speaker counts.}
\label{tab:trainset_stats}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ll|rrrr|rrrr|rrrr}
\toprule
\textbf{LID} & \textbf{\#Dialects} & 
\multicolumn{4}{c|}{\textbf{Clean Train Set}} & 
\multicolumn{4}{c|}{\textbf{Semi-noisy Train Set}} & 
\multicolumn{4}{c}{\textbf{Noisy Train Set}} \\
 &  & \textbf{Dur (h)} & \textbf{\#Utts} & \textbf{\#Sents} & \textbf{\#Spks} & 
       \textbf{Dur (h)} & \textbf{\#Utts} & \textbf{\#Sents} & \textbf{\#Spks} & 
       \textbf{Dur (h)} & \textbf{\#Utts} & \textbf{\#Sents} & \textbf{\#Spks} \\
\midrule
bh & 3 & 889.61  & 694738 & 21967 & 1851 & 91.83   & 58179  & 16493 & 1505 & 68.53  & 43543  & 15256 & 1392 \\
bn & 5 & 894.60  & 645791 & 20187 & 1791 & 162.84  & 92208  & 17766 & 1479 & 5.77   & 6587   & 5393  & 805  \\
ch & 4 & 1009.09 & 640649 & 19400 & 2082 & 161.28  & 78942  & 17334 & 2048 & 73.96  & 36486  & 14456 & 2024 \\
hi & 5 & 712.61  & 574544 & 19345 & 2305 & 273.27  & 181710 & 19180 & 2304 & 304.55 & 186275 & 19286 & 2314 \\
kn & 5 & 888.72  & 554398 & 23294 & 1931 & 202.42  & 104324 & 22004 & 1919 & 191.18 & 97806  & 21918 & 1935 \\
mg & 4 & 1066.30 & 769679 & 21500 & 2078 & 91.30   & 53414  & 16361 & 2037 & 60.18  & 36206  & 14943 & 2025 \\
mr & 4 & 1026.06 & 809934 & 23069 & 2644 & 285.46  & 191098 & 21669 & 2629 & 174.16 & 109910 & 20306 & 2634 \\
mt & 4 & 552.16  & 392739 & 23108 & 1917 & 400.04  & 237484 & 23154 & 1916 & 314.21 & 174313 & 22609 & 1934 \\
te & 4 & 1021.16 & 702883 & 19978 & 2287 & 186.10  & 111471 & 19370 & 2273 & 127.66 & 73390  & 17914 & 2258 \\
\midrule
\textbf{Total} & \textbf{38} & \textbf{8060.31} & \textbf{5785355} & \textbf{191848} & \textbf{18886} & 
\textbf{1854.54} & \textbf{1108830} & \textbf{173331} & \textbf{18110} & 
\textbf{1320.20} & \textbf{764516} & \textbf{152081} & \textbf{17325} \\
\bottomrule
\end{tabular}
}
\begin{minipage}{\textwidth}
\footnotesize
\textbf{LID}: Language ID, \textbf{\#Dialects}: number of dialects, \textbf{Dur}: duration in hours, 
\textbf{\#Utts}: number of utterances, \textbf{\#Sents}: number of unique sentences, \textbf{\#Spks}: number of speakers.
\end{minipage}%\vspace{-5mm}
\end{table}

Table~\ref{tab:trainset_stats} provides a comprehensive summary of language-wise training set statistics across the Clean, Semi-noisy, and Noisy slabs in the RESPIN corpus. For each language, the table presents the number of dialects, total duration in hours, number of utterances, number of unique sentences, and number of speakers for each slab.

The Clean Train Set comprises high-quality, curated audio data and represents the largest portion of the training set, totaling 8060.31 hours and over 5.78 million utterances. Languages like Marathi (mr), Magahi (mg), and Chhattisgarhi (ch) contribute significantly to the clean set, with over 1000 hours each. The number of speakers per language ranges from approximately 1800 to over 2600, ensuring high speaker diversity.

The Semi-noisy Train Set introduces moderate background noise and variability, offering a middle ground between clean and highly degraded conditions. It adds approximately 1854.54 hours and 1.1 million utterances to the training data. Hindi (hi), Maithili (mt), and Marathi (mr) are the top contributors in terms of duration. Speaker coverage remains uniformly distributed, preserving balance across languages.

The Noisy Train Set is characterized by challenging acoustic conditions and contains 1320.20 hours and 764,516 utterances. Despite filtering for quality, a sizable portion of the data remains usable. Notably, languages such as Hindi and Maithili still provide substantial noisy data, while Bengali (bn) contributes significantly less due to stringent filtering, offering only 5.77 hours.

Across all three slabs combined, the RESPIN training corpus comprises approximately 11,235 hours of audio spanning over 7.96 million utterances, 517,260 unique sentences, and 54,321 speakers (non-unique across slabs). This extensive and diverse training set enables the development of robust speech models capable of generalizing across noise conditions, dialectal variations, and speaker demographics.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Audio Data Analysis}
\label{appendix:audio_analysis}
The distributions of SNR values, durations and speaking rates for RESPIN audio data are specified in sections \ref{sect:snr_dist} and \ref{sect:dur_sr_dist}, respectively

\subsection{SNR-Based Audio Quality Analysis}
\label{sect:snr_dist}

\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/bn_snr.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/bh_snr.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/ch_snr.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/hi_snr.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/kn_snr.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/mg_snr.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/mt_snr.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/mr_snr.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-100/te_snr.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of SNR values for slab \textit{Clean}}
\label{fig:snr_clean}
\end{figure}


% \textbf{Slab Semi-noisy Audio}

\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/bn_50_snr.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/bh_50_snr.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/ch_50_snr.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/hi_50_snr.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/kn_50_snr.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/mg_50_snr.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/mt_50_snr.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/mr_50_snr.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-50/te_50_snr.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of SNR values for slab \textit{Semi-noisy}}
\label{fig:snr_semi}
\end{figure}


% \textbf{Slab Noisy Audio}

\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/bn_0_snr.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/bh_0_snr.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/ch_0_snr.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/hi_0_snr.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/kn_0_snr.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/mg_0_snr.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/mt_0_snr.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/mr_0_snr.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SNR/slab-0/te_0_snr.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of SNR values for slab \textit{Noisy}}
\label{fig:snr_noisy}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Language-wise audio statistics in the Semi-noisy slab of RESPIN. The table shows the number of files, count and percentage of utterances with SNR $<$ 4 dB, and speaking rate features.}
\label{tab:audio_stats_semi}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{lccccccccc}
\toprule
\textbf{LID} & bn & bh & ch & hi & kn & mg & mt & mr & te \\
\midrule
\textbf{\# Files}      & 107{,}920 & 66{,}966 & 98{,}095 & 238{,}370 & 129{,}533 & 65{,}546 & 304{,}410 & 223{,}938 & 133{,}286 \\
\textbf{\# Low SNR}    & 4{,}358   & 1{,}607  & 844      & 1{,}453   & 877      & 989      & 1{,}706   & 1{,}445   & 1{,}509 \\
\textbf{\% Low SNR}    & 4.04      & 2.40     & 0.86     & 0.61      & 0.68     & 1.51     & 0.56      & 0.65      & 1.13 \\
\textbf{Wds/Aud}       & 9.28      & 10.11    & 14.91    & 11.59     & 9.12     & 10.72    & 11.19     & 9.04      & 9.02 \\
\textbf{Dur (s)}       & 5.38      & 4.91     & 6.45     & 4.75      & 5.95     & 5.30     & 5.22      & 4.64      & 5.15 \\
\textbf{WPM}           & 118.90    & 134.19   & 149.81   & 157.58    & 100.05   & 136.48   & 137.63    & 124.74    & 112.55 \\
\bottomrule
\end{tabular}
\end{adjustbox}

\vspace{6pt}
\noindent
\small \textbf{Abbreviations:} LID = Language ID; \# Files = Number of audio files; \# Low SNR = Number of utterances with SNR $<$ 4 dB; \% Low SNR = Proportion of low-SNR utterances; Wds/Aud = Average words per utterance; Dur (s) = Average utterance duration in seconds; WPM = Words per minute.
\end{table}



\begin{table}[htbp]
\centering
\caption{Language-wise audio statistics in the Noisy slab of RESPIN, showing number of files, count and percentage of utterances with SNR $<$ 4 dB.}
\label{tab:audio_stats_noisy}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{lccccccccc}
\toprule
\textbf{LID} & bn & bh & ch & hi & kn & mg & mt & mr & te \\
\midrule
\textbf{\# Files} & 10{,}413 & 50{,}799 & 47{,}657 & 238{,}990 & 118{,}999 & 46{,}119 & 227{,}340 & 128{,}562 & 88{,}378 \\
\textbf{\# Low SNR} & 4{,}170 & 7{,}436 & 4{,}102 & 10{,}130 & 6{,}774 & 4{,}198 & 7{,}620 & 6{,}077 & 4{,}859 \\
\textbf{\% Low SNR} & 40.05 & 14.64 & 8.61 & 4.24 & 5.69 & 9.10 & 3.35 & 4.73 & 5.50 \\
\bottomrule
\end{tabular}
\end{adjustbox}

\vspace{6pt}
\noindent
\small \textbf{Abbreviations:} LID = Language ID; \# Files = Number of audio files; \# Low SNR = Number of utterances with SNR $<$ 4 dB; \% Low SNR = Proportion of low-SNR utterances.
\end{table}


We use Signal-to-Noise Ratio (SNR) as a proxy to quantify audio quality across the RESPIN dataset. SNR measures the ratio of speech signal energy to background noise energy, expressed in decibels (dB). A higher SNR indicates a cleaner, less noisy recording, making it a useful metric for characterizing the quality of speech data.

\paragraph{SNR Computation Methodology.}
As shown in Figure~\ref{fig:snr_computation}, we compute the SNR of each utterance using the pretrained \texttt{FB-Denoiser}\footnote{\url{https://github.com/facebookresearch/denoiser}} model, which is based on the DEMUCS architecture for speech enhancement. Given an original audio sample \( x_m \), we generate its denoised counterpart \( y_m \), and compute the estimated noise as \( n_m = x_m - y_m \). To ensure that the SNR reflects only the spoken content, non-speech segments at the start and end of the audio are trimmed using forced alignment timestamps generated by a Kaldi TDNN-HMM model trained on slab Clean.

The SNR value is calculated using the standard formula:
\begin{equation}
\text{SNR} = 10 \log_{10} \left( \frac{X}{N} \right)
\label{equ:snr}
\end{equation}
where \( X \) is the average signal power, and \( N \) is the average noise power. These are computed as:
\begin{equation}
X = \frac{1}{m} \sum_{i=1}^{m} (x_i)^2
\label{equ:sig_en}
\end{equation}
\begin{equation}
N = \frac{1}{m} \sum_{i=1}^{m} (n_i)^2
\label{equ:noise_en}
\end{equation}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.6\linewidth]{figures/SNR/snr_computation.jpeg}
\caption{Calculation of SNR values using FB-Denoiser}
\label{fig:snr_computation}
\end{figure}

\paragraph{SNR Distributions Across Slabs:}
Figures~\ref{fig:snr_clean}, \ref{fig:snr_semi}, and~\ref{fig:snr_noisy} visualize the distributions of SNR values for all nine languages across the Clean, Semi-noisy, and Noisy slabs, respectively. These histograms clearly show how signal quality varies across slabs.

In the Clean slab (Figure~\ref{fig:snr_clean}), SNR values are tightly concentrated between 25 and 40 dB, with minimal low-SNR outliers. Languages like Hindi, Kannada, and Maithili demonstrate particularly clean profiles with sharp histogram peaks and minimal long tails.

In the Semi-noisy slab (Figure~\ref{fig:snr_semi}), the SNR distributions are noticeably wider, and the peaks shift leftward. Table~\ref{tab:audio_stats_semi} reports that the percentage of utterances with SNR below 4 dB is still relatively low—ranging from 0.56\% for Maithili to 4.04\% for Bengali. Hindi and Maithili continue to exhibit stable audio quality, while Bengali, Bhojpuri, and Magahi show modest increases in low-SNR samples.

In the Noisy slab (Figure~\ref{fig:snr_noisy}), the degradation becomes more evident. Bengali has the most substantial drop in quality, with 40.05\% of its utterances falling below 4 dB, as shown in Table~\ref{tab:audio_stats_noisy}. Bhojpuri, Chhattisgarhi, and Magahi also show elevated noise levels. In contrast, Hindi and Maithili retain a relatively small proportion of low-SNR files even in the Noisy slab, suggesting better recording conditions or noise resilience during collection.

SNR variation is not only slab-dependent but also language-specific. Bengali consistently exhibits more low-SNR recordings, while Maithili and Hindi maintain relatively clean audio across all slabs. This difference may stem from differences in recording environments, devices used, or speaker demographics. Additionally, the long-tailed nature of the Noisy distributions reflects the presence of a small but significant number of extremely noisy files that may require additional filtering in downstream tasks.

\paragraph{Summary}
These SNR-based analyses validate the design of the slab-based data curation pipeline. The progressive leftward shift of SNR distributions from Clean to Semi-noisy to Noisy slabs confirms the intended stratification of audio quality. This tiered structure makes RESPIN suitable for benchmarking ASR models under varying levels of noise and enables controlled experimentation for noise-robust speech recognition in diverse Indian language dialects.


\subsection{Speech Duration and Speaking Rate Analysis}
\label{sect:dur_sr_dist}

% \textbf{Slab Clean Audio}
\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/bn_100_sr_plot.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/bh_100_sr_plot.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/ch_100_sr_plot.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/hi_100_sr_plot.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/kn_100_sr_plot.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/mg_100_sr_plot.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/mt_100_sr_plot.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/mr_100_sr_plot.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-100/te_100_sr_plot.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of WPM values for slab Clean}
\label{fig:wpm_clean}
\end{figure}


% \textbf{Slab Semi-noisy Audio}

\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/bn_50_sr_plot.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/bh_50_sr_plot.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/ch_50_sr_plot.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/hi_50_sr_plot.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/kn_50_sr_plot.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/mg_50_sr_plot.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/mt_50_sr_plot.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/mr_50_sr_plot.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{figures/SR/slab-50/te_50_sr_plot.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of WPM values for slab Semi-noisy}
\label{fig:wpm_semi}
\end{figure}


% \textbf{Slab Clean Audio}
\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/bn_100_durs_plot.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/bh_100_durs_plot.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/ch_100_durs_plot.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/hi_100_durs_plot.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/kn_100_durs_plot.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/mg_100_durs_plot.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/mt_100_durs_plot.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/mr_100_durs_plot.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-100/te_100_durs_plot.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of audio durations for slab Clean}
\label{fig:dur_clean}
\end{figure}


% \textbf{Slab Semi-noisy Audio}
\begin{figure}[htbp]
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{minipage}{\linewidth}
\centering
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/bn_50_durs_plot.eps}
  \caption{Bengali audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/bh_50_durs_plot.eps}
  \caption{Bhojpuri audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/ch_50_durs_plot.eps}
  \caption{Chhattisgarhi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/hi_50_durs_plot.eps}
  \caption{Hindi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/kn_50_durs_plot.eps}
  \caption{Kannada audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/mg_50_durs_plot.eps}
  \caption{Magahi audio}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/mt_50_durs_plot.eps}
  \caption{Maithili audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/mr_50_durs_plot.eps}
  \caption{Marathi audio}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\linewidth}
  \includegraphics[width=\linewidth]{Styles/figures/SR/slab-50/te_50_durs_plot.eps}
  \caption{Telugu audio}
\end{subfigure}
\end{minipage}%
}
\caption{Histograms of audio durations for slab Semi-noisy}
\label{fig:dur_semi}
\end{figure}

We analyze two important prosodic features of the RESPIN audio corpus—utterance duration and speaking rate (in words per minute, WPM)—across the Clean and Semi-noisy slabs. These metrics help assess consistency in recording conditions and variability in natural speech across languages.

% \vspace{0.5em}
\textbf{Utterance Duration:}
Utterance duration is computed from forced alignment outputs obtained using a Kaldi TDNN-HMM model trained on Clean audio. We define the duration as the time interval between the start of the first word and the end of the last word, ignoring leading/trailing silences. Figures~\ref{fig:dur_clean} and~\ref{fig:dur_semi} show the distribution of utterance durations for each language in the Clean and Semi-noisy slabs, respectively.

In the Clean slab, most languages exhibit tightly peaked distributions around 4–6 seconds with minimal long tails, indicating consistent control during scripted recording. In contrast, the Semi-noisy slab shows greater variability and longer tails, especially for Kannada and Maithili, which display more spread-out patterns. These differences reflect the less constrained and more spontaneous nature of recordings in Semi-noisy conditions.

% \vspace{0.5em}
\textbf{Speaking Rate:}
Speaking rate is defined as the number of words spoken per minute in each utterance. We compute this by dividing the number of tokenized words by the utterance duration (in seconds) and scaling appropriately. Figures~\ref{fig:wpm_clean} and~\ref{fig:wpm_semi} present the WPM histograms for Clean and Semi-noisy slabs.

In the Clean slab, the distributions are largely Gaussian-shaped and fall between 110–150 WPM. Languages such as Hindi, Maithili, and Chhattisgarhi exhibit higher average rates (above 140 WPM), while Kannada and Bengali cluster toward the lower end. The Semi-noisy slab, however, reveals increased spread and irregularities in speaking rate. Notably, Bengali and Kannada show low WPM outliers (<50 WPM), whereas Hindi retains a relatively consistent and faster rate. These trends are quantitatively supported by Table~\ref{tab:audio_stats_semi}, which shows Hindi reaching 157.58 WPM, and Kannada at 100.05 WPM.

Together, these observations highlight how the Clean slab offers tightly controlled speech characteristics, while the Semi-noisy slab introduces useful variability for training robust ASR models suited to real-world speech.

\section{Benchmarking ASR performance}

\begin{table}[htbp]
    \centering
    \caption{Dialect-wise and overall CER and WER (\%) for different fairseq-based models across languages. \textbf{Pretrained models} refer to models fine-tuned on publicly available data other than RESPIN. \textbf{Fine-tuned models} are pretrained SSL models further fine-tuned on a subset of RESPIN. For pretrained SSL models, \textbf{bh}, \textbf{ch}, \textbf{mg}, and \textbf{mt} are evaluated using Hindi-tuned models.}
    \resizebox{\columnwidth}{!}{%
    % \begin{tabular}{lccccccccccccccccccc}
    \begin{tabular}{lccccc@{\hskip 8pt}ccccc|ccccccccc}
    \midrule
     & \multicolumn{1}{l}{} & \multicolumn{9}{c}{\textbf{CER (\%)}} & \multicolumn{9}{c}{\textbf{WER (\%)}} \\ \cmidrule(lr){3-11} \cmidrule(lr){12-20}
    \multicolumn{1}{c}{\textbf{Model}} & \textbf{Dialect} & \textbf{bh} & \textbf{bn} & \textbf{ch} & \textbf{hi} & \textbf{kn} & \textbf{mg} & \textbf{mr} & \textbf{mt} & \textbf{te} & \textbf{bh} & \textbf{bn} & \textbf{ch} & \textbf{hi} & \textbf{kn} & \textbf{mg} & \textbf{mr} & \textbf{mt} & \textbf{te} \\ \hline

    \multicolumn{7}{l}{\cellcolor[HTML]{FFFFFF}\textbf{pre-trained IndicW2V2 (fine-tuned on non-RESPIN public data)}} \\
    \rowcolor[HTML]{FFFFFF} & D1 & 17.69 & 15.01 & 23.17 & 9.70 & 8.32 & 22.14 & 16.86 & 24.44 & 8.94 & 52.69 & 45.45 & 65.38 & 25.75 & 38.89 & 55.66 & 63.59 & 68.47 & 37.63 \\
    \rowcolor[HTML]{F2F2F2} & D2 & 15.76 & 12.64 & 22.27 & 13.35 & 5.00 & 17.11 & 15.32 & 23.08 & 8.47 & 48.54 & 38.38 & 65.45 & 34.32 & 21.92 & 48.95 & 52.81 & 65.38 & 38.93 \\
    \rowcolor[HTML]{FFFFFF} & D3 & 17.65 & 10.89 & 22.94 & 9.44 & 16.36 & 19.01 & 12.54 & 23.86 & 8.17 & 53.28 & 33.25 & 67.51 & 25.07 & 59.02 & 52.82 & 44.87 & 67.12 & 36.25 \\
    \rowcolor[HTML]{F2F2F2} & D4 & - & 17.09 & 22.75 & 12.54 & 17.96 & 20.99 & 15.50 & 21.84 & 8.90 & - & 52.33 & 65.70 & 31.49 & 65.06 & 62.68 & 53.68 & 63.58 & 38.64 \\
    \rowcolor[HTML]{FFFFFF} & D5 & - & 16.08 & - & 8.48 & 5.66 & - & - & - & - & - & 45.88 & - & 21.99 & 27.27 & - & - & - & - \\
    \rowcolor[HTML]{F2F2F2} & Overall & 17.08 & 14.27 & 22.77 & 11.02 & 10.37 & 19.64 & 15.09 & 23.30 & 8.61 & 51.61 & 42.83 & 65.98 & 28.34 & 42.37 & 54.32 & 53.91 & 66.10 & 37.82 \\
    \multicolumn{7}{l}{\cellcolor[HTML]{FFFFFF}\textbf{pre-trained SPRING-W2V2 (fine-tuned on non-RESPIN public data) }} \\
     \rowcolor[HTML]{FFFFFF}& D1 & 15.75 & 11.81 & 20.70 & 8.68 & 9.50 & 18.30 & 9.38 & 22.11 & 6.57 & 42.38 & 26.03 & 54.07 & 23.66 & 39.48 & 44.48 & 40.43 & 57.98 & 33.05 \\
     \rowcolor[HTML]{F2F2F2}& D2 & 15.08 & 12.85 & 20.23 & 9.76 & 5.34 & 13.78 & 7.47 & 17.25 & 6.93 & 40.64 & 25.22 & 53.62 & 24.87 & 27.99 & 34.24 & 35.15 & 49.38 & 37.42 \\
     \rowcolor[HTML]{FFFFFF}& D3 & 14.48 & 11.39 & 20.88 & 8.23 & 16.79 & 15.74 & 5.43 & 20.12 & 6.80 & 40.86 & 21.82 & 55.89 & 20.98 & 56.79 & 41.67 & 25.78 & 54.19 & 35.31 \\
     \rowcolor[HTML]{F2F2F2}& D4 & - & 14.44 & 21.40 & 8.68 & 20.91 & 18.46 & 7.81 & 21.93 & 7.67 & \multicolumn{1}{l}{-} & 30.30 & 57.91 & 24.11 & 64.73 & 51.46 & 34.68 & 54.41 & 39.69 \\
     \rowcolor[HTML]{FFFFFF}& D5 & - & 12.03 & - & 8.33 & 6.07 & - & - & - & - & - & 26.65 & - & 20.55 & 32.71 & - & - & - & - \\
     \rowcolor[HTML]{F2F2F2}& Overall & 15.10 & 12.50 & 20.81 & 8.80 & 11.43 & 16.35 & 7.56 & 20.12 & 6.97 & 41.32 & 25.93 & 55.42 & 22.99 & 44.35 & 42.09 & 34.15 & 53.69 & 36.32 \\

    \multicolumn{9}{l}{\cellcolor[HTML]{FFFFFF}\textbf{pre-trained SPRING-Data2Vec-AQC (fine-tuned on non-RESPIN public data)}} \\
    \rowcolor[HTML]{FFFFFF}  & D1 &  15.83 & 11.17 & 21.16 & 6.31 & 8.73 & 17.14 & 9.60 & 20.77 & 5.90 &  43.67   & 23.99 & 55.35 & 20.74 & 37.37 & 44.33 & 40.64 & 56.27 & 31.01 \\
    \rowcolor[HTML]{F2F2F2}  & D2 & 14.88 & 12.59 & 20.78 & 8.62 & 4.63 & 13.43 & 7.54 & 18.10 & 6.60 &  42.00 & 24.18 & 54.73 & 22.85 & 25.90 & 34.69 & 34.34 & 50.42 & 34.90 \\
    \rowcolor[HTML]{FFFFFF}  & D3 &  14.38  & 11.36 & 21.39 & 6.09 & 15.07 & 15.49 & 5.22 & 20.73 & 6.43 &  41.37 & 21.55 & 56.26 & 18.86 & 55.92 & 42.68 & 24.85 & 55.95 & 32.50 \\
    \rowcolor[HTML]{F2F2F2}  & D4 & - & 13.78 & 21.71 & 7.01 & 21.15 & 18.02 & 7.43 & 20.59 & 7.30 & - & 27.00 & 58.19 & 21.43 & 64.04 & 51.60 & 33.13 & 52.91 & 37.75 \\
    \rowcolor[HTML]{FFFFFF}  & D5 & - & 10.80 & - & 7.25 & 5.57 & - & - & - & - & - & 22.08 & - & 19.89 & 30.65 & - & - & - & - \\ 
    \rowcolor[HTML]{F2F2F2}  & Overall & 15.02 \rowcolor[HTML]{F2F2F2}& 11.94 & 21.26 & 7.20 & 10.78 & 15.81 & 7.49 & 19.91 & 6.53 &  42.35 & 23.69 & 56.17 & 20.93 & 42.79 & 42.47 & 33.40 & 53.65 & 33.98 \\ \midrule
    \multicolumn{7}{l}{\cellcolor[HTML]{FFFFFF}\textbf{IndicW2V2 (fine-tuned on RESPIN subset) }}  \\
    \rowcolor[HTML]{FFFFFF} & D1 & 4.45 & 4.09 & 3.13 & 2.59 & 4.15 & 6.76 & 4.54 & 5.7 & 4.78 & 15.89 & 15.57 & 11.13 & 9.14 & 23.46 & 23.90 & 21.61 & 21.43 & 23.55 \\
    \rowcolor[HTML]{F2F2F2} & D2 & 4.22 & 3.55 & 2.96 & 4.23 & 2.12 & 5.52 & 3.08 & 5.49 & 4.07 & 15.64 & 15.02 & 10.40 & 12.93 & 12.08 & 17.99 & 16.19 & 19.26 & 22.87 \\
    \rowcolor[HTML]{FFFFFF} & D3 & 4.57 & 3.01 & 2.88 & 1.92 & 7.05 & 5.55 & 2.11 & 4.4 & 4.41 & 16.63 & 11.29 & 10.56 & 7.29 & 35.98 & 21.49 & 8.66 & 17.55 & 23.82 \\
    \rowcolor[HTML]{F2F2F2} & D4 & - & 6.1 & 3.89 & 4.01 & 8.03 & 6.43 & 2.92 & 5.07 & 4.91 & - & 22.53 & 13.16 & 13.20 & 37.29 & 23.76 & 13.51 & 18.35 & 25.93 \\
    \rowcolor[HTML]{FFFFFF} & D5 & - & 4.75 & - & 2.18 & 2.68 & - & - & - & - & - & 19.13 & - & 7.77 & 15.98 & - & - & - & - \\
    \rowcolor[HTML]{F2F2F2} & Overall & 4.42 & 4.28 & 3.24 & 3.16 & 4.68 & 6.02 & 3.19 & 5.19 & 4.54 & 16.07 & 16.65 & 11.36 & 10.47 & 24.86 & 21.51 & 15.13 & 19.19 & 24.03 \\
    \multicolumn{7}{l}{\cellcolor[HTML]{FFFFFF}\textbf{SPRING-W2V2 (fine-tuned on RESPIN subset)}}   \\
    \rowcolor[HTML]{FFFFFF} & D1 & 3.94 & 3.45 & 2.75 & 2.04 & 3.75 & 5.77 & 3.62 & 4.80 & 3.81 & 14.81 & 13.22 & 9.56 & 7.61 & 21.89 & 20.87 & 18.02 & 18.12 & 20.05 \\
    \rowcolor[HTML]{F2F2F2} & D2 & 3.76 & 3.05 & 2.73 & 3.17 & 1.75 & 4.71 & 2.48 & 4.57 & 3.70 & 14.02 & 13.89 & 9.85 & 10.28 & 10.96 & 15.97 & 14.10 & 16.85 & 21.61 \\
    \rowcolor[HTML]{FFFFFF} & D3 & 4.04 & 2.82 & 2.75 & 1.49 & 6.75 & 4.94 & 1.39 & 3.74 & 3.70 & 14.93 & 9.83 & 10.47 & 5.60 & 35.82 & 19.81 & 6.33 & 15.19 & 21.51 \\
    \rowcolor[HTML]{F2F2F2} & D4 & - & 5.63 & 3.64 & 2.90 & 7.60 & 5.53 & 2.41 & 4.32 & 4.23 & - & 20.89 & 12.86 & 9.60 & 36.81 & 22.17 & 12.05 & 16.25 & 24.62 \\
    \rowcolor[HTML]{FFFFFF} & D5 & - & 4.40 & - & 1.69 & 2.33 & - & - & - & - & - & 17.94 & - & 6.69 & 14.55 & - & - & - & - \\
    \rowcolor[HTML]{F2F2F2} & Overall & 3.92 & 3.86 & 2.99 & 2.37 & 4.30 & 5.20 & 2.49 & 4.37 & 3.85 & 14.61 & 15.12 & 10.74 & 8.22 & 23.90 & 19.40 & 12.75 & 16.64 & 21.92 \\
    \multicolumn{1}{r}{\cellcolor[HTML]{FFFFFF}\textbf{SPRING-Data2Vec-AQC }} & \multicolumn{5}{l}{\cellcolor[HTML]{FFFFFF}\textbf{(fine-tuned on RESPIN subset)}} \\
    \rowcolor[HTML]{FFFFFF} & D1 & 3.95 & 3.48 & 2.68 & 2.03 & 3.59 & 5.48 & 3.73 & 4.95 & 3.60 & 14.79 & 13.27 & 9.72 & 7.77 & 19.95 & 20.01 & 18.56 & 19.05 & 19.27 \\
    \rowcolor[HTML]{F2F2F2} & D2 & 3.81 & 2.74 & 2.55 & 2.90 & 1.71 & 4.41 & 2.32 & 4.46 & 3.61 & 14.34 & 12.34 & 9.20 & 9.13 & 11.04 & 15.26 & 13.25 & 16.22 & 21.48 \\
    \rowcolor[HTML]{FFFFFF} & D3 & 4.07 & 2.48 & 2.54 & 1.67 & 6.40 & 5.00 & 1.35 & 3.58 & 3.61 & 15.33 & 8.87 & 9.51 & 6.19 & 35.02 & 19.56 & 6.43 & 14.50 & 20.42 \\
    \rowcolor[HTML]{F2F2F2} & D4 & - & 5.14 & 3.49 & 2.49 & 7.22 & 5.16 & 2.04 & 4.17 & 4.10 & - & 19.33 & 12.37 & 8.68 & 35.73 & 20.09 & 10.36 & 15.78 & 23.64 \\
    \rowcolor[HTML]{FFFFFF} & D5 & - & 4.37 & - & 1.87 & 2.26 & - & - & - & - & - & 17.22 & - & 7.05 & 14.42 & - & - & - & - \\  
    \rowcolor[HTML]{F2F2F2}  & Overall & 3.95 & 3.63 & 2.84 & 2.27 & 4.11 & 4.98 & 2.38 & 4.30 & 3.72 & 14.84 & 14.15 & 10.25 & 7.91 & 23.13 & 18.50 & 12.28 & 16.41 & 21.17 \\ \bottomrule
    
    
    \end{tabular}%
    }
    \label{tab:consolidated_resultsfariseq}
\end{table}


\begin{table}[htbp]
	\centering
        \caption{Dialect-wise and overall CER and WER (\%) for different Whisper and traditional models across languages. \textbf{Fine-tuned models} are Whisper models further fine-tuned on a subset of RESPIN. \textbf{Traditional models} are trained from scratch on RESPIN.}
	\resizebox{\columnwidth}{!}{%
	% \begin{tabular}{lccccccccccccccccccc}
	\begin{tabular}{lccccc@{\hskip 8pt}ccccc|ccccccccc}
	\midrule
		& \multicolumn{1}{l}{} & \multicolumn{9}{c}{\textbf{CER (\%)}} & \multicolumn{9}{c}{\textbf{WER (\%)}} \\ \cmidrule(lr){3-11} \cmidrule(lr){12-20}
	\multicolumn{1}{c}{\textbf{Model}} & \textbf{Dialect} & \textbf{bh} & \textbf{bn} & \textbf{ch} & \textbf{hi} & \textbf{kn} & \textbf{mg} & \textbf{mr} & \textbf{mt} & \textbf{te} & \textbf{bh} & \textbf{bn} & \textbf{ch} & \textbf{hi} & \textbf{kn} & \textbf{mg} & \textbf{mr} & \textbf{mt} & \textbf{te} \\ \hline
	
		\multicolumn{8}{l}{\cellcolor[HTML]{FFFFFF}\textbf{Whisper-Tiny (fine-tuned on RESPIN subset)}} \\
		\rowcolor[HTML]{FFFFFF}  & D1 & 9.45 & 11.72 & 6.81 & 7.27 & 12.38 & 16.8 & 10.8 & 11.69 & 13.2 & 26.84 & 32.65 & 19.22 & 19.43 & 46.98 & 40.19 & 37.21 & 34.8 & 44.05 \\
		\rowcolor[HTML]{F2F2F2}  & D2 & 9.2 & 10.45 & 6.7 & 14.03 & 8.57 & 11.53 & 9.39 & 11.44 & 9.15 & 26.8 & 29.85 & 20.22 & 25.45 & 35.34 & 30.49 & 32.8 & 33.16 & 37.85 \\
		\rowcolor[HTML]{FFFFFF}  & D3 & 10.14 & 9.1 & 6.44 & 6.85 & 17.14 & 13.11 & 6.74 & 8.88 & 10.29 & 28.62 & 26.04 & 18.82 & 17.37 & 61.88 & 35.51 & 22.16 & 28.6 & 40.28 \\
		\rowcolor[HTML]{F2F2F2}  & D4 & - & 14.39 & 8.4 & 11.38 & 17.74 & 15.13 & 9.5 & 10.67 & 13.21 & - & 39.01 & 24.49 & 26.84 & 61.02 & 41.64 & 30.91 & 30.72 & 44.42 \\
		\rowcolor[HTML]{FFFFFF}  & D5 & - & 12.53 & - & 6.26 & 8.43 & - & - & - & - & - & 35.55 & - & 16.2 & 38.16 & - & - & - & - \\
		\rowcolor[HTML]{F2F2F2}  & Overall & 9.62 & 11.6 & 7.13 & 9.69 & 12.62 & 13.98 & 9.15 & 10.73 & 11.43 & 27.45 & 32.51 & 20.81 & 21.71 & 48.54 & 36.4 & 30.93 & 31.96 & 41.61 \\

		\multicolumn{8}{l}{\textbf{Whisper-Base (fine-tuned on RESPIN subset)}} \\
		\rowcolor[HTML]{FFFFFF} & D1 & 7.22 & 7.34 & 5.21 & 4.4 & 7.35 & 11.41 & 8.2 & 8.17 & 8.71 & 22.3 & 24.35 & 15.8 & 12.62 & 34.45 & 32.84 & 31.66 & 27.55 & 34.44 \\
		\rowcolor[HTML]{F2F2F2} & D2 & 6.73 & 7.05 & 4.81 & 7.9 & 4.68 & 8.94 & 6.51 & 8.02 & 6.52 & 22.11 & 22.52 & 15.48 & 19.27 & 23.38 & 26.05 & 26.28 & 25.09 & 30.88 \\
		\rowcolor[HTML]{FFFFFF} & D3 & 7.46 & 5.38 & 4.77 & 4.18 & 11.67 & 9.92 & 4.21 & 6.35 & 7.16 & 23.07 & 17.94 & 15.16 & 11.23 & 48.4 & 30.01 & 15.92 & 21.93 & 31.45 \\
		\rowcolor[HTML]{F2F2F2} & D4 & - & 10.19 & 6.48 & 7.24 & 12.69 & 12.04 & 5.84 & 7.35 & 7.62 & - & 30.92 & 19.89 & 18.45 & 49.88 & 35.06 & 22.59 & 24.38 & 35.38 \\
		\rowcolor[HTML]{FFFFFF} \multirow{-5}{*}{} & D5 & - & 8.62 & - & 3.72 & 5.03 & - & - & - & - & - & 28.27 & - & 11.48 & 26.93 & - & - & - & - \\
		\rowcolor[HTML]{F2F2F2} & Overall & 7.15 & 7.69 & 5.36 & 5.8 & 8.1 & 10.44 & 6.23 & 7.51 & 7.51 & 22.51 & 24.71 & 16.67 & 15.19 & 36.52 & 30.54 & 24.28 & 24.8 & 32.99 \\
		\multicolumn{8}{l}{\textbf{Whisper-Small (fine-tuned on RESPIN subset)}} \\
		\rowcolor[HTML]{FFFFFF} & D1 & 8.43 & 4.96 & 3.86 & 3.28 & 5.52 & 7.99 & 5.6 & 6.97 & 6.24 & 19.44 & 17.93 & 11.81 & 10.1 & 27.73 & 24.5 & 23.93 & 23.79 & 27.16 \\
		\rowcolor[HTML]{F2F2F2} & D2 & 8.8 & 4.38 & 3.52 & 5.39 & 2.79 & 6.55 & 3.8 & 6.34 & 4.74 & 18.19 & 16.18 & 11.78 & 14.44 & 15.41 & 20.43 & 18.31 & 20.6 & 25.91 \\
		\rowcolor[HTML]{FFFFFF} & D3 & 6.61 & 3.77 & 3.51 & 2.87 & 9.09 & 7.19 & 2.57 & 4.72 & 8.81 & 19.34 & 14.24 & 11.32 & 8.8 & 41.89 & 24.18 & 9.66 & 17.02 & 28.1 \\
		\rowcolor[HTML]{F2F2F2} & D4 & - & 7.92 & 4.43 & 5.53 & 9.86 & 8.47 & 3.64 & 5.57 & 6.17 & - & 25.63 & 14.29 & 14.49 & 43.46 & 28.21 & 15.29 & 19.43 & 30.14 \\
		\rowcolor[HTML]{FFFFFF} & D5 & - & 6.34 & - & 2.58 & 3.57 & - & - & - & - & - & 20.87 & - & 8.95 & 20.39 & - & - & - & - \\
		\rowcolor[HTML]{F2F2F2} & Overall & 7.9 & 5.46 & 3.85 & 4.16 & 6 & 7.46 & 3.93 & 5.94 & 6.54 & 19.02 & 18.91 & 12.36 & 11.78 & 29.66 & 23.94 & 16.95 & 20.28 & 27.82 \\
		\multicolumn{8}{l}{\textbf{E-Branchformer (trained from scratch on RESPIN subset)}} \\
		\rowcolor[HTML]{FFFFFF} & D1 & 4.97 & 3.90 & 3.60 & 2.98 & 4.14 & 7.44 & 4.84 & 6.18 & 4.51 & 14.91 & 13.90 & 9.98 & 8.71 & 22.90 & 21.89 & 21.05 & 19.64 & 21.58 \\
		\rowcolor[HTML]{F2F2F2} & D2 & 4.87 & 3.86 & 3.10 & 4.62 & 1.85 & 5.82 & 3.01 & 5.90 & 3.19 & 14.82 & 13.86 & 9.78 & 12.18 & 11.64 & 16.55 & 16.07 & 17.79 & 19.72 \\
		\rowcolor[HTML]{FFFFFF} & D3 & 5.00 & 2.53 & 3.37 & 2.25 & 7.43 & 6.16 & 1.86 & 4.82 & 3.80 & 15.86 & 9.72 & 10.35 & 7.36 & 36.96 & 21.10 & 7.69 & 15.90 & 20.78 \\
		\rowcolor[HTML]{F2F2F2} & D4 & - & 6.56 & 4.37 & 4.45 & 8.44 & 7.61 & 2.92 & 6.05 & 4.40 & - & 20.67 & 12.10 & 11.99 & 37.06 & 23.28 & 12.51 & 18.41 & 24.62 \\
		\rowcolor[HTML]{FFFFFF} & D5 & - & 4.90 & - & 2.41 & 2.11 & - & - & \multicolumn{1}{l}{} & - & - & 16.94 & - & 7.79 & 14.52 & - & - & - & - \\
		\rowcolor[HTML]{F2F2F2} & Overall & 4.95 & 4.33 & 3.63 & 3.52 & 4.62 & 6.68 & 3.19 & 5.75 & 3.97 & 15.21 & 14.96 & 10.59 & 9.94 & 24.50 & 20.38 & 14.48 & 17.95 & 21.64 \\
		\multicolumn{1}{r}{\textbf{TDNN-HMM}}  & \multicolumn{6}{l}{\textbf{(trained from scratch on RESPIN subset)}}  \\
		\rowcolor[HTML]{FFFFFF} & D1 & 5.80 & 5.04 & 4.25 & 2.57 & 4.04 & 8.87 & 4.52 & 6.90 & 4.35 & 18.02 & 15.22 & 11.88 & 7.56 & 20.71 & 24.30 & 17.94 & 21.52 & 19.72 \\
		\rowcolor[HTML]{F2F2F2} & D2 & 5.55 & 4.51 & 3.63 & 3.95 & 2.33 & 7.07 & 3.22 & 7.18 & 3.17 & 17.36 & 15.26 & 11.19 & 9.55 & 12.84 & 19.04 & 13.92 & 20.80 & 19.79 \\
		\rowcolor[HTML]{FFFFFF} & D3 & 5.66 & 3.46 & 3.80 & 2.31 & 7.42 & 7.05 & 2.44 & 5.47 & 4.10 & 17.31 & 11.71 & 11.41 & 6.24 & 33.34 & 22.91 & 8.97 & 18.24 & 21.47 \\
		\rowcolor[HTML]{F2F2F2} & D4 & - & 8.48 & 5.92 & 4.21 & 9.22 & 7.92 & 2.92 & 6.39 & 4.12 & - & 26.33 & 15.94 & 10.94 & 35.63 & 23.99 & 12.37 & 19.66 & 22.26 \\
		\rowcolor[HTML]{FFFFFF} & D5 & - & 4.80 & - & 2.40 & 2.18 & - & - & - & - & - & 16.26 & - & 7.89 & 12.57 & - & - & - & - \\ 
		\rowcolor[HTML]{F2F2F2} & Overall & 5.67 & 5.22 & 4.45 & 3.25 & 4.88 & 7.69 & 3.30 & 6.53 & 3.94 & 17.57 & 16.87 & 12.69 & 8.72 & 23.01 & 22.33 & 13.40 & 20.13 & 20.81 \\ \bottomrule

\end{tabular}%
}
\label{tab:consolidated_resultsothers}
\end{table}

Tables~\ref{tab:consolidated_resultsfariseq} and~\ref{tab:consolidated_resultsothers} present a detailed comparison of baseline ASR models evaluated on the RESPIN test set. The experiments span Fairseq-based, ESPnet-based, Whisper-based, and Kaldi-based systems, offering a broad perspective across different modeling frameworks.  These results offer insights into how different modeling paradigms perform when confronted with the dialectal, phonetic, and structural diversity embedded in RESPIN.

The results show that models fine-tuned on RESPIN consistently outperform those trained or adapted solely on external corpora. This confirms that RESPIN’s dialect-rich coverage is essential for achieving reasonable performance across all nine languages. In particular, the differences in WER between fine-tuned and non-fine-tuned versions of the same model (e.g., SPRING-Data2Vec-AQC) illustrate how dialect-specific supervision significantly impacts accuracy.

Among the baselines, SPRING-Data2Vec-AQC (fine-tuned on RESPIN subset) performs best for almost all languages and dialects; E-Branchformer model trained from scratch on RESPIN performs competitively across many languages, especially those with high data coverage in the clean subset. Its performance in languages such as Hindi, Marathi and Telugu suggests that well-curated, supervised data can serve as a strong foundation even in the absence of large-scale pretraining.

Whisper models demonstrate robustness across languages, but tend to underperform compared to other RESPIN-finetuned models. In some languages, such as Chhattisgarhi and Magahi, the gap between Whisper and fine-tuned SSL baselines widens, indicating that Whisper’s general-purpose training may not adequately capture the nuances of dialectal variation.

The detailed language-wise breakdown also reveals non-uniform trends in CER and WER. For instance, Dravidian languages like Kannada and Telugu often yield lower CERs but relatively higher WERs, which may reflect challenges in tokenization or word boundary segmentation. On the other hand, languages like Hindi and Bhojpuri tend to show tighter CER–WER alignment.

Variability across dialects is also evident, with certain dialects (e.g., D2, D4 of Hindi, D4 of Bengali, D3, D4 of Kannada) consistently resulting in higher error rates. This highlights the importance of dialect-level metadata in training and evaluation, which RESPIN-S1.0 explicitly encodes.

% Overall, these results validate RESPIN-S1.0 ’s value as a benchmarking resource.
Overall, these results highlight the utility of RESPIN-S1.0 as a robust benchmarking resource. The diversity in performance across models, languages, and dialects underscores the need for future work on dialect-adaptive modeling and context-aware evaluation. The RESPIN-S1.0 dataset offers a controlled and consistent framework for such comparative analyzes and will continue to facilitate reproducible benchmarking of ASR systems aimed at capturing the linguistic diversity of Indian languages.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Non-Technical Appendices}

% \subsection{Onboarding Human Resources and Large-Scale Text Data Collection (NT)}



% The text data providers are herein referred to as Domain Experts (DEs). Domain experts went through a screening test and subsequent training process before sourcing text data from them. The data went through a quality assurance process and feedback was provided to them to ensure adherence to quality requirements.  \\
% The text data validators are herein referred to as Language consultants (LCs). Language consultants similarly went through a more extensive screening tests, interviews and subsequent training process before onboarding them for text data validation tasks.  \\
% The data from LCs went through review from Language Resource Managers (LRMs) / Senior LRMs. LRMs hold advanced degrees (PhD/Masters) in linguistics, and/or experience in linguistics. Finally, Speech Scientists and Computational Linguists audit the technical accuracy to guarantee consistency with our quality requirements. \\
% The Validation Team comprises Language Consultants (LCs), Language Resource Managers (LRMs), Senior LRMs, and Speech/Computational Scientists, organized in the following hierarchy as depicted below. The onboarding details of these resources are detailed in subsequent sections
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.5\linewidth]{Styles/figures/nt_hierarchy.png}
%     \caption{Validation Team Hierarchy Diagram}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \begin{minipage}[b]{0.48\linewidth}
%         \centering
%         \includegraphics[width=0.90\linewidth]{Styles/figures/google_form_for_text_data_collection.png}
%         % \caption*{(a) Page 1}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{Styles/figures/google_form_page2.png}
%         % \caption*{(b) Page 2}
%     \end{minipage}
%     \caption{Google Form interface used for text data collection.}
%     \label{fig:google-form-pages}
% \end{figure}

% \subsubsection{Domain Experts}

% \textbf{Domain Expert Qualification Criteria} \\
% Domain experts are individuals with specialized knowledge of both the dialect of a particular language and the specific domain for which sentence data is being collected. Candidates selection were subjected to satisfying the following criteria:
% \begin{itemize}
%     \item \textbf{Geographic Authenticity}: Resident of the district/village where the target dialect is natively spoken
%     \item \textbf{Literacy Criteria:} 
% Have the ability to RWS (Read, Write and Speak) the dialect.
% \item \textbf{Smartphone usage}: 
% Have the ability to type in the native script on a smartphone
% \item \textbf{Domain Expertise}: 
% Current employment or valid experience within the target domain (e.g., banking professionals for financial domain text, Farmers for Agricultural domain text)
% \item \textbf{User Engagement}: 
% Regular interaction with intended end-users of the speech recognition system (e.g., government agricultural officers with direct farmer engagement for Agricultural domain text)
% \item \textbf{End-user Perspective}: 
% When possible, domain experts should themselves be potential end-users of the intended system (e.g., farmers for agricultural e-commerce applications)
% \end{itemize}



% \textbf{Domain Expert Training Process}
% \textbf{Initial Orientation Protocol: }
% \begin{itemize}
%     \item Once domain experts were selected, on-ground coordinators conduct a comprehensive briefing regarding project objectives. 
% \end{itemize}

% \textbf{Task Specification Guidelines:}
% \begin{itemize}
%     \item Clear communication of task requirements was conducted via telephone with selected participants.
%     \item Natural conversational questions are required, reflecting authentic spoken interactions
% \item Local terminology for agricultural/banking concepts must be prioritized
% \item Regional variants of crop names, disease terminology, and farming practices should be included
% \item Data collection will proceed via digital forms (Google-forms) distributed through messaging platforms
% \item Each form corresponds to a specific agricultural subdomain
% \item Participants should provide 7 unique contextual questions per topic category
% Compensation structure is based on unique contributions meeting quality standards
% Duplicate or repetitive submissions will not qualify for compensation
% \end{itemize}

% \textbf{Technical Facilitation:}
% \begin{itemize}
%     \item Participants were provided with appropriate input method resources:
% Digital keyboard installation training and instructions for the target regional language
% \item Handwriting input method configuration guidance for participants with limited typing experience.
% \item Video tutorials demonstrating proper usage of input methods
% \item Detailed written instructions accessible through the data collection platform
% \end{itemize}
% \textbf{Quality Assurance Process}
% \begin{itemize}
%     \item Initial submissions are vetted carefully for common errors:
%     \item Non-target language usage (e.g., English typing instead of regional language)
%     \item  Statement responses rather than questions
% \item Repetitive or duplicated content across categories
% \item Transliterated text input rather than proper script usage
% \item Immediate corrective feedback was provided when necessary to ensure subsequent submissions meet quality requirements.
% \end{itemize}

% \textbf{Submission Timeline and Verification}
% \begin{itemize}
%     \item Participants received 1 - 2 weeks to complete all forms in a given domain
% Project coordinators maintain regular communication to ensure adherence to deadlines
% \item Technical team requires seven working days for initial validation and quality assessment post submission of the text data
% Compensation follows successful verification of submission quality and was prorated on the basis of accepted sentences.
% \item Verification focuses on linguistic authenticity, dialectal accuracy, and domain relevance.
% \item  Text data were collected from Domain Experts via Google Forms and then submitted to the Validation Team for verification and finalization before being uploaded for Voice Collection. The figure below shows a sample form for the subtopic “Climate and Weather” in Hindi, completed by a dialect-specific Domain Expert.
% \end{itemize}





% \subsubsection{Language Consultants (LCs):
% }

% \textbf{Language consultants Qualification Criteria}\\
% Language consultants are individuals with specialized knowledge of language structures and dialectal variations who validate corpus data for voice collection. Candidates who were hired satisfy the following criteria:
% \begin{enumerate}
%     \item \textbf{Dialectal Proficiency}: Native-level fluency in the target dialect with comprehensive understanding of regional variations.
% \item \textbf{Academic Qualification}: Advanced degree in linguistics or related fields with specialization in the target language (M.a/M.Phil)
% Literacy Competence: Advanced ability to read, write, and analyze text in the target language and its dialectal variants
% \item \textbf{Digital Literacy}: Proficiency with a laptop and the usage of indic language keyboards for linguistic analysis and data validation
% \item \textbf{Analytical Skills}: Demonstrated ability to identify phonological patterns and structural inconsistencies in language data
% \item \textbf{Transcription Expertise}: Optional experience with IPA transcription and transliteration between scripts
% \item \textbf{Sociolinguistic Awareness}: Optional understanding of sociolinguistic variation across demographic groups within the language community
% \end{enumerate}

% \subsection{Onboarding Human Resources and Large-Scale Text Data Collection (IISc)}

% As we planned to collect more sentences from the IISc side to increase vocabulary diversity, there was a need to find and recruit dialect experts from each targeted region. Our final aim was to collect an additional 15k dialect as well as domain-rich sentences from each of the 9 languages. So we started advertising the job requirements of dialect experts through online and offline modes. There were mainly 2 major job openings for the dialect experts- \textbf{Sentence Composition} and \textbf{Sentence Translation}. Based on the applications, we have shortlisted the native people and conducted a few rounds of PILOT tests to check their language and dialect knowledge. After the final selection, a training session was provided to up-skill them based on our requirements.

% \subsubsection{Sentence Composition:}

% Composers are individuals with good knowledge of the dialect of a particular language and have good writing skills. Candidates selection were subjected to satisfying the following criteria.
% \textbf{Qualification Criteria}
% \begin{itemize}
% \item Undergraduate, Graduate, Post Graduate can apply.
% \item Freshers and experienced both can apply for this job. However, candidates with prior experience with sentence composition will be given more preference.
% \end{itemize}

% \textbf{Skills/ requirements}
% \begin{itemize}
% \item Basic Computer Knowledge and typing skills
% \item Should have good internet connectivity.
% \item Must have their Laptop / Desktop at home and good knowledge of Google Docs, sheets.
% \item Should be native to one of the nine languages (Hindi, Telugu, Chhattisgarhi, Bengali, Kannada, Marathi, Maithili, Magahi, Bhojpuri).
% \item Having a degree in native language related course or program is a plus.
% \item Should have basic knowledge in one or multiple domains (agriculture, finance) and should be able to compose best quality sentences on the topics given from each domain.
% \item Should have excellent writing skills and grammatical knowledge.
% Excellent typing skills with at least 40 WPM
% \end{itemize}

% \textbf{Job description and responsibility}
% \begin{itemize}
% \item We are looking for a candidate who is very active and has excellent writing skills.
% \item The candidate is expected to write the sentences in a dialect your language.
% \item His/ her role will be to compose a few sentences in their dialect for each topic by using rich vocabulary and increasing
% the vocabulary size gradually as more sentences are composed, building sentences of 8-15 words long and without making any grammatical errors.
% \item There should be no copyright text or google translated text from other languages.
% \item The candidate is expected to use relevant terminologies and compose sentences for each given topic in that language.
% \item The candidate will be responsible for sentences entered in a given Google document.
% \item The candidate should be regular with the given task.
% \end{itemize}

% \subsubsection{Sentence Translation:}
% Apart from the Composers, Translators won't have much sentence creativity skills. They are individuals with good knowledge of the dialect of a particular language and have good translation skills from standard style to their native dialect. Candidates selection were subjected to satisfying the following criteria.

% \textbf{Qualification Criteria}
% \begin{itemize}
% \item Undergraduate, Graduate, Post Graduate can apply.
% \item Freshers and Experienced both can apply for these jobs.
% \end{itemize}

% \textbf{Skills/ requirements}
% \begin{itemize}
% \item Basic Computer Knowledge and typing skills
% \item Should be able to read and understand native language (local dialect as well the standard version of the language)
% \item Should be native to one of the nine languages (Hindi, Telugu, Chhattisgarhi, Bengali, Kannada, Marathi, Maithili, Magahi, Bhojpuri).
% \item Having a degree in a native language-related course or program is a plus
% \item Must have their Laptop / Desktop at home.
% \item Must have good knowledge of Google Docs, sheets, Google Transliteration
% \item Should have good internet connectivity.
% \end{itemize}

% \textbf{Job description and responsibility}
% \begin{itemize}
% \item A set of sentences will be given to you in the standard dialect of the chosen language.
% \item The task will be to read \& understand the sentence. Then if required, we expect you to modify/translate the sentence to match your native dialect
% \item The candidate is expected to write the sentences without any typos/ grammatical errors.
% \item The candidate should be regular with the given task.
% \item An interested candidate needs to go through a pilot task and will be selected for the job only if they are found to perform satisfactorily.
% \end{itemize}





% The text data received from NT and IISc undergoes a multi-stage review process by the Validation Team. It is first split by domain, dialect, and language, then assigned to dialect-specific language consultants. Each consultant provides the corrected sentence, its translation, and the proper feature category to which the sentence belongs. \\
% Below are a couple of Marwari-dialect Hindi sentences from the agricultural domain, as reviewed and corrected by the Validation Team.

% % Please add the following required packages to your document preamble:
% % \usepackage{graphicx}
% % \usepackage[table,xcdraw]{xcolor}
% % Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% \begin{table}[]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{
% >{\columncolor[HTML]{FFFFFF}}c 
% >{\columncolor[HTML]{FFFFFF}}c 
% >{\columncolor[HTML]{FFFFFF}}c 
% >{\columncolor[HTML]{FFFFFF}}l 
% >{\columncolor[HTML]{FFFFFF}}l 
% >{\columncolor[HTML]{FFFFFF}}l 
% >{\columncolor[HTML]{FFFFFF}}l }
% \toprule
% \textbf{Name of DE} & \textbf{PinCode} & \textbf{Name of District} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{Feature Category}} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{Sentence provided by DE}} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{Corrected sentence in Hindi by LC}} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{Translation provided by LC}} \\
% \midrule
% XXXX & 341319 & Nagaur & 1 - Crop Names and Crop seasons & \begin{hindi}
%     अगेती रबी मौसम के अंतर्गत कौन से महीने आते हैं?\end{hindi}  & \begin{hindi}
%     अगेती रबी मौसम के अंतर्गत कौनसे महीने आते हैं ?
        
%     \end{hindi}
%  & Which months fall under the early rabi season ? \\
% XXXX & 341319 & Nagaur & 2 - Seeds, Seed Names, Varieties and Hybrids & \begin{hindi}
% मक्का की सबसे अच्छी उपज वाली किस्म कोनसी है? 
%     \end{hindi}& \begin{hindi}
%         मक्का की सबसे अच्छी उपज वाली किस्म कौनसी है ? 
%     \end{hindi}
% & Which is the best yielding variety of maize crop ? \\ 
% \bottomrule
% \end{tabular}%
% }
% \vspace{1em}
% \caption{Illustration of Text data validation}
% \label{tab:validation_table}
% \end{table}
% We programmed scripts to automatically tag issues, helping the Validation Team with their preliminary screening. Below is an example for a single sentence to illustrate this process. In the actual validation workflow, the items generated for each field in every sentence are presented as columns in a Google Sheet.


% % Please add the following required packages to your document preamble:
% % \usepackage{graphicx}
% \begin{table}[]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|l|l|}
% \hline
% LC Sentences & \begin{hindi}
    
% अगेती रबी मौसम के अंतर्गत कौनसे महीने आते हैं ?\end{hindi} \\ \hline
% Transliteration & ageetii rabii mousam kee atargat koun see mahiinee aatee hei \\ \hline
% Translations & Which months come under the early rabi season ? \\ \hline
% Special Symbols & {[}"?"{]} \\ \hline
% Acronyms & {[}{]} \\ \hline
% Roman Numerals & {[}{]} \\ \hline
% Alphanumerics & {[}{]} \\ \hline
% Numerics & {[}{]} \\ \hline
% English Words & {[}{]} \\ \hline
% Abbreviations & {[}{]} \\ \hline
% Letters & {[}{]} \\ \hline
% Letter Translit & { [}\begin{hindi}
    
% 'के'\end{hindi}{] } \\ \hline
% Numeric Transl & {[}{]} \\ \hline
% Non Whitespaces & {[}{]} \\ \hline
% Original sentences &  \\ \hline
% \end{tabular}%
% }
% \caption{Illustration of Automatic tagging of Text data validation
% }
% \label{tab:table2}
% \end{table}
% The data underwent multiple rounds of automatic issue tagging, followed by manual corrections by language consultants and a subsequent multi-stage review by the Validation Team.

% \subsection{Translation Workflow: Email and Instructions}
% \label{appendix:translation_mail}

% Includes templates sent to volunteers translating under-represented dialects.

% \subsection{Contributor Compensation}
% \label{appendix:payment_details}

% Summarizes payment structure across roles (composers, validators, translators), varying by task complexity and word length.

% \subsection{Recruitment Materials}
% \label{appendix:recruitment_ads}

% Samples of recruitment ads—WhatsApp banners, email flyers, posters—used online/offline.

% \subsection{Contributor Demographics}
% \label{appendix:contributor_stats}

% Aggregated contributor statistics (language, dialect, district, role), anonymized for privacy.

% \subsection{Domain Topic Preparation}
% \label{appendix:topic_prep}

% Instructions and link collections provided to contributors for agriculture and finance domains.

% \subsection{Pilot Task Communication}
% \label{appendix:pilot_mail}

% Instructions, onboarding, and feedback shared with early contributors for pilot evaluation.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Non-Technical Appendices}

\subsection{Contributor Onboarding, Validation, and Management in Phase-1 and Phase-2}

\subsubsection{Validation Team Hierarchy and Oversight}

A structured multi-tier validation team was formed to manage linguistic quality assurance throughout both phases of text data collection. The team included Language Consultants (LCs), Language Resource Managers (LRMs), Senior LRMs, and technical reviewers such as Speech Scientists and Computational Linguists. LCs were responsible for validating dialectal data and were supported by LRMs and Senior LRMs, who held advanced linguistic degrees or equivalent field experience. Final review was conducted by technical experts to ensure linguistic accuracy, dialectal fidelity, and overall corpus consistency. Figure~\ref{fig:slab_def_fig1} shows the hierarchy diagram of the validation team.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.65\linewidth]{Styles/figures_appendix/slab_def_fig1.png}
\caption{Validation Team Hierarchy Diagram}
\label{fig:slab_def_fig1}
\end{figure}

\subsubsection{Phase-1: Domain Expert Recruitment and Collection Process}

Text data in Phase-1 was sourced from Domain Experts (DEs), selected for their geographic authenticity, native fluency in the target dialect, and experience in relevant domains such as agriculture or finance. Many were directly engaged with the target user groups—for instance, agricultural officers, field workers, or local farmers—and often doubled as the intended end-users of the speech recognition systems.

After selection, DEs received orientation through on-ground coordinators who explained the project goals and tasks. Verbal briefings were reinforced with examples and instructions. DEs were expected to generate natural, contextually relevant questions that reflected spoken usage in their dialect. They were guided to use regionally appropriate terminology, including local crop names, disease references, and finance-related language.

Data was submitted using Google Forms circulated via messaging platforms. Each form focused on a specific subdomain (e.g., weather, irrigation, savings). For each topic, DEs were instructed to contribute seven unique questions. Duplicate or generic questions were not permitted, and compensation was based on the count of accepted entries. 

Technical support included installation guides for regional language keyboards and handwriting input methods. Contributors also received video tutorials and written documentation. Submissions were checked for frequent issues such as use of English or transliteration, sentence fragments, and repeated entries. Corrective feedback was issued promptly to allow resubmission. Typically, DEs had one to two weeks to complete their assigned forms. A preliminary technical validation was conducted within seven working days, after which validated contributions were passed to the LC-led validation pipeline.

\subsubsection{Phase-2: Dialect Expert Recruitment for Text Expansion}

To further expand vocabulary coverage, Phase-2 aimed to collect an additional 15,000 dialect- and domain-rich sentences per language. Recruitment drives were launched online and offline to attract native speakers from each target region. Two major roles were advertised: Sentence Composition and Sentence Translation.

Applicants underwent one or more rounds of pilot tasks designed to evaluate their native dialect knowledge, grammatical accuracy, and overall writing fluency. Final selections were followed by training sessions to align contributors with sentence structure expectations, domain terminology, and submission procedures.

\paragraph{Sentence Composers}

Composers were expected to generate original, grammatically accurate, domain-relevant sentences in their native dialects. They could be freshers or experienced writers, but preference was given to those with prior linguistic work. Minimum qualifications included undergraduate-level education and native-level dialect proficiency.

Contributors worked from home using their own laptop or desktop with stable internet access. Familiarity with Google Docs and basic digital tools was mandatory. Composers were instructed to write sentences between 8–15 words in length, using domain-specific vocabulary for agriculture or finance. They were encouraged to gradually increase vocabulary variety, while avoiding copied, machine-translated, or overly simplistic text. Daily consistency and attention to grammatical quality were strongly emphasized. Sentences were written directly into Google Docs and reviewed periodically.

\paragraph{Sentence Translators}

Translators were tasked with converting standard dialect sentences into regional dialects. Candidates had to demonstrate their ability to understand both formal and informal linguistic registers and accurately map structure and vocabulary to dialectal forms.

Translators also worked remotely using their own devices. In addition to reading comprehension, they were expected to ensure proper grammar, eliminate typos, and maintain the core meaning of each prompt. Like composers, translators were evaluated via pilot tests before onboarding and were required to regularly complete assignments shared through collaborative platforms.

\subsubsection{Validation and Review Workflow}

All data collected from Phase-1 and Phase-2 underwent a standardized review pipeline. After initial formatting and cleaning, the data was grouped by language, dialect, and domain. It was then assigned to dialect-specific Language Consultants (LCs), who reviewed each entry for correctness, dialectal fidelity, and semantic appropriateness.

Each validated sentence included a corrected version in the target script, an English translation, and a feature category tag (e.g., crop names, disease, finance). This multi-field structure enabled easier filtering for downstream processing. LCs submitted their validations through structured spreadsheets which were then escalated to LRMs and Senior LRMs for secondary review.

To further assist validation, automated scripts flagged possible issues including spelling anomalies, punctuation errors, presence of English terms, improper transliteration, or use of unsupported symbols. These flagged fields were automatically included as columns in internal Google Sheets, making it easier for validators to filter and prioritize corrections.

\subsection{Contributor Communication and Management}

\paragraph{Translation Workflow Coordination}

Contributors working on sentence translation into under-represented dialects received templated instructions via email. These instructions clarified formatting rules, translation scope, common pitfalls, and expected turnaround timelines.

\paragraph{Compensation and Incentives}

Compensation policies varied by role and task type. For composers and translators, payments were tied to the number of accepted entries and sometimes adjusted based on sentence complexity and length. For validators, compensation reflected the volume and quality of validated entries. Contributors were informed of their compensation slabs during onboarding.

\paragraph{Recruitment Materials and Outreach}

Recruitment efforts were supported by diverse media including WhatsApp banners, email flyers, and community posters. These were customized by region and dialect to maximize outreach in both rural and semi-urban areas.

\paragraph{Contributor Metadata and Demographics}

Each contributor’s demographic details—such as district, dialect, role, and language—were logged and anonymized for analysis and internal audit. These statistics also informed dialect balance in the collected corpus.

\paragraph{Domain Preparation Resources}

Contributors were provided curated resources to assist with sentence creation. These included lists of subdomains (e.g., irrigation, pest control, loan schemes), topic-specific vocabulary references, and guidelines on question framing for each domain.

\paragraph{Pilot Task Evaluation}

Before onboarding, all contributors participated in pilot tasks that mirrored their actual assignments. These pilots helped identify errors in grammar, dialect usage, or formatting. Personalized feedback was shared to align expectations and ensure readiness for full-scale contribution.


\end{document}
